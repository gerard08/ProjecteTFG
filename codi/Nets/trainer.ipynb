{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m-64z_tpWotj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from tensorboardX import SummaryWriter\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "\n",
        "#writer = SummaryWriter('runs/exp-1')\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oo185FDPWzFQ",
        "outputId": "8a9c6c86-086b-4954-9095-27a8d3f211f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'NVIDIA GeForce 940MX'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TzshwdgWYCG-"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, in_cn, out_cn):\n",
        "    super().__init__()\n",
        "    #cada block te dos convolucions\n",
        "    self.conv1 = nn.Conv2d(in_cn, out_cn, 3)\n",
        "    self.conv2 = nn.Conv2d(out_cn, out_cn, 3)\n",
        "    #definim la funció d'activació\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.relu(self.conv2(self.relu(self.conv1(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q2lzluIjk8gJ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  #a chs passem les dimensions dels Blocks que tindrem\n",
        "  def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "    super().__init__()\n",
        "    #creem una llista de mòduls amb els blocks,\n",
        "    #les dimensions seràn i i i+1, sent aquestes valors de la llista chs\n",
        "    self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs) - 1)])\n",
        "    self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    ftrs = []\n",
        "    #per cada Block de la llista\n",
        "    for block in self.enc_blocks:\n",
        "      #obtenim l'output\n",
        "      x = block(x)\n",
        "      #l'afegim a la llista ftrs\n",
        "      ftrs.append(x)\n",
        "      #reduïm dimensions\n",
        "      x = self.pool(x)\n",
        "    return ftrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vzON7MPjvFnh"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  #a chs passem les dimensions dels Blocks que tindrem\n",
        "  def __init__(self, chs = (1024, 512, 256, 128, 64)):\n",
        "    super().__init__()\n",
        "    self.chs = chs\n",
        "    #definim les upconvs com una llista de up-convolucions que anirà tenint els \n",
        "    #inputs i outputs i i i+1 definits a chs\n",
        "    self.upconvs = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs) - 1)])\n",
        "    #creem una llista de mòduls amb els blocks,\n",
        "    #les dimensions seràn i i i+1, sent aquestes valors de la llista chs\n",
        "    self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs) - 1)])\n",
        "\n",
        "  def forward(self, x, encoder_features):\n",
        "    #per cada valor dels tamanys\n",
        "    for i in range(len(self.chs) - 1):\n",
        "      #fem la up-convolution corresponent\n",
        "      x = self.upconvs[i](x)\n",
        "      #retallem el marge dels resultats obtinguts per l'encoder\n",
        "      # (buit per culpa de la convolució)\n",
        "      enc_ftrs = self.crop(encoder_features[i], x)\n",
        "      #concatenem el valor actual amb els features retallats anteriorment\n",
        "      x = torch.cat([x, enc_ftrs], dim=1)\n",
        "      #ho passem al Block corresponent del Decoder\n",
        "      x = self.dec_blocks[i](x)\n",
        "    return x\n",
        "  \n",
        "  def crop(self, enc_ftrs, x):\n",
        "    #optenim l'altura i amplada de x\n",
        "    _, _, H, W = x.shape\n",
        "    #retallem\n",
        "    enc_ftrs = torchvision.transforms.CenterCrop([H,W])(enc_ftrs)\n",
        "    return enc_ftrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A0tc8DZ84vfN"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_chs)\n",
        "    self.decoder = Decoder(dec_chs)\n",
        "    self.head = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
        "    self.retain_dim = retain_dim\n",
        "    self.out_sz = out_sz\n",
        "\n",
        "  def forward(self, x):\n",
        "    enc_ftrs = self.encoder(x)\n",
        "    out = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
        "    out = self.head(out)\n",
        "    if self.retain_dim:\n",
        "      out = nn.functional.interpolate(out, self.out_sz)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, img_lst, gth_lst):\n",
        "        self.img_lst = img_lst\n",
        "        self.gth_lst = gth_lst\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = Image.open(self.img_lst[index])\n",
        "        y = Image.open(self.gth_lst[index])\n",
        "\n",
        "        return {'img':x, 'gth':y}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "params = {'batch_size': 64,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 4}\n",
        "max_epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = 'C:/Users/ger-m/Desktop/UNI/4t/TFG/dataset'\n",
        "pimg = os.listdir(PATH + '/hd')\n",
        "pgth = os.listdir(PATH + '/sd')\n",
        "images = [x for x in pimg]\n",
        "ground = [y for y in pgth]\n",
        "\n",
        "imtrain = images[:int(len(images)*0.75)]\n",
        "imtest = images[int(len(images)*0.75):]\n",
        "\n",
        "gttrain = ground[:int(len(ground)*0.75)]\n",
        "gttest = ground[int(len(ground)*0.75):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = MyDataset(images, ground)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generators\n",
        "training_set = MyDataset(imtrain, gttrain)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
        "\n",
        "validation_set = MyDataset(imtest, gttest)\n",
        "validation_generator = torch.utils.data.DataLoader(validation_set, **params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "unet = UNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(unet.parameters(), lr=1e-4, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(epoch, dataloader, model, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    lensamples = len(dataloader)\n",
        "    for i_batch, sample_batched in enumerate(dataloader):\n",
        "        images = sample_batched['img'].to(device)\n",
        "        ground = sample_batched['gth'].to(device)\n",
        "        n_iter = epoch*lensamples + i_batch\n",
        "        \n",
        "        output = model(images)\n",
        "        \n",
        "        if n_iter%100==0:\n",
        "            xi = vutils.make_grid(images, normalize=True, scale_each=True)\n",
        "            xg = vutils.make_grid(ground,  normalize=True, scale_each=True)\n",
        "            xo = vutils.make_grid(output, normalize=True, scale_each=True)\n",
        "            x = torch.cat((xi,xg,xo),1)\n",
        "            writer.add_image('train/output', x, n_iter)\n",
        "\n",
        "        loss = criterion(output, ground)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        writer.add_scalar('train/loss', loss.item(), n_iter)\n",
        "        \n",
        "        print('Train -> sample/numSamples/epoch: {0}/{1}/{2}, Loss: {3}' \\\n",
        "              .format(i_batch, lensamples, epoch, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(max_epochs):\n",
        "    # Training\n",
        "    train(epoch, training_generator, unet, criterion, optimizer, device)          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "trainer.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "9f8a9dc4ad06f0e7b5d9e8711e5733d3ee7942bbf98b6f3a18e0e585db7611ee"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
