{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:20.691976Z","iopub.status.busy":"2021-11-22T10:10:20.691624Z","iopub.status.idle":"2021-11-22T10:10:22.269777Z","shell.execute_reply":"2021-11-22T10:10:22.269032Z","shell.execute_reply.started":"2021-11-22T10:10:20.691885Z"},"id":"m-64z_tpWotj","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","from tensorboardX import SummaryWriter\n","import torchvision.utils as vutils\n","import numpy as np\n","\n","#writer = SummaryWriter()\n","writer = SummaryWriter('runs/exp-2Test')\n","\n","BATCH = 2\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:45.861546Z","iopub.status.busy":"2021-11-22T10:10:45.861271Z","iopub.status.idle":"2021-11-22T10:10:45.929306Z","shell.execute_reply":"2021-11-22T10:10:45.928069Z","shell.execute_reply.started":"2021-11-22T10:10:45.861516Z"},"id":"oo185FDPWzFQ","outputId":"e973d778-e3ce-4793-a936-9369136a914d","trusted":true},"outputs":[{"data":{"text/plain":["'NVIDIA GeForce 940MX'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","torch.cuda.get_device_name(0)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:47.031962Z","iopub.status.busy":"2021-11-22T10:10:47.030192Z","iopub.status.idle":"2021-11-22T10:10:47.04122Z","shell.execute_reply":"2021-11-22T10:10:47.040584Z","shell.execute_reply.started":"2021-11-22T10:10:47.031916Z"},"id":"TzshwdgWYCG-","trusted":true},"outputs":[],"source":["class Block(nn.Module):\n","  def __init__(self, in_cn, out_cn, mid_cn = None):\n","    super(Block, self).__init__()\n","    if mid_cn is None:\n","      mid_cn = out_cn\n","    #cada block te dos convolucions\n","    self.conv1 = nn.Conv2d(in_cn, mid_cn, 3, padding=1)\n","    #self.bn1 = nn.BatchNorm2d(mid_cn)\n","    #definim la funci贸 d'activaci贸\n","    self.relu1 = nn.ReLU(inplace=True)\n","\n","    self.conv2 = nn.Conv2d(mid_cn, out_cn, 3, padding=1)\n","    #self.bn2 = nn.BatchNorm2d(out_cn)\n","    #definim la funci贸 d'activaci贸\n","    self.relu2 = nn.ReLU(inplace=True)\n","\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    #x = self.bn1(x)\n","    x = self.relu1(x)\n","    x = self.conv2(x)\n","    #x = self.bn2(x)\n","    x = self.relu2(x)\n","    return x\n","\n","  def layersList(self):\n","    return [self.conv1, self.bn1, self.conv2, self.bn2]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:47.333767Z","iopub.status.busy":"2021-11-22T10:10:47.332934Z","iopub.status.idle":"2021-11-22T10:10:47.341974Z","shell.execute_reply":"2021-11-22T10:10:47.341114Z","shell.execute_reply.started":"2021-11-22T10:10:47.333673Z"},"id":"q2lzluIjk8gJ","trusted":true},"outputs":[],"source":["class Encoder(nn.Module):\n","  #a chs passem les dimensions dels Blocks que tindrem\n","  def __init__(self, in_cn, out_cn, pool=True):\n","    super(Encoder, self).__init__()\n","    self.pool = pool\n","    if pool:\n","      self.mp = nn.MaxPool2d(2)\n","    self.block = Block(in_cn, out_cn)\n","\n","  def layers_list(self):\n","    return self.block.layersList()\n","\n","  def forward(self, x):\n","    if self.pool:\n","      x = self.mp(x)\n","    x = self.block(x)\n","    return x"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:47.717241Z","iopub.status.busy":"2021-11-22T10:10:47.716682Z","iopub.status.idle":"2021-11-22T10:10:47.726543Z","shell.execute_reply":"2021-11-22T10:10:47.725651Z","shell.execute_reply.started":"2021-11-22T10:10:47.717204Z"},"id":"vzON7MPjvFnh","trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","  def __init__(self, in_cn, out_cn, mid_cn):\n","    super(Decoder, self).__init__()\n","    self.decoder = nn.Upsample(scale_factor=2, mode='nearest')\n","    self.block = Block(in_cn, out_cn, mid_cn=mid_cn)\n","\n","  def layers_list(self):\n","    return self.block.layersList()\n","\n","  def forward(self, x1, x2=None, x3=None):\n","    x1 = self.decoder(x1)\n","    if x2 is None and x3 is None:\n","      x = x1\n","    elif x3 is None:\n","      x = torch.cat([x2,x1], dim=1)\n","    else:\n","      x = torch.cat([x2,x3,x1], dim=1)\n","    x = self.block(x)\n","    return x"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class Mid(nn.Module):\n","    def __init__(self, in_cn, out_cn, small_cn=None):\n","        super(Mid, self).__init__()\n","        self.mp = nn.MaxPool2d(2)\n","        self.conv1 = nn.Conv2d(in_cn, out_cn, 3, padding=1)\n","        if small_cn is None:\n","            self.conv2 = nn.Conv2d(out_cn, in_cn, 3, padding=1)\n","        else:\n","            self.conv2 = nn.Conv2d(out_cn, small_cn, 3, padding=1)\n","\n","    def layers_list(self):\n","        return [self.conv1, self.conv2]\n","    \n","    def forward(self, x):\n","        x = self.mp(x)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        return x\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class OutConv(nn.Module):\n","    def __init__(self, in_cn, out_cn):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_cn, out_cn, 1)\n","    \n","    def layerslist(self):\n","        return [self.conv]\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:48.013948Z","iopub.status.busy":"2021-11-22T10:10:48.013507Z","iopub.status.idle":"2021-11-22T10:10:48.021677Z","shell.execute_reply":"2021-11-22T10:10:48.020784Z","shell.execute_reply.started":"2021-11-22T10:10:48.013912Z"},"id":"A0tc8DZ84vfN","trusted":true},"outputs":[],"source":["class UNet(nn.Module):\n","  def __init__(self, num_class):\n","    super(UNet, self).__init__()\n","    self.enc1 = Encoder(3, 64, pool=False)\n","    self.enc2 = Encoder(64, 128)\n","    self.enc3 = Encoder(128, 256)\n","    self.enc4 = Encoder(256, 512)\n","    self.mid = Mid(512, 1024)\n","    self.dec1 = Decoder(1024, 256, 512)\n","    self.dec2 = Decoder(512, 128, 256)\n","    self.dec3 = Decoder(256, 64, 128)\n","    self.dec4 = Decoder(128, 64, 64)\n","    self.outcnv = OutConv(64, num_class)\n","\n","  def forward(self, x):\n","    x1 = self.enc1(x)\n","    x2 = self.enc2(x1)\n","    x3 = self.enc3(x2)\n","    x4 = self.enc4(x3)\n","    x5 = self.mid(x4)\n","    x = self.dec1(x5, x4)\n","    x = self.dec2(x, x3)\n","    x = self.dec3(x, x2)\n","    x = self.dec4(x, x1)\n","    x = self.outcnv(x)\n","\n","    return x"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:48.311131Z","iopub.status.busy":"2021-11-22T10:10:48.310484Z","iopub.status.idle":"2021-11-22T10:10:48.480529Z","shell.execute_reply":"2021-11-22T10:10:48.479766Z","shell.execute_reply.started":"2021-11-22T10:10:48.311083Z"},"id":"3EGOPzNmf3xe","trusted":true},"outputs":[],"source":["PATH = 'C:/Users/ger-m/Desktop/UNI/4t/TFG/minidataset/'\n","\n","import numpy as np\n","from torch.utils.data import Dataset\n","import cv2\n","\n","\n","class MyDataset(Dataset):\n","    def __init__(self, lst):\n","        self.lst = lst\n","    \n","    def __getitem__(self, index):\n","        \n","        imsd = 'sd/' + self.lst[index]\n","        imhd = 'hd/' + self.lst[index]\n","        \n","        x = cv2.imread(PATH + imsd)\n","        y = cv2.imread(PATH + imhd)\n","\n","        x = np.transpose(x,(2,0,1))\n","        y = np.transpose(y,(2,0,1))\n","\n","        #return x, y\n","        return {'img':x, 'gth':y}\n","\n","    def __len__(self):\n","        return len(self.lst)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:48.61459Z","iopub.status.busy":"2021-11-22T10:10:48.614353Z","iopub.status.idle":"2021-11-22T10:10:48.641676Z","shell.execute_reply":"2021-11-22T10:10:48.641004Z","shell.execute_reply.started":"2021-11-22T10:10:48.614562Z"},"id":"5U0cc9v4f3xf","trusted":true},"outputs":[],"source":["f = open('C:/Users/ger-m/Desktop/UNI/4t/TFG/listfile.txt', 'r')\n","images = f.read()\n","images = images.split(\"\\n\")\n","f.close()\n","\n","train = images[:int(len(images)*0.05)]\n","test = images[int(len(images)*0.05):int(len(images)*0.07)]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:49.28485Z","iopub.status.busy":"2021-11-22T10:10:49.284536Z","iopub.status.idle":"2021-11-22T10:10:49.290583Z","shell.execute_reply":"2021-11-22T10:10:49.289669Z","shell.execute_reply.started":"2021-11-22T10:10:49.284798Z"},"id":"GZoUalLif3xf","trusted":true},"outputs":[],"source":["# Generators\n","training_set = MyDataset(train)\n","training_generator = torch.utils.data.DataLoader(training_set, batch_size=BATCH, shuffle=True)\n","\n","validation_set = MyDataset(test)\n","validation_generator = torch.utils.data.DataLoader(validation_set, batch_size=BATCH, shuffle=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:49.625198Z","iopub.status.busy":"2021-11-22T10:10:49.62466Z","iopub.status.idle":"2021-11-22T10:10:52.730301Z","shell.execute_reply":"2021-11-22T10:10:52.729495Z","shell.execute_reply.started":"2021-11-22T10:10:49.625152Z"},"id":"bLnRp6qEf3xg","outputId":"1347848c-f69f-4dea-a4ff-8b691e0bd098","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["unet = UNet(num_class=3)\n","from torch import optim\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(unet.parameters(), lr=1e-4, weight_decay=1e-4)\n","unet.to(device)\n","print()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:10:52.732544Z","iopub.status.busy":"2021-11-22T10:10:52.732075Z","iopub.status.idle":"2021-11-22T10:10:52.745891Z","shell.execute_reply":"2021-11-22T10:10:52.744013Z","shell.execute_reply.started":"2021-11-22T10:10:52.732504Z"},"id":"LS7cXnKef3xh","trusted":true},"outputs":[],"source":["def train(epoch, dataloader, model, criterion, optimizer, dev):\n","    model.train()\n","    #print(\"despres train\")\n","    lensamples = len(dataloader)\n","    #print(\"despres len\")\n","    for i_batch, sample_batched in enumerate(dataloader):\n","        #print(\"abans images\")\n","        images_ = sample_batched['img'].to(device = dev, dtype=torch.float)\n","        #print(\"abans ground\")\n","        ground_ = sample_batched['gth'].to(device = dev, dtype=torch.float)\n","        n_iter = epoch*lensamples + i_batch\n","        \n","        #print(images_.shape)\n","\n","        #print(\"abans out\")\n","        output_ = model(images_)\n","        #print(\"despres out\")\n","        if n_iter%100==0:\n","            xi = vutils.make_grid(images_, normalize=True, scale_each=True)\n","            xg = vutils.make_grid(ground_,  normalize=True, scale_each=True)\n","            xo = vutils.make_grid(output_, normalize=True, scale_each=True)\n","            x = torch.cat((xi,xg,xo),1)\n","            writer.add_image('train/output'+ str(n_iter) + '_' + str(i_batch), x, n_iter)\n","            print('imatge guardada')\n","        #print(\"abans loss\")\n","        loss = criterion(output_, ground_)\n","        #print(\"despres loss\")\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        #print(\"xd\")\n","        writer.add_scalar('train/loss', loss.item(), n_iter)\n","        \n","        print('Train -> sample/numSamples/epoch: {0}/{1}/{2}, Loss: {3}' \\\n","              .format(i_batch, lensamples, epoch, loss.item()))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-11-22T10:11:00.759296Z","iopub.status.busy":"2021-11-22T10:11:00.758591Z"},"id":"okuaFLFbf3xh","outputId":"17e73fb2-2c2e-4962-ae2d-2785cf59d4d5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\ger-m\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"name":"stdout","output_type":"stream","text":["imatge guardada\n","Train -> sample/numSamples/epoch: 0/1182/0, Loss: 10306.4970703125\n","Train -> sample/numSamples/epoch: 1/1182/0, Loss: 8977.8564453125\n","Train -> sample/numSamples/epoch: 2/1182/0, Loss: 6124.06396484375\n","Train -> sample/numSamples/epoch: 3/1182/0, Loss: 6289.6318359375\n","Train -> sample/numSamples/epoch: 4/1182/0, Loss: 8589.8740234375\n","Train -> sample/numSamples/epoch: 5/1182/0, Loss: 6795.96826171875\n","Train -> sample/numSamples/epoch: 6/1182/0, Loss: 7742.0087890625\n","Train -> sample/numSamples/epoch: 7/1182/0, Loss: 5282.04931640625\n","Train -> sample/numSamples/epoch: 8/1182/0, Loss: 5179.2099609375\n","Train -> sample/numSamples/epoch: 9/1182/0, Loss: 3178.016357421875\n","Train -> sample/numSamples/epoch: 10/1182/0, Loss: 3344.745361328125\n","Train -> sample/numSamples/epoch: 11/1182/0, Loss: 1920.812255859375\n","Train -> sample/numSamples/epoch: 12/1182/0, Loss: 2419.204345703125\n","Train -> sample/numSamples/epoch: 13/1182/0, Loss: 3743.239013671875\n","Train -> sample/numSamples/epoch: 14/1182/0, Loss: 2247.029296875\n","Train -> sample/numSamples/epoch: 15/1182/0, Loss: 2147.410400390625\n","Train -> sample/numSamples/epoch: 16/1182/0, Loss: 1738.061279296875\n","Train -> sample/numSamples/epoch: 17/1182/0, Loss: 3544.916015625\n","Train -> sample/numSamples/epoch: 18/1182/0, Loss: 2570.04833984375\n","Train -> sample/numSamples/epoch: 19/1182/0, Loss: 2114.94775390625\n","Train -> sample/numSamples/epoch: 20/1182/0, Loss: 2375.276123046875\n","Train -> sample/numSamples/epoch: 21/1182/0, Loss: 3083.282958984375\n","Train -> sample/numSamples/epoch: 22/1182/0, Loss: 2315.819091796875\n","Train -> sample/numSamples/epoch: 23/1182/0, Loss: 1932.7801513671875\n","Train -> sample/numSamples/epoch: 24/1182/0, Loss: 1737.241455078125\n","Train -> sample/numSamples/epoch: 25/1182/0, Loss: 2244.08349609375\n","Train -> sample/numSamples/epoch: 26/1182/0, Loss: 1844.40283203125\n","Train -> sample/numSamples/epoch: 27/1182/0, Loss: 1470.8980712890625\n","Train -> sample/numSamples/epoch: 28/1182/0, Loss: 1577.3360595703125\n","Train -> sample/numSamples/epoch: 29/1182/0, Loss: 2101.780517578125\n","Train -> sample/numSamples/epoch: 30/1182/0, Loss: 2656.42822265625\n","Train -> sample/numSamples/epoch: 31/1182/0, Loss: 2743.826171875\n","Train -> sample/numSamples/epoch: 32/1182/0, Loss: 1778.4229736328125\n","Train -> sample/numSamples/epoch: 33/1182/0, Loss: 2334.057861328125\n","Train -> sample/numSamples/epoch: 34/1182/0, Loss: 1822.4930419921875\n","Train -> sample/numSamples/epoch: 35/1182/0, Loss: 1839.435302734375\n","Train -> sample/numSamples/epoch: 36/1182/0, Loss: 1453.9306640625\n","Train -> sample/numSamples/epoch: 37/1182/0, Loss: 1639.0667724609375\n","Train -> sample/numSamples/epoch: 38/1182/0, Loss: 1660.963134765625\n","Train -> sample/numSamples/epoch: 39/1182/0, Loss: 1492.93798828125\n","Train -> sample/numSamples/epoch: 40/1182/0, Loss: 1440.34619140625\n","Train -> sample/numSamples/epoch: 41/1182/0, Loss: 1642.7115478515625\n","Train -> sample/numSamples/epoch: 42/1182/0, Loss: 1332.1988525390625\n","Train -> sample/numSamples/epoch: 43/1182/0, Loss: 1595.4178466796875\n","Train -> sample/numSamples/epoch: 44/1182/0, Loss: 1416.850341796875\n","Train -> sample/numSamples/epoch: 45/1182/0, Loss: 1512.17041015625\n","Train -> sample/numSamples/epoch: 46/1182/0, Loss: 1597.458984375\n","Train -> sample/numSamples/epoch: 47/1182/0, Loss: 1751.31884765625\n","Train -> sample/numSamples/epoch: 48/1182/0, Loss: 1369.5113525390625\n","Train -> sample/numSamples/epoch: 49/1182/0, Loss: 1566.474365234375\n","Train -> sample/numSamples/epoch: 50/1182/0, Loss: 1133.4710693359375\n","Train -> sample/numSamples/epoch: 51/1182/0, Loss: 1278.0115966796875\n","Train -> sample/numSamples/epoch: 52/1182/0, Loss: 1525.888671875\n","Train -> sample/numSamples/epoch: 53/1182/0, Loss: 1139.2877197265625\n","Train -> sample/numSamples/epoch: 54/1182/0, Loss: 1360.75634765625\n","Train -> sample/numSamples/epoch: 55/1182/0, Loss: 1334.5194091796875\n","Train -> sample/numSamples/epoch: 56/1182/0, Loss: 1392.480712890625\n","Train -> sample/numSamples/epoch: 57/1182/0, Loss: 1200.93310546875\n","Train -> sample/numSamples/epoch: 58/1182/0, Loss: 1457.5037841796875\n","Train -> sample/numSamples/epoch: 59/1182/0, Loss: 1209.3111572265625\n","Train -> sample/numSamples/epoch: 60/1182/0, Loss: 1190.2093505859375\n","Train -> sample/numSamples/epoch: 61/1182/0, Loss: 1593.349609375\n","Train -> sample/numSamples/epoch: 62/1182/0, Loss: 1075.404541015625\n","Train -> sample/numSamples/epoch: 63/1182/0, Loss: 1408.780517578125\n","Train -> sample/numSamples/epoch: 64/1182/0, Loss: 1662.3402099609375\n","Train -> sample/numSamples/epoch: 65/1182/0, Loss: 1330.40869140625\n","Train -> sample/numSamples/epoch: 66/1182/0, Loss: 1242.15380859375\n","Train -> sample/numSamples/epoch: 67/1182/0, Loss: 882.2734375\n","Train -> sample/numSamples/epoch: 68/1182/0, Loss: 1413.633056640625\n","Train -> sample/numSamples/epoch: 69/1182/0, Loss: 1122.7283935546875\n","Train -> sample/numSamples/epoch: 70/1182/0, Loss: 1242.6876220703125\n","Train -> sample/numSamples/epoch: 71/1182/0, Loss: 1919.706787109375\n","Train -> sample/numSamples/epoch: 72/1182/0, Loss: 1232.037353515625\n","Train -> sample/numSamples/epoch: 73/1182/0, Loss: 1358.3927001953125\n","Train -> sample/numSamples/epoch: 74/1182/0, Loss: 1104.0936279296875\n","Train -> sample/numSamples/epoch: 75/1182/0, Loss: 1663.2244873046875\n","Train -> sample/numSamples/epoch: 76/1182/0, Loss: 1253.2935791015625\n","Train -> sample/numSamples/epoch: 77/1182/0, Loss: 1296.6876220703125\n","Train -> sample/numSamples/epoch: 78/1182/0, Loss: 1198.861328125\n","Train -> sample/numSamples/epoch: 79/1182/0, Loss: 1233.178466796875\n","Train -> sample/numSamples/epoch: 80/1182/0, Loss: 969.3311157226562\n","Train -> sample/numSamples/epoch: 81/1182/0, Loss: 1067.17041015625\n","Train -> sample/numSamples/epoch: 82/1182/0, Loss: 1140.808837890625\n","Train -> sample/numSamples/epoch: 83/1182/0, Loss: 1495.80859375\n","Train -> sample/numSamples/epoch: 84/1182/0, Loss: 1890.4481201171875\n","Train -> sample/numSamples/epoch: 85/1182/0, Loss: 1015.61376953125\n","Train -> sample/numSamples/epoch: 86/1182/0, Loss: 1308.9849853515625\n","Train -> sample/numSamples/epoch: 87/1182/0, Loss: 1363.1175537109375\n","Train -> sample/numSamples/epoch: 88/1182/0, Loss: 1252.592041015625\n","Train -> sample/numSamples/epoch: 89/1182/0, Loss: 1271.5113525390625\n","Train -> sample/numSamples/epoch: 90/1182/0, Loss: 999.0728149414062\n","Train -> sample/numSamples/epoch: 91/1182/0, Loss: 954.7620849609375\n","Train -> sample/numSamples/epoch: 92/1182/0, Loss: 1020.2161254882812\n","Train -> sample/numSamples/epoch: 93/1182/0, Loss: 1002.7875366210938\n","Train -> sample/numSamples/epoch: 94/1182/0, Loss: 1371.23876953125\n","Train -> sample/numSamples/epoch: 95/1182/0, Loss: 768.11083984375\n","Train -> sample/numSamples/epoch: 96/1182/0, Loss: 924.3499755859375\n","Train -> sample/numSamples/epoch: 97/1182/0, Loss: 1217.1363525390625\n","Train -> sample/numSamples/epoch: 98/1182/0, Loss: 1517.77685546875\n","Train -> sample/numSamples/epoch: 99/1182/0, Loss: 1397.837890625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 100/1182/0, Loss: 1267.8614501953125\n","Train -> sample/numSamples/epoch: 101/1182/0, Loss: 1577.083984375\n","Train -> sample/numSamples/epoch: 102/1182/0, Loss: 1061.87939453125\n","Train -> sample/numSamples/epoch: 103/1182/0, Loss: 1254.261962890625\n","Train -> sample/numSamples/epoch: 104/1182/0, Loss: 1054.1146240234375\n","Train -> sample/numSamples/epoch: 105/1182/0, Loss: 817.2205810546875\n","Train -> sample/numSamples/epoch: 106/1182/0, Loss: 952.6348266601562\n","Train -> sample/numSamples/epoch: 107/1182/0, Loss: 1749.266357421875\n","Train -> sample/numSamples/epoch: 108/1182/0, Loss: 694.4410400390625\n","Train -> sample/numSamples/epoch: 109/1182/0, Loss: 1166.269775390625\n","Train -> sample/numSamples/epoch: 110/1182/0, Loss: 1082.8980712890625\n","Train -> sample/numSamples/epoch: 111/1182/0, Loss: 1537.5784912109375\n","Train -> sample/numSamples/epoch: 112/1182/0, Loss: 1012.4259643554688\n","Train -> sample/numSamples/epoch: 113/1182/0, Loss: 1237.2509765625\n","Train -> sample/numSamples/epoch: 114/1182/0, Loss: 1184.0723876953125\n","Train -> sample/numSamples/epoch: 115/1182/0, Loss: 1346.88720703125\n","Train -> sample/numSamples/epoch: 116/1182/0, Loss: 1109.0872802734375\n","Train -> sample/numSamples/epoch: 117/1182/0, Loss: 968.275390625\n","Train -> sample/numSamples/epoch: 118/1182/0, Loss: 1563.9212646484375\n","Train -> sample/numSamples/epoch: 119/1182/0, Loss: 1422.832763671875\n","Train -> sample/numSamples/epoch: 120/1182/0, Loss: 1542.19580078125\n","Train -> sample/numSamples/epoch: 121/1182/0, Loss: 1386.75537109375\n","Train -> sample/numSamples/epoch: 122/1182/0, Loss: 969.8438110351562\n","Train -> sample/numSamples/epoch: 123/1182/0, Loss: 1020.487548828125\n","Train -> sample/numSamples/epoch: 124/1182/0, Loss: 1511.2869873046875\n","Train -> sample/numSamples/epoch: 125/1182/0, Loss: 1508.3082275390625\n","Train -> sample/numSamples/epoch: 126/1182/0, Loss: 1110.688232421875\n","Train -> sample/numSamples/epoch: 127/1182/0, Loss: 715.9109497070312\n","Train -> sample/numSamples/epoch: 128/1182/0, Loss: 1541.9896240234375\n","Train -> sample/numSamples/epoch: 129/1182/0, Loss: 1270.5509033203125\n","Train -> sample/numSamples/epoch: 130/1182/0, Loss: 1095.8145751953125\n","Train -> sample/numSamples/epoch: 131/1182/0, Loss: 1311.1661376953125\n","Train -> sample/numSamples/epoch: 132/1182/0, Loss: 1183.63134765625\n","Train -> sample/numSamples/epoch: 133/1182/0, Loss: 1242.9244384765625\n","Train -> sample/numSamples/epoch: 134/1182/0, Loss: 1170.202880859375\n","Train -> sample/numSamples/epoch: 135/1182/0, Loss: 1047.6368408203125\n","Train -> sample/numSamples/epoch: 136/1182/0, Loss: 815.0291137695312\n","Train -> sample/numSamples/epoch: 137/1182/0, Loss: 1230.3662109375\n","Train -> sample/numSamples/epoch: 138/1182/0, Loss: 1152.752685546875\n","Train -> sample/numSamples/epoch: 139/1182/0, Loss: 1177.121826171875\n","Train -> sample/numSamples/epoch: 140/1182/0, Loss: 1046.328125\n","Train -> sample/numSamples/epoch: 141/1182/0, Loss: 1253.6658935546875\n","Train -> sample/numSamples/epoch: 142/1182/0, Loss: 1471.730224609375\n","Train -> sample/numSamples/epoch: 143/1182/0, Loss: 1273.9434814453125\n","Train -> sample/numSamples/epoch: 144/1182/0, Loss: 1301.8896484375\n","Train -> sample/numSamples/epoch: 145/1182/0, Loss: 863.6126098632812\n","Train -> sample/numSamples/epoch: 146/1182/0, Loss: 1127.1226806640625\n","Train -> sample/numSamples/epoch: 147/1182/0, Loss: 1351.5360107421875\n","Train -> sample/numSamples/epoch: 148/1182/0, Loss: 1031.03076171875\n","Train -> sample/numSamples/epoch: 149/1182/0, Loss: 1630.22607421875\n","Train -> sample/numSamples/epoch: 150/1182/0, Loss: 1441.9879150390625\n","Train -> sample/numSamples/epoch: 151/1182/0, Loss: 920.1893920898438\n","Train -> sample/numSamples/epoch: 152/1182/0, Loss: 930.9381103515625\n","Train -> sample/numSamples/epoch: 153/1182/0, Loss: 867.1671752929688\n","Train -> sample/numSamples/epoch: 154/1182/0, Loss: 1078.2159423828125\n","Train -> sample/numSamples/epoch: 155/1182/0, Loss: 1061.12158203125\n","Train -> sample/numSamples/epoch: 156/1182/0, Loss: 797.8504638671875\n","Train -> sample/numSamples/epoch: 157/1182/0, Loss: 1196.78466796875\n","Train -> sample/numSamples/epoch: 158/1182/0, Loss: 909.3840942382812\n","Train -> sample/numSamples/epoch: 159/1182/0, Loss: 974.9288940429688\n","Train -> sample/numSamples/epoch: 160/1182/0, Loss: 1416.947998046875\n","Train -> sample/numSamples/epoch: 161/1182/0, Loss: 944.8289184570312\n","Train -> sample/numSamples/epoch: 162/1182/0, Loss: 901.2554321289062\n","Train -> sample/numSamples/epoch: 163/1182/0, Loss: 983.9779052734375\n","Train -> sample/numSamples/epoch: 164/1182/0, Loss: 872.1464233398438\n","Train -> sample/numSamples/epoch: 165/1182/0, Loss: 955.8748168945312\n","Train -> sample/numSamples/epoch: 166/1182/0, Loss: 1159.29736328125\n","Train -> sample/numSamples/epoch: 167/1182/0, Loss: 848.8298950195312\n","Train -> sample/numSamples/epoch: 168/1182/0, Loss: 996.2007446289062\n","Train -> sample/numSamples/epoch: 169/1182/0, Loss: 1463.9698486328125\n","Train -> sample/numSamples/epoch: 170/1182/0, Loss: 1084.669189453125\n","Train -> sample/numSamples/epoch: 171/1182/0, Loss: 1027.241455078125\n","Train -> sample/numSamples/epoch: 172/1182/0, Loss: 1158.2969970703125\n","Train -> sample/numSamples/epoch: 173/1182/0, Loss: 1041.9207763671875\n","Train -> sample/numSamples/epoch: 174/1182/0, Loss: 1330.003173828125\n","Train -> sample/numSamples/epoch: 175/1182/0, Loss: 1145.325439453125\n","Train -> sample/numSamples/epoch: 176/1182/0, Loss: 949.5712280273438\n","Train -> sample/numSamples/epoch: 177/1182/0, Loss: 1168.0552978515625\n","Train -> sample/numSamples/epoch: 178/1182/0, Loss: 1115.3533935546875\n","Train -> sample/numSamples/epoch: 179/1182/0, Loss: 1196.0870361328125\n","Train -> sample/numSamples/epoch: 180/1182/0, Loss: 1350.797119140625\n","Train -> sample/numSamples/epoch: 181/1182/0, Loss: 1414.5084228515625\n","Train -> sample/numSamples/epoch: 182/1182/0, Loss: 1322.187744140625\n","Train -> sample/numSamples/epoch: 183/1182/0, Loss: 1664.8536376953125\n","Train -> sample/numSamples/epoch: 184/1182/0, Loss: 1373.34326171875\n","Train -> sample/numSamples/epoch: 185/1182/0, Loss: 1305.0543212890625\n","Train -> sample/numSamples/epoch: 186/1182/0, Loss: 1030.828125\n","Train -> sample/numSamples/epoch: 187/1182/0, Loss: 1092.8033447265625\n","Train -> sample/numSamples/epoch: 188/1182/0, Loss: 809.2730712890625\n","Train -> sample/numSamples/epoch: 189/1182/0, Loss: 1158.4407958984375\n","Train -> sample/numSamples/epoch: 190/1182/0, Loss: 1269.9293212890625\n","Train -> sample/numSamples/epoch: 191/1182/0, Loss: 1079.3370361328125\n","Train -> sample/numSamples/epoch: 192/1182/0, Loss: 864.8214721679688\n","Train -> sample/numSamples/epoch: 193/1182/0, Loss: 1222.1729736328125\n","Train -> sample/numSamples/epoch: 194/1182/0, Loss: 984.0494384765625\n","Train -> sample/numSamples/epoch: 195/1182/0, Loss: 903.9173583984375\n","Train -> sample/numSamples/epoch: 196/1182/0, Loss: 764.736083984375\n","Train -> sample/numSamples/epoch: 197/1182/0, Loss: 1024.6680908203125\n","Train -> sample/numSamples/epoch: 198/1182/0, Loss: 986.970458984375\n","Train -> sample/numSamples/epoch: 199/1182/0, Loss: 619.7167358398438\n","imatge guardada\n","Train -> sample/numSamples/epoch: 200/1182/0, Loss: 788.6378173828125\n","Train -> sample/numSamples/epoch: 201/1182/0, Loss: 1754.9713134765625\n","Train -> sample/numSamples/epoch: 202/1182/0, Loss: 1416.34765625\n","Train -> sample/numSamples/epoch: 203/1182/0, Loss: 1264.302978515625\n","Train -> sample/numSamples/epoch: 204/1182/0, Loss: 861.3884887695312\n","Train -> sample/numSamples/epoch: 205/1182/0, Loss: 985.7313842773438\n","Train -> sample/numSamples/epoch: 206/1182/0, Loss: 714.3934936523438\n","Train -> sample/numSamples/epoch: 207/1182/0, Loss: 1283.7181396484375\n","Train -> sample/numSamples/epoch: 208/1182/0, Loss: 927.0906982421875\n","Train -> sample/numSamples/epoch: 209/1182/0, Loss: 1492.404052734375\n","Train -> sample/numSamples/epoch: 210/1182/0, Loss: 1543.239013671875\n","Train -> sample/numSamples/epoch: 211/1182/0, Loss: 1413.01513671875\n","Train -> sample/numSamples/epoch: 212/1182/0, Loss: 984.12060546875\n","Train -> sample/numSamples/epoch: 213/1182/0, Loss: 1447.8778076171875\n","Train -> sample/numSamples/epoch: 214/1182/0, Loss: 1347.5888671875\n","Train -> sample/numSamples/epoch: 215/1182/0, Loss: 1253.03955078125\n","Train -> sample/numSamples/epoch: 216/1182/0, Loss: 1001.39794921875\n","Train -> sample/numSamples/epoch: 217/1182/0, Loss: 951.3695068359375\n","Train -> sample/numSamples/epoch: 218/1182/0, Loss: 956.982177734375\n","Train -> sample/numSamples/epoch: 219/1182/0, Loss: 1134.3492431640625\n","Train -> sample/numSamples/epoch: 220/1182/0, Loss: 1186.607421875\n","Train -> sample/numSamples/epoch: 221/1182/0, Loss: 970.2628173828125\n","Train -> sample/numSamples/epoch: 222/1182/0, Loss: 1081.11328125\n","Train -> sample/numSamples/epoch: 223/1182/0, Loss: 1132.5372314453125\n","Train -> sample/numSamples/epoch: 224/1182/0, Loss: 1275.7093505859375\n","Train -> sample/numSamples/epoch: 225/1182/0, Loss: 1204.4891357421875\n","Train -> sample/numSamples/epoch: 226/1182/0, Loss: 956.3207397460938\n","Train -> sample/numSamples/epoch: 227/1182/0, Loss: 992.3180541992188\n","Train -> sample/numSamples/epoch: 228/1182/0, Loss: 630.9517822265625\n","Train -> sample/numSamples/epoch: 229/1182/0, Loss: 1602.8536376953125\n","Train -> sample/numSamples/epoch: 230/1182/0, Loss: 1218.955322265625\n","Train -> sample/numSamples/epoch: 231/1182/0, Loss: 599.696533203125\n","Train -> sample/numSamples/epoch: 232/1182/0, Loss: 1043.28857421875\n","Train -> sample/numSamples/epoch: 233/1182/0, Loss: 1205.04541015625\n","Train -> sample/numSamples/epoch: 234/1182/0, Loss: 614.534423828125\n","Train -> sample/numSamples/epoch: 235/1182/0, Loss: 1668.8023681640625\n","Train -> sample/numSamples/epoch: 236/1182/0, Loss: 1059.255859375\n","Train -> sample/numSamples/epoch: 237/1182/0, Loss: 1113.6846923828125\n","Train -> sample/numSamples/epoch: 238/1182/0, Loss: 2000.966796875\n","Train -> sample/numSamples/epoch: 239/1182/0, Loss: 1040.9495849609375\n","Train -> sample/numSamples/epoch: 240/1182/0, Loss: 1056.0076904296875\n","Train -> sample/numSamples/epoch: 241/1182/0, Loss: 1014.2388916015625\n","Train -> sample/numSamples/epoch: 242/1182/0, Loss: 1477.586181640625\n","Train -> sample/numSamples/epoch: 243/1182/0, Loss: 730.30712890625\n","Train -> sample/numSamples/epoch: 244/1182/0, Loss: 1357.524169921875\n","Train -> sample/numSamples/epoch: 245/1182/0, Loss: 1623.9603271484375\n","Train -> sample/numSamples/epoch: 246/1182/0, Loss: 1077.2279052734375\n","Train -> sample/numSamples/epoch: 247/1182/0, Loss: 1612.216796875\n","Train -> sample/numSamples/epoch: 248/1182/0, Loss: 1018.3306884765625\n","Train -> sample/numSamples/epoch: 249/1182/0, Loss: 942.6433715820312\n","Train -> sample/numSamples/epoch: 250/1182/0, Loss: 1191.0859375\n","Train -> sample/numSamples/epoch: 251/1182/0, Loss: 1185.835693359375\n","Train -> sample/numSamples/epoch: 252/1182/0, Loss: 1296.173095703125\n","Train -> sample/numSamples/epoch: 253/1182/0, Loss: 1315.890625\n","Train -> sample/numSamples/epoch: 254/1182/0, Loss: 1436.1846923828125\n","Train -> sample/numSamples/epoch: 255/1182/0, Loss: 752.4129638671875\n","Train -> sample/numSamples/epoch: 256/1182/0, Loss: 1194.62158203125\n","Train -> sample/numSamples/epoch: 257/1182/0, Loss: 916.8496704101562\n","Train -> sample/numSamples/epoch: 258/1182/0, Loss: 1672.9261474609375\n","Train -> sample/numSamples/epoch: 259/1182/0, Loss: 1088.9449462890625\n","Train -> sample/numSamples/epoch: 260/1182/0, Loss: 1000.8148803710938\n","Train -> sample/numSamples/epoch: 261/1182/0, Loss: 1461.694091796875\n","Train -> sample/numSamples/epoch: 262/1182/0, Loss: 879.2184448242188\n","Train -> sample/numSamples/epoch: 263/1182/0, Loss: 1503.3829345703125\n","Train -> sample/numSamples/epoch: 264/1182/0, Loss: 982.1575317382812\n","Train -> sample/numSamples/epoch: 265/1182/0, Loss: 1427.037353515625\n","Train -> sample/numSamples/epoch: 266/1182/0, Loss: 998.9022216796875\n","Train -> sample/numSamples/epoch: 267/1182/0, Loss: 934.3031616210938\n","Train -> sample/numSamples/epoch: 268/1182/0, Loss: 1302.71337890625\n","Train -> sample/numSamples/epoch: 269/1182/0, Loss: 1263.2216796875\n","Train -> sample/numSamples/epoch: 270/1182/0, Loss: 1322.2532958984375\n","Train -> sample/numSamples/epoch: 271/1182/0, Loss: 1103.7489013671875\n","Train -> sample/numSamples/epoch: 272/1182/0, Loss: 1051.38818359375\n","Train -> sample/numSamples/epoch: 273/1182/0, Loss: 907.5369262695312\n","Train -> sample/numSamples/epoch: 274/1182/0, Loss: 1057.636474609375\n","Train -> sample/numSamples/epoch: 275/1182/0, Loss: 1318.343017578125\n","Train -> sample/numSamples/epoch: 276/1182/0, Loss: 923.3421630859375\n","Train -> sample/numSamples/epoch: 277/1182/0, Loss: 860.9554443359375\n","Train -> sample/numSamples/epoch: 278/1182/0, Loss: 1080.15673828125\n","Train -> sample/numSamples/epoch: 279/1182/0, Loss: 1460.7711181640625\n","Train -> sample/numSamples/epoch: 280/1182/0, Loss: 1395.4447021484375\n","Train -> sample/numSamples/epoch: 281/1182/0, Loss: 919.4641723632812\n","Train -> sample/numSamples/epoch: 282/1182/0, Loss: 867.9884643554688\n","Train -> sample/numSamples/epoch: 283/1182/0, Loss: 1044.7239990234375\n","Train -> sample/numSamples/epoch: 284/1182/0, Loss: 972.2435913085938\n","Train -> sample/numSamples/epoch: 285/1182/0, Loss: 954.8564453125\n","Train -> sample/numSamples/epoch: 286/1182/0, Loss: 1552.4349365234375\n","Train -> sample/numSamples/epoch: 287/1182/0, Loss: 1434.5548095703125\n","Train -> sample/numSamples/epoch: 288/1182/0, Loss: 1247.5799560546875\n","Train -> sample/numSamples/epoch: 289/1182/0, Loss: 1446.8338623046875\n","Train -> sample/numSamples/epoch: 290/1182/0, Loss: 1088.2025146484375\n","Train -> sample/numSamples/epoch: 291/1182/0, Loss: 965.547119140625\n","Train -> sample/numSamples/epoch: 292/1182/0, Loss: 1271.8636474609375\n","Train -> sample/numSamples/epoch: 293/1182/0, Loss: 983.4646606445312\n","Train -> sample/numSamples/epoch: 294/1182/0, Loss: 982.5086059570312\n","Train -> sample/numSamples/epoch: 295/1182/0, Loss: 1192.82177734375\n","Train -> sample/numSamples/epoch: 296/1182/0, Loss: 1033.15625\n","Train -> sample/numSamples/epoch: 297/1182/0, Loss: 663.6553344726562\n","Train -> sample/numSamples/epoch: 298/1182/0, Loss: 580.4380493164062\n","Train -> sample/numSamples/epoch: 299/1182/0, Loss: 966.4924926757812\n","imatge guardada\n","Train -> sample/numSamples/epoch: 300/1182/0, Loss: 707.860595703125\n","Train -> sample/numSamples/epoch: 301/1182/0, Loss: 1153.2061767578125\n","Train -> sample/numSamples/epoch: 302/1182/0, Loss: 1478.2626953125\n","Train -> sample/numSamples/epoch: 303/1182/0, Loss: 1157.86181640625\n","Train -> sample/numSamples/epoch: 304/1182/0, Loss: 1586.80908203125\n","Train -> sample/numSamples/epoch: 305/1182/0, Loss: 1141.1044921875\n","Train -> sample/numSamples/epoch: 306/1182/0, Loss: 924.3244018554688\n","Train -> sample/numSamples/epoch: 307/1182/0, Loss: 1382.198486328125\n","Train -> sample/numSamples/epoch: 308/1182/0, Loss: 886.269287109375\n","Train -> sample/numSamples/epoch: 309/1182/0, Loss: 1136.455078125\n","Train -> sample/numSamples/epoch: 310/1182/0, Loss: 795.2898559570312\n","Train -> sample/numSamples/epoch: 311/1182/0, Loss: 954.5394897460938\n","Train -> sample/numSamples/epoch: 312/1182/0, Loss: 1676.345458984375\n","Train -> sample/numSamples/epoch: 313/1182/0, Loss: 1204.9444580078125\n","Train -> sample/numSamples/epoch: 314/1182/0, Loss: 1057.287109375\n","Train -> sample/numSamples/epoch: 315/1182/0, Loss: 1302.8797607421875\n","Train -> sample/numSamples/epoch: 316/1182/0, Loss: 737.2205810546875\n","Train -> sample/numSamples/epoch: 317/1182/0, Loss: 789.5908203125\n","Train -> sample/numSamples/epoch: 318/1182/0, Loss: 1341.0604248046875\n","Train -> sample/numSamples/epoch: 319/1182/0, Loss: 637.5501708984375\n","Train -> sample/numSamples/epoch: 320/1182/0, Loss: 814.3724975585938\n","Train -> sample/numSamples/epoch: 321/1182/0, Loss: 857.66162109375\n","Train -> sample/numSamples/epoch: 322/1182/0, Loss: 1136.0340576171875\n","Train -> sample/numSamples/epoch: 323/1182/0, Loss: 966.1527099609375\n","Train -> sample/numSamples/epoch: 324/1182/0, Loss: 1191.6790771484375\n","Train -> sample/numSamples/epoch: 325/1182/0, Loss: 1181.927734375\n","Train -> sample/numSamples/epoch: 326/1182/0, Loss: 1198.6279296875\n","Train -> sample/numSamples/epoch: 327/1182/0, Loss: 1293.568115234375\n","Train -> sample/numSamples/epoch: 328/1182/0, Loss: 1722.19775390625\n","Train -> sample/numSamples/epoch: 329/1182/0, Loss: 476.5957946777344\n","Train -> sample/numSamples/epoch: 330/1182/0, Loss: 642.8743896484375\n","Train -> sample/numSamples/epoch: 331/1182/0, Loss: 1001.7293090820312\n","Train -> sample/numSamples/epoch: 332/1182/0, Loss: 966.2727661132812\n","Train -> sample/numSamples/epoch: 333/1182/0, Loss: 1002.6207275390625\n","Train -> sample/numSamples/epoch: 334/1182/0, Loss: 1305.4716796875\n","Train -> sample/numSamples/epoch: 335/1182/0, Loss: 1151.1839599609375\n","Train -> sample/numSamples/epoch: 336/1182/0, Loss: 718.1199340820312\n","Train -> sample/numSamples/epoch: 337/1182/0, Loss: 1002.521484375\n","Train -> sample/numSamples/epoch: 338/1182/0, Loss: 1381.4918212890625\n","Train -> sample/numSamples/epoch: 339/1182/0, Loss: 1593.8284912109375\n","Train -> sample/numSamples/epoch: 340/1182/0, Loss: 1096.65087890625\n","Train -> sample/numSamples/epoch: 341/1182/0, Loss: 1300.315185546875\n","Train -> sample/numSamples/epoch: 342/1182/0, Loss: 832.4707641601562\n","Train -> sample/numSamples/epoch: 343/1182/0, Loss: 1187.97216796875\n","Train -> sample/numSamples/epoch: 344/1182/0, Loss: 1678.2642822265625\n","Train -> sample/numSamples/epoch: 345/1182/0, Loss: 1143.6031494140625\n","Train -> sample/numSamples/epoch: 346/1182/0, Loss: 1552.3612060546875\n","Train -> sample/numSamples/epoch: 347/1182/0, Loss: 1443.7109375\n","Train -> sample/numSamples/epoch: 348/1182/0, Loss: 1279.0120849609375\n","Train -> sample/numSamples/epoch: 349/1182/0, Loss: 681.4529418945312\n","Train -> sample/numSamples/epoch: 350/1182/0, Loss: 1388.5775146484375\n","Train -> sample/numSamples/epoch: 351/1182/0, Loss: 1084.4749755859375\n","Train -> sample/numSamples/epoch: 352/1182/0, Loss: 457.1514587402344\n","Train -> sample/numSamples/epoch: 353/1182/0, Loss: 846.3692626953125\n","Train -> sample/numSamples/epoch: 354/1182/0, Loss: 1390.7503662109375\n","Train -> sample/numSamples/epoch: 355/1182/0, Loss: 1288.51708984375\n","Train -> sample/numSamples/epoch: 356/1182/0, Loss: 828.0438232421875\n","Train -> sample/numSamples/epoch: 357/1182/0, Loss: 1119.8548583984375\n","Train -> sample/numSamples/epoch: 358/1182/0, Loss: 1031.319580078125\n","Train -> sample/numSamples/epoch: 359/1182/0, Loss: 1197.34423828125\n","Train -> sample/numSamples/epoch: 360/1182/0, Loss: 1349.5509033203125\n","Train -> sample/numSamples/epoch: 361/1182/0, Loss: 1369.809814453125\n","Train -> sample/numSamples/epoch: 362/1182/0, Loss: 1345.30419921875\n","Train -> sample/numSamples/epoch: 363/1182/0, Loss: 1288.3131103515625\n","Train -> sample/numSamples/epoch: 364/1182/0, Loss: 905.9020385742188\n","Train -> sample/numSamples/epoch: 365/1182/0, Loss: 736.52197265625\n","Train -> sample/numSamples/epoch: 366/1182/0, Loss: 1129.888671875\n","Train -> sample/numSamples/epoch: 367/1182/0, Loss: 870.4093017578125\n","Train -> sample/numSamples/epoch: 368/1182/0, Loss: 1173.5919189453125\n","Train -> sample/numSamples/epoch: 369/1182/0, Loss: 1068.3187255859375\n","Train -> sample/numSamples/epoch: 370/1182/0, Loss: 913.5599365234375\n","Train -> sample/numSamples/epoch: 371/1182/0, Loss: 694.2061767578125\n","Train -> sample/numSamples/epoch: 372/1182/0, Loss: 1350.067626953125\n","Train -> sample/numSamples/epoch: 373/1182/0, Loss: 1088.73681640625\n","Train -> sample/numSamples/epoch: 374/1182/0, Loss: 954.5784301757812\n","Train -> sample/numSamples/epoch: 375/1182/0, Loss: 1143.253173828125\n","Train -> sample/numSamples/epoch: 376/1182/0, Loss: 1010.4671020507812\n","Train -> sample/numSamples/epoch: 377/1182/0, Loss: 1024.0118408203125\n","Train -> sample/numSamples/epoch: 378/1182/0, Loss: 1430.0341796875\n","Train -> sample/numSamples/epoch: 379/1182/0, Loss: 1863.41796875\n","Train -> sample/numSamples/epoch: 380/1182/0, Loss: 1503.25634765625\n","Train -> sample/numSamples/epoch: 381/1182/0, Loss: 1061.2169189453125\n","Train -> sample/numSamples/epoch: 382/1182/0, Loss: 1040.05126953125\n","Train -> sample/numSamples/epoch: 383/1182/0, Loss: 1189.8536376953125\n","Train -> sample/numSamples/epoch: 384/1182/0, Loss: 1187.526611328125\n","Train -> sample/numSamples/epoch: 385/1182/0, Loss: 1238.893798828125\n","Train -> sample/numSamples/epoch: 386/1182/0, Loss: 1159.6624755859375\n","Train -> sample/numSamples/epoch: 387/1182/0, Loss: 829.2283935546875\n","Train -> sample/numSamples/epoch: 388/1182/0, Loss: 813.748291015625\n","Train -> sample/numSamples/epoch: 389/1182/0, Loss: 1458.938720703125\n","Train -> sample/numSamples/epoch: 390/1182/0, Loss: 801.4727172851562\n","Train -> sample/numSamples/epoch: 391/1182/0, Loss: 1417.5291748046875\n","Train -> sample/numSamples/epoch: 392/1182/0, Loss: 1222.71630859375\n","Train -> sample/numSamples/epoch: 393/1182/0, Loss: 1430.1663818359375\n","Train -> sample/numSamples/epoch: 394/1182/0, Loss: 1227.2073974609375\n","Train -> sample/numSamples/epoch: 395/1182/0, Loss: 1000.1114501953125\n","Train -> sample/numSamples/epoch: 396/1182/0, Loss: 973.6006469726562\n","Train -> sample/numSamples/epoch: 397/1182/0, Loss: 1066.6400146484375\n","Train -> sample/numSamples/epoch: 398/1182/0, Loss: 1494.9425048828125\n","Train -> sample/numSamples/epoch: 399/1182/0, Loss: 1379.2708740234375\n","imatge guardada\n","Train -> sample/numSamples/epoch: 400/1182/0, Loss: 1068.0389404296875\n","Train -> sample/numSamples/epoch: 401/1182/0, Loss: 1080.824951171875\n","Train -> sample/numSamples/epoch: 402/1182/0, Loss: 1142.2811279296875\n","Train -> sample/numSamples/epoch: 403/1182/0, Loss: 1140.2376708984375\n","Train -> sample/numSamples/epoch: 404/1182/0, Loss: 876.1021728515625\n","Train -> sample/numSamples/epoch: 405/1182/0, Loss: 1403.3497314453125\n","Train -> sample/numSamples/epoch: 406/1182/0, Loss: 907.1202392578125\n","Train -> sample/numSamples/epoch: 407/1182/0, Loss: 1064.5716552734375\n","Train -> sample/numSamples/epoch: 408/1182/0, Loss: 1226.05322265625\n","Train -> sample/numSamples/epoch: 409/1182/0, Loss: 1327.9219970703125\n","Train -> sample/numSamples/epoch: 410/1182/0, Loss: 1267.2408447265625\n","Train -> sample/numSamples/epoch: 411/1182/0, Loss: 741.5631103515625\n","Train -> sample/numSamples/epoch: 412/1182/0, Loss: 950.8862915039062\n","Train -> sample/numSamples/epoch: 413/1182/0, Loss: 1101.1405029296875\n","Train -> sample/numSamples/epoch: 414/1182/0, Loss: 1410.631103515625\n","Train -> sample/numSamples/epoch: 415/1182/0, Loss: 965.0233154296875\n","Train -> sample/numSamples/epoch: 416/1182/0, Loss: 826.9426879882812\n","Train -> sample/numSamples/epoch: 417/1182/0, Loss: 1305.331298828125\n","Train -> sample/numSamples/epoch: 418/1182/0, Loss: 1011.2313232421875\n","Train -> sample/numSamples/epoch: 419/1182/0, Loss: 1483.5362548828125\n","Train -> sample/numSamples/epoch: 420/1182/0, Loss: 1641.9000244140625\n","Train -> sample/numSamples/epoch: 421/1182/0, Loss: 928.251708984375\n","Train -> sample/numSamples/epoch: 422/1182/0, Loss: 1371.3790283203125\n","Train -> sample/numSamples/epoch: 423/1182/0, Loss: 988.0615844726562\n","Train -> sample/numSamples/epoch: 424/1182/0, Loss: 807.2960815429688\n","Train -> sample/numSamples/epoch: 425/1182/0, Loss: 1035.7974853515625\n","Train -> sample/numSamples/epoch: 426/1182/0, Loss: 1299.2906494140625\n","Train -> sample/numSamples/epoch: 427/1182/0, Loss: 965.1334228515625\n","Train -> sample/numSamples/epoch: 428/1182/0, Loss: 1165.154052734375\n","Train -> sample/numSamples/epoch: 429/1182/0, Loss: 765.6538696289062\n","Train -> sample/numSamples/epoch: 430/1182/0, Loss: 865.3954467773438\n","Train -> sample/numSamples/epoch: 431/1182/0, Loss: 1301.1798095703125\n","Train -> sample/numSamples/epoch: 432/1182/0, Loss: 611.1477661132812\n","Train -> sample/numSamples/epoch: 433/1182/0, Loss: 958.8804321289062\n","Train -> sample/numSamples/epoch: 434/1182/0, Loss: 1008.5488891601562\n","Train -> sample/numSamples/epoch: 435/1182/0, Loss: 1467.430419921875\n","Train -> sample/numSamples/epoch: 436/1182/0, Loss: 1134.0150146484375\n","Train -> sample/numSamples/epoch: 437/1182/0, Loss: 1689.5655517578125\n","Train -> sample/numSamples/epoch: 438/1182/0, Loss: 678.7498779296875\n","Train -> sample/numSamples/epoch: 439/1182/0, Loss: 824.491455078125\n","Train -> sample/numSamples/epoch: 440/1182/0, Loss: 960.02587890625\n","Train -> sample/numSamples/epoch: 441/1182/0, Loss: 1374.9129638671875\n","Train -> sample/numSamples/epoch: 442/1182/0, Loss: 1114.6834716796875\n","Train -> sample/numSamples/epoch: 443/1182/0, Loss: 1049.8851318359375\n","Train -> sample/numSamples/epoch: 444/1182/0, Loss: 1083.6837158203125\n","Train -> sample/numSamples/epoch: 445/1182/0, Loss: 1142.152587890625\n","Train -> sample/numSamples/epoch: 446/1182/0, Loss: 1128.3555908203125\n","Train -> sample/numSamples/epoch: 447/1182/0, Loss: 940.5197143554688\n","Train -> sample/numSamples/epoch: 448/1182/0, Loss: 1081.81298828125\n","Train -> sample/numSamples/epoch: 449/1182/0, Loss: 825.932861328125\n","Train -> sample/numSamples/epoch: 450/1182/0, Loss: 914.0196533203125\n","Train -> sample/numSamples/epoch: 451/1182/0, Loss: 1198.7130126953125\n","Train -> sample/numSamples/epoch: 452/1182/0, Loss: 991.2985229492188\n","Train -> sample/numSamples/epoch: 453/1182/0, Loss: 1305.962646484375\n","Train -> sample/numSamples/epoch: 454/1182/0, Loss: 1584.9300537109375\n","Train -> sample/numSamples/epoch: 455/1182/0, Loss: 1045.78955078125\n","Train -> sample/numSamples/epoch: 456/1182/0, Loss: 1017.4195556640625\n","Train -> sample/numSamples/epoch: 457/1182/0, Loss: 940.5945434570312\n","Train -> sample/numSamples/epoch: 458/1182/0, Loss: 1131.7454833984375\n","Train -> sample/numSamples/epoch: 459/1182/0, Loss: 1161.945068359375\n","Train -> sample/numSamples/epoch: 460/1182/0, Loss: 1224.9920654296875\n","Train -> sample/numSamples/epoch: 461/1182/0, Loss: 1151.243408203125\n","Train -> sample/numSamples/epoch: 462/1182/0, Loss: 976.2700805664062\n","Train -> sample/numSamples/epoch: 463/1182/0, Loss: 1473.979248046875\n","Train -> sample/numSamples/epoch: 464/1182/0, Loss: 970.0031127929688\n","Train -> sample/numSamples/epoch: 465/1182/0, Loss: 732.6165161132812\n","Train -> sample/numSamples/epoch: 466/1182/0, Loss: 1419.8214111328125\n","Train -> sample/numSamples/epoch: 467/1182/0, Loss: 1298.3729248046875\n","Train -> sample/numSamples/epoch: 468/1182/0, Loss: 888.081787109375\n","Train -> sample/numSamples/epoch: 469/1182/0, Loss: 858.585693359375\n","Train -> sample/numSamples/epoch: 470/1182/0, Loss: 1174.7177734375\n","Train -> sample/numSamples/epoch: 471/1182/0, Loss: 1037.2022705078125\n","Train -> sample/numSamples/epoch: 472/1182/0, Loss: 1177.684814453125\n","Train -> sample/numSamples/epoch: 473/1182/0, Loss: 1152.1185302734375\n","Train -> sample/numSamples/epoch: 474/1182/0, Loss: 849.3637084960938\n","Train -> sample/numSamples/epoch: 475/1182/0, Loss: 1374.220458984375\n","Train -> sample/numSamples/epoch: 476/1182/0, Loss: 1155.0244140625\n","Train -> sample/numSamples/epoch: 477/1182/0, Loss: 839.149169921875\n","Train -> sample/numSamples/epoch: 478/1182/0, Loss: 811.0728759765625\n","Train -> sample/numSamples/epoch: 479/1182/0, Loss: 903.5715942382812\n","Train -> sample/numSamples/epoch: 480/1182/0, Loss: 1133.89111328125\n","Train -> sample/numSamples/epoch: 481/1182/0, Loss: 715.0157470703125\n","Train -> sample/numSamples/epoch: 482/1182/0, Loss: 1334.5716552734375\n","Train -> sample/numSamples/epoch: 483/1182/0, Loss: 1175.6300048828125\n","Train -> sample/numSamples/epoch: 484/1182/0, Loss: 1126.307861328125\n","Train -> sample/numSamples/epoch: 485/1182/0, Loss: 1016.33935546875\n","Train -> sample/numSamples/epoch: 486/1182/0, Loss: 1294.713134765625\n","Train -> sample/numSamples/epoch: 487/1182/0, Loss: 1609.9410400390625\n","Train -> sample/numSamples/epoch: 488/1182/0, Loss: 1318.3848876953125\n","Train -> sample/numSamples/epoch: 489/1182/0, Loss: 922.8077392578125\n","Train -> sample/numSamples/epoch: 490/1182/0, Loss: 637.7282104492188\n","Train -> sample/numSamples/epoch: 491/1182/0, Loss: 1073.643310546875\n","Train -> sample/numSamples/epoch: 492/1182/0, Loss: 1033.04052734375\n","Train -> sample/numSamples/epoch: 493/1182/0, Loss: 617.4344482421875\n","Train -> sample/numSamples/epoch: 494/1182/0, Loss: 1092.828857421875\n","Train -> sample/numSamples/epoch: 495/1182/0, Loss: 1014.6691284179688\n","Train -> sample/numSamples/epoch: 496/1182/0, Loss: 1029.3734130859375\n","Train -> sample/numSamples/epoch: 497/1182/0, Loss: 960.1132202148438\n","Train -> sample/numSamples/epoch: 498/1182/0, Loss: 1212.4700927734375\n","Train -> sample/numSamples/epoch: 499/1182/0, Loss: 863.3848266601562\n","imatge guardada\n","Train -> sample/numSamples/epoch: 500/1182/0, Loss: 1056.2684326171875\n","Train -> sample/numSamples/epoch: 501/1182/0, Loss: 1286.9771728515625\n","Train -> sample/numSamples/epoch: 502/1182/0, Loss: 1409.33251953125\n","Train -> sample/numSamples/epoch: 503/1182/0, Loss: 1251.1319580078125\n","Train -> sample/numSamples/epoch: 504/1182/0, Loss: 1068.7864990234375\n","Train -> sample/numSamples/epoch: 505/1182/0, Loss: 997.2243041992188\n","Train -> sample/numSamples/epoch: 506/1182/0, Loss: 708.6056518554688\n","Train -> sample/numSamples/epoch: 507/1182/0, Loss: 938.3016967773438\n","Train -> sample/numSamples/epoch: 508/1182/0, Loss: 1139.9522705078125\n","Train -> sample/numSamples/epoch: 509/1182/0, Loss: 1109.6920166015625\n","Train -> sample/numSamples/epoch: 510/1182/0, Loss: 1212.135986328125\n","Train -> sample/numSamples/epoch: 511/1182/0, Loss: 996.4588623046875\n","Train -> sample/numSamples/epoch: 512/1182/0, Loss: 1342.015625\n","Train -> sample/numSamples/epoch: 513/1182/0, Loss: 941.862060546875\n","Train -> sample/numSamples/epoch: 514/1182/0, Loss: 873.1393432617188\n","Train -> sample/numSamples/epoch: 515/1182/0, Loss: 1018.4300537109375\n","Train -> sample/numSamples/epoch: 516/1182/0, Loss: 1163.6199951171875\n","Train -> sample/numSamples/epoch: 517/1182/0, Loss: 1271.9193115234375\n","Train -> sample/numSamples/epoch: 518/1182/0, Loss: 874.4080810546875\n","Train -> sample/numSamples/epoch: 519/1182/0, Loss: 1093.349853515625\n","Train -> sample/numSamples/epoch: 520/1182/0, Loss: 1005.2681884765625\n","Train -> sample/numSamples/epoch: 521/1182/0, Loss: 1166.6368408203125\n","Train -> sample/numSamples/epoch: 522/1182/0, Loss: 825.0670166015625\n","Train -> sample/numSamples/epoch: 523/1182/0, Loss: 1252.6741943359375\n","Train -> sample/numSamples/epoch: 524/1182/0, Loss: 1321.5482177734375\n","Train -> sample/numSamples/epoch: 525/1182/0, Loss: 1011.4476928710938\n","Train -> sample/numSamples/epoch: 526/1182/0, Loss: 565.5619506835938\n","Train -> sample/numSamples/epoch: 527/1182/0, Loss: 1088.0712890625\n","Train -> sample/numSamples/epoch: 528/1182/0, Loss: 1396.9921875\n","Train -> sample/numSamples/epoch: 529/1182/0, Loss: 892.3949584960938\n","Train -> sample/numSamples/epoch: 530/1182/0, Loss: 1033.166259765625\n","Train -> sample/numSamples/epoch: 531/1182/0, Loss: 1302.2286376953125\n","Train -> sample/numSamples/epoch: 532/1182/0, Loss: 901.7470092773438\n","Train -> sample/numSamples/epoch: 533/1182/0, Loss: 1020.42333984375\n","Train -> sample/numSamples/epoch: 534/1182/0, Loss: 1315.30078125\n","Train -> sample/numSamples/epoch: 535/1182/0, Loss: 499.8089904785156\n","Train -> sample/numSamples/epoch: 536/1182/0, Loss: 1076.444580078125\n","Train -> sample/numSamples/epoch: 537/1182/0, Loss: 694.182373046875\n","Train -> sample/numSamples/epoch: 538/1182/0, Loss: 1213.1416015625\n","Train -> sample/numSamples/epoch: 539/1182/0, Loss: 1258.9486083984375\n","Train -> sample/numSamples/epoch: 540/1182/0, Loss: 1176.24072265625\n","Train -> sample/numSamples/epoch: 541/1182/0, Loss: 981.1373901367188\n","Train -> sample/numSamples/epoch: 542/1182/0, Loss: 1428.768310546875\n","Train -> sample/numSamples/epoch: 543/1182/0, Loss: 1013.1663208007812\n","Train -> sample/numSamples/epoch: 544/1182/0, Loss: 1107.400634765625\n","Train -> sample/numSamples/epoch: 545/1182/0, Loss: 1297.9459228515625\n","Train -> sample/numSamples/epoch: 546/1182/0, Loss: 779.7888793945312\n","Train -> sample/numSamples/epoch: 547/1182/0, Loss: 1101.7479248046875\n","Train -> sample/numSamples/epoch: 548/1182/0, Loss: 896.3092651367188\n","Train -> sample/numSamples/epoch: 549/1182/0, Loss: 1344.8536376953125\n","Train -> sample/numSamples/epoch: 550/1182/0, Loss: 687.4421997070312\n","Train -> sample/numSamples/epoch: 551/1182/0, Loss: 1009.4550170898438\n","Train -> sample/numSamples/epoch: 552/1182/0, Loss: 945.1141357421875\n","Train -> sample/numSamples/epoch: 553/1182/0, Loss: 804.330078125\n","Train -> sample/numSamples/epoch: 554/1182/0, Loss: 920.8726196289062\n","Train -> sample/numSamples/epoch: 555/1182/0, Loss: 894.1240234375\n","Train -> sample/numSamples/epoch: 556/1182/0, Loss: 1367.5694580078125\n","Train -> sample/numSamples/epoch: 557/1182/0, Loss: 1342.6064453125\n","Train -> sample/numSamples/epoch: 558/1182/0, Loss: 987.3060302734375\n","Train -> sample/numSamples/epoch: 559/1182/0, Loss: 1030.9639892578125\n","Train -> sample/numSamples/epoch: 560/1182/0, Loss: 1042.095458984375\n","Train -> sample/numSamples/epoch: 561/1182/0, Loss: 1344.6580810546875\n","Train -> sample/numSamples/epoch: 562/1182/0, Loss: 789.2553100585938\n","Train -> sample/numSamples/epoch: 563/1182/0, Loss: 1475.744384765625\n","Train -> sample/numSamples/epoch: 564/1182/0, Loss: 1204.9671630859375\n","Train -> sample/numSamples/epoch: 565/1182/0, Loss: 930.956787109375\n","Train -> sample/numSamples/epoch: 566/1182/0, Loss: 869.794189453125\n","Train -> sample/numSamples/epoch: 567/1182/0, Loss: 880.7969360351562\n","Train -> sample/numSamples/epoch: 568/1182/0, Loss: 1211.35107421875\n","Train -> sample/numSamples/epoch: 569/1182/0, Loss: 979.2177734375\n","Train -> sample/numSamples/epoch: 570/1182/0, Loss: 679.8827514648438\n","Train -> sample/numSamples/epoch: 571/1182/0, Loss: 827.3223266601562\n","Train -> sample/numSamples/epoch: 572/1182/0, Loss: 923.1871337890625\n","Train -> sample/numSamples/epoch: 573/1182/0, Loss: 891.9775390625\n","Train -> sample/numSamples/epoch: 574/1182/0, Loss: 1028.852783203125\n","Train -> sample/numSamples/epoch: 575/1182/0, Loss: 761.4523315429688\n","Train -> sample/numSamples/epoch: 576/1182/0, Loss: 847.2510986328125\n","Train -> sample/numSamples/epoch: 577/1182/0, Loss: 1610.003662109375\n","Train -> sample/numSamples/epoch: 578/1182/0, Loss: 1414.21337890625\n","Train -> sample/numSamples/epoch: 579/1182/0, Loss: 1400.955810546875\n","Train -> sample/numSamples/epoch: 580/1182/0, Loss: 1186.1351318359375\n","Train -> sample/numSamples/epoch: 581/1182/0, Loss: 1095.7254638671875\n","Train -> sample/numSamples/epoch: 582/1182/0, Loss: 1023.7665405273438\n","Train -> sample/numSamples/epoch: 583/1182/0, Loss: 1458.8231201171875\n","Train -> sample/numSamples/epoch: 584/1182/0, Loss: 983.7169799804688\n","Train -> sample/numSamples/epoch: 585/1182/0, Loss: 913.0945434570312\n","Train -> sample/numSamples/epoch: 586/1182/0, Loss: 898.3005981445312\n","Train -> sample/numSamples/epoch: 587/1182/0, Loss: 1026.6510009765625\n","Train -> sample/numSamples/epoch: 588/1182/0, Loss: 989.0677490234375\n","Train -> sample/numSamples/epoch: 589/1182/0, Loss: 1173.7750244140625\n","Train -> sample/numSamples/epoch: 590/1182/0, Loss: 1207.3563232421875\n","Train -> sample/numSamples/epoch: 591/1182/0, Loss: 1061.590576171875\n","Train -> sample/numSamples/epoch: 592/1182/0, Loss: 820.7408447265625\n","Train -> sample/numSamples/epoch: 593/1182/0, Loss: 764.486328125\n","Train -> sample/numSamples/epoch: 594/1182/0, Loss: 1278.676513671875\n","Train -> sample/numSamples/epoch: 595/1182/0, Loss: 1369.6796875\n","Train -> sample/numSamples/epoch: 596/1182/0, Loss: 939.2105102539062\n","Train -> sample/numSamples/epoch: 597/1182/0, Loss: 616.5109252929688\n","Train -> sample/numSamples/epoch: 598/1182/0, Loss: 941.2200317382812\n","Train -> sample/numSamples/epoch: 599/1182/0, Loss: 973.9222412109375\n","imatge guardada\n","Train -> sample/numSamples/epoch: 600/1182/0, Loss: 916.2237548828125\n","Train -> sample/numSamples/epoch: 601/1182/0, Loss: 1311.1043701171875\n","Train -> sample/numSamples/epoch: 602/1182/0, Loss: 1340.4420166015625\n","Train -> sample/numSamples/epoch: 603/1182/0, Loss: 1284.5792236328125\n","Train -> sample/numSamples/epoch: 604/1182/0, Loss: 1238.25244140625\n","Train -> sample/numSamples/epoch: 605/1182/0, Loss: 467.7337646484375\n","Train -> sample/numSamples/epoch: 606/1182/0, Loss: 926.6072998046875\n","Train -> sample/numSamples/epoch: 607/1182/0, Loss: 898.4163208007812\n","Train -> sample/numSamples/epoch: 608/1182/0, Loss: 1192.4625244140625\n","Train -> sample/numSamples/epoch: 609/1182/0, Loss: 990.3539428710938\n","Train -> sample/numSamples/epoch: 610/1182/0, Loss: 1086.4229736328125\n","Train -> sample/numSamples/epoch: 611/1182/0, Loss: 1076.475830078125\n","Train -> sample/numSamples/epoch: 612/1182/0, Loss: 822.8417358398438\n","Train -> sample/numSamples/epoch: 613/1182/0, Loss: 984.7886352539062\n","Train -> sample/numSamples/epoch: 614/1182/0, Loss: 728.1558227539062\n","Train -> sample/numSamples/epoch: 615/1182/0, Loss: 986.3282470703125\n","Train -> sample/numSamples/epoch: 616/1182/0, Loss: 995.4929809570312\n","Train -> sample/numSamples/epoch: 617/1182/0, Loss: 1528.8748779296875\n","Train -> sample/numSamples/epoch: 618/1182/0, Loss: 712.3948364257812\n","Train -> sample/numSamples/epoch: 619/1182/0, Loss: 1172.2462158203125\n","Train -> sample/numSamples/epoch: 620/1182/0, Loss: 1310.7049560546875\n","Train -> sample/numSamples/epoch: 621/1182/0, Loss: 884.08056640625\n","Train -> sample/numSamples/epoch: 622/1182/0, Loss: 1036.5372314453125\n","Train -> sample/numSamples/epoch: 623/1182/0, Loss: 1315.2130126953125\n","Train -> sample/numSamples/epoch: 624/1182/0, Loss: 1185.2906494140625\n","Train -> sample/numSamples/epoch: 625/1182/0, Loss: 1143.5472412109375\n","Train -> sample/numSamples/epoch: 626/1182/0, Loss: 934.1025390625\n","Train -> sample/numSamples/epoch: 627/1182/0, Loss: 1064.480224609375\n","Train -> sample/numSamples/epoch: 628/1182/0, Loss: 1074.933837890625\n","Train -> sample/numSamples/epoch: 629/1182/0, Loss: 1003.5067749023438\n","Train -> sample/numSamples/epoch: 630/1182/0, Loss: 924.6697387695312\n","Train -> sample/numSamples/epoch: 631/1182/0, Loss: 1283.498046875\n","Train -> sample/numSamples/epoch: 632/1182/0, Loss: 952.6621704101562\n","Train -> sample/numSamples/epoch: 633/1182/0, Loss: 882.9779663085938\n","Train -> sample/numSamples/epoch: 634/1182/0, Loss: 1361.4395751953125\n","Train -> sample/numSamples/epoch: 635/1182/0, Loss: 1306.227783203125\n","Train -> sample/numSamples/epoch: 636/1182/0, Loss: 1296.238037109375\n","Train -> sample/numSamples/epoch: 637/1182/0, Loss: 1356.5775146484375\n","Train -> sample/numSamples/epoch: 638/1182/0, Loss: 1128.6964111328125\n","Train -> sample/numSamples/epoch: 639/1182/0, Loss: 1266.2864990234375\n","Train -> sample/numSamples/epoch: 640/1182/0, Loss: 843.0317993164062\n","Train -> sample/numSamples/epoch: 641/1182/0, Loss: 722.1600341796875\n","Train -> sample/numSamples/epoch: 642/1182/0, Loss: 947.0507202148438\n","Train -> sample/numSamples/epoch: 643/1182/0, Loss: 917.1348266601562\n","Train -> sample/numSamples/epoch: 644/1182/0, Loss: 1435.0390625\n","Train -> sample/numSamples/epoch: 645/1182/0, Loss: 931.9612426757812\n","Train -> sample/numSamples/epoch: 646/1182/0, Loss: 664.8060302734375\n","Train -> sample/numSamples/epoch: 647/1182/0, Loss: 1167.8824462890625\n","Train -> sample/numSamples/epoch: 648/1182/0, Loss: 1034.0716552734375\n","Train -> sample/numSamples/epoch: 649/1182/0, Loss: 1203.9820556640625\n","Train -> sample/numSamples/epoch: 650/1182/0, Loss: 1063.8515625\n","Train -> sample/numSamples/epoch: 651/1182/0, Loss: 1441.4324951171875\n","Train -> sample/numSamples/epoch: 652/1182/0, Loss: 1087.2054443359375\n","Train -> sample/numSamples/epoch: 653/1182/0, Loss: 860.1602172851562\n","Train -> sample/numSamples/epoch: 654/1182/0, Loss: 1341.234375\n","Train -> sample/numSamples/epoch: 655/1182/0, Loss: 1240.4937744140625\n","Train -> sample/numSamples/epoch: 656/1182/0, Loss: 1414.7706298828125\n","Train -> sample/numSamples/epoch: 657/1182/0, Loss: 835.119873046875\n","Train -> sample/numSamples/epoch: 658/1182/0, Loss: 998.4030151367188\n","Train -> sample/numSamples/epoch: 659/1182/0, Loss: 871.1593627929688\n","Train -> sample/numSamples/epoch: 660/1182/0, Loss: 1288.519287109375\n","Train -> sample/numSamples/epoch: 661/1182/0, Loss: 883.9998168945312\n","Train -> sample/numSamples/epoch: 662/1182/0, Loss: 1131.9527587890625\n","Train -> sample/numSamples/epoch: 663/1182/0, Loss: 1356.7471923828125\n","Train -> sample/numSamples/epoch: 664/1182/0, Loss: 1100.434326171875\n","Train -> sample/numSamples/epoch: 665/1182/0, Loss: 917.5929565429688\n","Train -> sample/numSamples/epoch: 666/1182/0, Loss: 1250.7313232421875\n","Train -> sample/numSamples/epoch: 667/1182/0, Loss: 1042.9283447265625\n","Train -> sample/numSamples/epoch: 668/1182/0, Loss: 1073.1195068359375\n","Train -> sample/numSamples/epoch: 669/1182/0, Loss: 1264.9932861328125\n","Train -> sample/numSamples/epoch: 670/1182/0, Loss: 656.588623046875\n","Train -> sample/numSamples/epoch: 671/1182/0, Loss: 704.9274291992188\n","Train -> sample/numSamples/epoch: 672/1182/0, Loss: 1150.2303466796875\n","Train -> sample/numSamples/epoch: 673/1182/0, Loss: 1198.1221923828125\n","Train -> sample/numSamples/epoch: 674/1182/0, Loss: 1161.8480224609375\n","Train -> sample/numSamples/epoch: 675/1182/0, Loss: 1380.32373046875\n","Train -> sample/numSamples/epoch: 676/1182/0, Loss: 834.791015625\n","Train -> sample/numSamples/epoch: 677/1182/0, Loss: 1139.4447021484375\n","Train -> sample/numSamples/epoch: 678/1182/0, Loss: 1321.226318359375\n","Train -> sample/numSamples/epoch: 679/1182/0, Loss: 942.5718994140625\n","Train -> sample/numSamples/epoch: 680/1182/0, Loss: 1134.7523193359375\n","Train -> sample/numSamples/epoch: 681/1182/0, Loss: 1096.4666748046875\n","Train -> sample/numSamples/epoch: 682/1182/0, Loss: 670.8485717773438\n","Train -> sample/numSamples/epoch: 683/1182/0, Loss: 1074.0372314453125\n","Train -> sample/numSamples/epoch: 684/1182/0, Loss: 955.8663330078125\n","Train -> sample/numSamples/epoch: 685/1182/0, Loss: 909.6802978515625\n","Train -> sample/numSamples/epoch: 686/1182/0, Loss: 981.9368896484375\n","Train -> sample/numSamples/epoch: 687/1182/0, Loss: 776.6424560546875\n","Train -> sample/numSamples/epoch: 688/1182/0, Loss: 1157.0650634765625\n","Train -> sample/numSamples/epoch: 689/1182/0, Loss: 730.9639892578125\n","Train -> sample/numSamples/epoch: 690/1182/0, Loss: 1352.1407470703125\n","Train -> sample/numSamples/epoch: 691/1182/0, Loss: 673.9288940429688\n","Train -> sample/numSamples/epoch: 692/1182/0, Loss: 783.598876953125\n","Train -> sample/numSamples/epoch: 693/1182/0, Loss: 925.905517578125\n","Train -> sample/numSamples/epoch: 694/1182/0, Loss: 540.007080078125\n","Train -> sample/numSamples/epoch: 695/1182/0, Loss: 1118.1234130859375\n","Train -> sample/numSamples/epoch: 696/1182/0, Loss: 965.987060546875\n","Train -> sample/numSamples/epoch: 697/1182/0, Loss: 912.5901489257812\n","Train -> sample/numSamples/epoch: 698/1182/0, Loss: 1466.84765625\n","Train -> sample/numSamples/epoch: 699/1182/0, Loss: 1117.6805419921875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 700/1182/0, Loss: 939.2127075195312\n","Train -> sample/numSamples/epoch: 701/1182/0, Loss: 1015.934814453125\n","Train -> sample/numSamples/epoch: 702/1182/0, Loss: 801.1576538085938\n","Train -> sample/numSamples/epoch: 703/1182/0, Loss: 1053.8525390625\n","Train -> sample/numSamples/epoch: 704/1182/0, Loss: 1377.479248046875\n","Train -> sample/numSamples/epoch: 705/1182/0, Loss: 818.0556030273438\n","Train -> sample/numSamples/epoch: 706/1182/0, Loss: 738.9673461914062\n","Train -> sample/numSamples/epoch: 707/1182/0, Loss: 772.708740234375\n","Train -> sample/numSamples/epoch: 708/1182/0, Loss: 1348.4058837890625\n","Train -> sample/numSamples/epoch: 709/1182/0, Loss: 1302.2427978515625\n","Train -> sample/numSamples/epoch: 710/1182/0, Loss: 870.4539184570312\n","Train -> sample/numSamples/epoch: 711/1182/0, Loss: 910.6422119140625\n","Train -> sample/numSamples/epoch: 712/1182/0, Loss: 1363.286376953125\n","Train -> sample/numSamples/epoch: 713/1182/0, Loss: 860.0514526367188\n","Train -> sample/numSamples/epoch: 714/1182/0, Loss: 959.7986450195312\n","Train -> sample/numSamples/epoch: 715/1182/0, Loss: 719.4464111328125\n","Train -> sample/numSamples/epoch: 716/1182/0, Loss: 1020.642578125\n","Train -> sample/numSamples/epoch: 717/1182/0, Loss: 837.4358520507812\n","Train -> sample/numSamples/epoch: 718/1182/0, Loss: 653.1922607421875\n","Train -> sample/numSamples/epoch: 719/1182/0, Loss: 734.7138061523438\n","Train -> sample/numSamples/epoch: 720/1182/0, Loss: 1158.236572265625\n","Train -> sample/numSamples/epoch: 721/1182/0, Loss: 835.0233154296875\n","Train -> sample/numSamples/epoch: 722/1182/0, Loss: 1193.7274169921875\n","Train -> sample/numSamples/epoch: 723/1182/0, Loss: 777.3326416015625\n","Train -> sample/numSamples/epoch: 724/1182/0, Loss: 1225.0313720703125\n","Train -> sample/numSamples/epoch: 725/1182/0, Loss: 865.1266479492188\n","Train -> sample/numSamples/epoch: 726/1182/0, Loss: 1017.5091552734375\n","Train -> sample/numSamples/epoch: 727/1182/0, Loss: 865.3619384765625\n","Train -> sample/numSamples/epoch: 728/1182/0, Loss: 1005.3136596679688\n","Train -> sample/numSamples/epoch: 729/1182/0, Loss: 998.0338745117188\n","Train -> sample/numSamples/epoch: 730/1182/0, Loss: 791.642822265625\n","Train -> sample/numSamples/epoch: 731/1182/0, Loss: 1297.3599853515625\n","Train -> sample/numSamples/epoch: 732/1182/0, Loss: 1141.7740478515625\n","Train -> sample/numSamples/epoch: 733/1182/0, Loss: 1255.2203369140625\n","Train -> sample/numSamples/epoch: 734/1182/0, Loss: 1062.4942626953125\n","Train -> sample/numSamples/epoch: 735/1182/0, Loss: 1547.545166015625\n","Train -> sample/numSamples/epoch: 736/1182/0, Loss: 964.4170532226562\n","Train -> sample/numSamples/epoch: 737/1182/0, Loss: 1040.187255859375\n","Train -> sample/numSamples/epoch: 738/1182/0, Loss: 1081.6322021484375\n","Train -> sample/numSamples/epoch: 739/1182/0, Loss: 884.771240234375\n","Train -> sample/numSamples/epoch: 740/1182/0, Loss: 1070.3187255859375\n","Train -> sample/numSamples/epoch: 741/1182/0, Loss: 1261.2708740234375\n","Train -> sample/numSamples/epoch: 742/1182/0, Loss: 857.0910034179688\n","Train -> sample/numSamples/epoch: 743/1182/0, Loss: 837.5419311523438\n","Train -> sample/numSamples/epoch: 744/1182/0, Loss: 864.4048461914062\n","Train -> sample/numSamples/epoch: 745/1182/0, Loss: 1204.742919921875\n","Train -> sample/numSamples/epoch: 746/1182/0, Loss: 1194.9703369140625\n","Train -> sample/numSamples/epoch: 747/1182/0, Loss: 1520.7578125\n","Train -> sample/numSamples/epoch: 748/1182/0, Loss: 743.1844482421875\n","Train -> sample/numSamples/epoch: 749/1182/0, Loss: 801.2699584960938\n","Train -> sample/numSamples/epoch: 750/1182/0, Loss: 1594.100341796875\n","Train -> sample/numSamples/epoch: 751/1182/0, Loss: 965.8252563476562\n","Train -> sample/numSamples/epoch: 752/1182/0, Loss: 1121.8681640625\n","Train -> sample/numSamples/epoch: 753/1182/0, Loss: 943.0145874023438\n","Train -> sample/numSamples/epoch: 754/1182/0, Loss: 1436.38720703125\n","Train -> sample/numSamples/epoch: 755/1182/0, Loss: 1120.2916259765625\n","Train -> sample/numSamples/epoch: 756/1182/0, Loss: 1487.6431884765625\n","Train -> sample/numSamples/epoch: 757/1182/0, Loss: 856.8648071289062\n","Train -> sample/numSamples/epoch: 758/1182/0, Loss: 1121.4842529296875\n","Train -> sample/numSamples/epoch: 759/1182/0, Loss: 832.2632446289062\n","Train -> sample/numSamples/epoch: 760/1182/0, Loss: 1123.2471923828125\n","Train -> sample/numSamples/epoch: 761/1182/0, Loss: 950.2725219726562\n","Train -> sample/numSamples/epoch: 762/1182/0, Loss: 1189.5869140625\n","Train -> sample/numSamples/epoch: 763/1182/0, Loss: 1252.513916015625\n","Train -> sample/numSamples/epoch: 764/1182/0, Loss: 1255.1943359375\n","Train -> sample/numSamples/epoch: 765/1182/0, Loss: 717.4281005859375\n","Train -> sample/numSamples/epoch: 766/1182/0, Loss: 1205.42724609375\n","Train -> sample/numSamples/epoch: 767/1182/0, Loss: 1205.1273193359375\n","Train -> sample/numSamples/epoch: 768/1182/0, Loss: 1417.692138671875\n","Train -> sample/numSamples/epoch: 769/1182/0, Loss: 1096.99560546875\n","Train -> sample/numSamples/epoch: 770/1182/0, Loss: 1190.388427734375\n","Train -> sample/numSamples/epoch: 771/1182/0, Loss: 1026.9613037109375\n","Train -> sample/numSamples/epoch: 772/1182/0, Loss: 971.82470703125\n","Train -> sample/numSamples/epoch: 773/1182/0, Loss: 650.5941162109375\n","Train -> sample/numSamples/epoch: 774/1182/0, Loss: 876.9305419921875\n","Train -> sample/numSamples/epoch: 775/1182/0, Loss: 1231.1605224609375\n","Train -> sample/numSamples/epoch: 776/1182/0, Loss: 1120.78466796875\n","Train -> sample/numSamples/epoch: 777/1182/0, Loss: 1384.741455078125\n","Train -> sample/numSamples/epoch: 778/1182/0, Loss: 1188.111328125\n","Train -> sample/numSamples/epoch: 779/1182/0, Loss: 876.2883911132812\n","Train -> sample/numSamples/epoch: 780/1182/0, Loss: 1136.376953125\n","Train -> sample/numSamples/epoch: 781/1182/0, Loss: 1314.75732421875\n","Train -> sample/numSamples/epoch: 782/1182/0, Loss: 1111.195556640625\n","Train -> sample/numSamples/epoch: 783/1182/0, Loss: 1342.487548828125\n","Train -> sample/numSamples/epoch: 784/1182/0, Loss: 1164.2861328125\n","Train -> sample/numSamples/epoch: 785/1182/0, Loss: 1123.9249267578125\n","Train -> sample/numSamples/epoch: 786/1182/0, Loss: 1480.989501953125\n","Train -> sample/numSamples/epoch: 787/1182/0, Loss: 622.240478515625\n","Train -> sample/numSamples/epoch: 788/1182/0, Loss: 729.3934326171875\n","Train -> sample/numSamples/epoch: 789/1182/0, Loss: 1308.823974609375\n","Train -> sample/numSamples/epoch: 790/1182/0, Loss: 987.4402465820312\n","Train -> sample/numSamples/epoch: 791/1182/0, Loss: 1258.3355712890625\n","Train -> sample/numSamples/epoch: 792/1182/0, Loss: 1165.0506591796875\n","Train -> sample/numSamples/epoch: 793/1182/0, Loss: 981.1365356445312\n","Train -> sample/numSamples/epoch: 794/1182/0, Loss: 1302.8453369140625\n","Train -> sample/numSamples/epoch: 795/1182/0, Loss: 1237.3680419921875\n","Train -> sample/numSamples/epoch: 796/1182/0, Loss: 818.7537231445312\n","Train -> sample/numSamples/epoch: 797/1182/0, Loss: 1029.42041015625\n","Train -> sample/numSamples/epoch: 798/1182/0, Loss: 608.4444580078125\n","Train -> sample/numSamples/epoch: 799/1182/0, Loss: 1509.9498291015625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 800/1182/0, Loss: 979.1085205078125\n","Train -> sample/numSamples/epoch: 801/1182/0, Loss: 1119.938232421875\n","Train -> sample/numSamples/epoch: 802/1182/0, Loss: 1209.2061767578125\n","Train -> sample/numSamples/epoch: 803/1182/0, Loss: 928.8165283203125\n","Train -> sample/numSamples/epoch: 804/1182/0, Loss: 1087.4605712890625\n","Train -> sample/numSamples/epoch: 805/1182/0, Loss: 932.2288818359375\n","Train -> sample/numSamples/epoch: 806/1182/0, Loss: 843.248779296875\n","Train -> sample/numSamples/epoch: 807/1182/0, Loss: 1867.95751953125\n","Train -> sample/numSamples/epoch: 808/1182/0, Loss: 1031.5457763671875\n","Train -> sample/numSamples/epoch: 809/1182/0, Loss: 1407.002685546875\n","Train -> sample/numSamples/epoch: 810/1182/0, Loss: 911.3988647460938\n","Train -> sample/numSamples/epoch: 811/1182/0, Loss: 798.9541015625\n","Train -> sample/numSamples/epoch: 812/1182/0, Loss: 873.033203125\n","Train -> sample/numSamples/epoch: 813/1182/0, Loss: 440.1124572753906\n","Train -> sample/numSamples/epoch: 814/1182/0, Loss: 715.6072998046875\n","Train -> sample/numSamples/epoch: 815/1182/0, Loss: 934.225830078125\n","Train -> sample/numSamples/epoch: 816/1182/0, Loss: 1055.4749755859375\n","Train -> sample/numSamples/epoch: 817/1182/0, Loss: 707.37158203125\n","Train -> sample/numSamples/epoch: 818/1182/0, Loss: 1022.195068359375\n","Train -> sample/numSamples/epoch: 819/1182/0, Loss: 900.791015625\n","Train -> sample/numSamples/epoch: 820/1182/0, Loss: 1224.88818359375\n","Train -> sample/numSamples/epoch: 821/1182/0, Loss: 1269.48046875\n","Train -> sample/numSamples/epoch: 822/1182/0, Loss: 883.6231079101562\n","Train -> sample/numSamples/epoch: 823/1182/0, Loss: 1102.2216796875\n","Train -> sample/numSamples/epoch: 824/1182/0, Loss: 1515.5902099609375\n","Train -> sample/numSamples/epoch: 825/1182/0, Loss: 896.1676635742188\n","Train -> sample/numSamples/epoch: 826/1182/0, Loss: 709.2603759765625\n","Train -> sample/numSamples/epoch: 827/1182/0, Loss: 1001.1851806640625\n","Train -> sample/numSamples/epoch: 828/1182/0, Loss: 1139.8370361328125\n","Train -> sample/numSamples/epoch: 829/1182/0, Loss: 1430.109375\n","Train -> sample/numSamples/epoch: 830/1182/0, Loss: 1473.9593505859375\n","Train -> sample/numSamples/epoch: 831/1182/0, Loss: 925.5152587890625\n","Train -> sample/numSamples/epoch: 832/1182/0, Loss: 910.1704711914062\n","Train -> sample/numSamples/epoch: 833/1182/0, Loss: 1613.521728515625\n","Train -> sample/numSamples/epoch: 834/1182/0, Loss: 906.0503540039062\n","Train -> sample/numSamples/epoch: 835/1182/0, Loss: 1154.525634765625\n","Train -> sample/numSamples/epoch: 836/1182/0, Loss: 1019.183349609375\n","Train -> sample/numSamples/epoch: 837/1182/0, Loss: 1007.6892700195312\n","Train -> sample/numSamples/epoch: 838/1182/0, Loss: 852.0326538085938\n","Train -> sample/numSamples/epoch: 839/1182/0, Loss: 993.0889282226562\n","Train -> sample/numSamples/epoch: 840/1182/0, Loss: 1267.57177734375\n","Train -> sample/numSamples/epoch: 841/1182/0, Loss: 1025.0037841796875\n","Train -> sample/numSamples/epoch: 842/1182/0, Loss: 929.3345336914062\n","Train -> sample/numSamples/epoch: 843/1182/0, Loss: 1336.2618408203125\n","Train -> sample/numSamples/epoch: 844/1182/0, Loss: 1163.654052734375\n","Train -> sample/numSamples/epoch: 845/1182/0, Loss: 1164.017578125\n","Train -> sample/numSamples/epoch: 846/1182/0, Loss: 1300.8775634765625\n","Train -> sample/numSamples/epoch: 847/1182/0, Loss: 1135.1192626953125\n","Train -> sample/numSamples/epoch: 848/1182/0, Loss: 1041.3858642578125\n","Train -> sample/numSamples/epoch: 849/1182/0, Loss: 1377.3095703125\n","Train -> sample/numSamples/epoch: 850/1182/0, Loss: 844.138916015625\n","Train -> sample/numSamples/epoch: 851/1182/0, Loss: 935.7283935546875\n","Train -> sample/numSamples/epoch: 852/1182/0, Loss: 1270.94873046875\n","Train -> sample/numSamples/epoch: 853/1182/0, Loss: 769.6531982421875\n","Train -> sample/numSamples/epoch: 854/1182/0, Loss: 1160.8026123046875\n","Train -> sample/numSamples/epoch: 855/1182/0, Loss: 909.3446655273438\n","Train -> sample/numSamples/epoch: 856/1182/0, Loss: 1265.63134765625\n","Train -> sample/numSamples/epoch: 857/1182/0, Loss: 1146.6436767578125\n","Train -> sample/numSamples/epoch: 858/1182/0, Loss: 1382.0321044921875\n","Train -> sample/numSamples/epoch: 859/1182/0, Loss: 1158.5523681640625\n","Train -> sample/numSamples/epoch: 860/1182/0, Loss: 984.6525268554688\n","Train -> sample/numSamples/epoch: 861/1182/0, Loss: 832.3951416015625\n","Train -> sample/numSamples/epoch: 862/1182/0, Loss: 1052.8675537109375\n","Train -> sample/numSamples/epoch: 863/1182/0, Loss: 1339.96142578125\n","Train -> sample/numSamples/epoch: 864/1182/0, Loss: 1020.759033203125\n","Train -> sample/numSamples/epoch: 865/1182/0, Loss: 990.889404296875\n","Train -> sample/numSamples/epoch: 866/1182/0, Loss: 1291.1370849609375\n","Train -> sample/numSamples/epoch: 867/1182/0, Loss: 1127.7884521484375\n","Train -> sample/numSamples/epoch: 868/1182/0, Loss: 874.9719848632812\n","Train -> sample/numSamples/epoch: 869/1182/0, Loss: 1057.2249755859375\n","Train -> sample/numSamples/epoch: 870/1182/0, Loss: 900.0840454101562\n","Train -> sample/numSamples/epoch: 871/1182/0, Loss: 911.5405883789062\n","Train -> sample/numSamples/epoch: 872/1182/0, Loss: 1134.7271728515625\n","Train -> sample/numSamples/epoch: 873/1182/0, Loss: 600.96875\n","Train -> sample/numSamples/epoch: 874/1182/0, Loss: 1558.1766357421875\n","Train -> sample/numSamples/epoch: 875/1182/0, Loss: 889.4033813476562\n","Train -> sample/numSamples/epoch: 876/1182/0, Loss: 1025.1734619140625\n","Train -> sample/numSamples/epoch: 877/1182/0, Loss: 864.4208374023438\n","Train -> sample/numSamples/epoch: 878/1182/0, Loss: 1086.689208984375\n","Train -> sample/numSamples/epoch: 879/1182/0, Loss: 1212.65869140625\n","Train -> sample/numSamples/epoch: 880/1182/0, Loss: 1470.191650390625\n","Train -> sample/numSamples/epoch: 881/1182/0, Loss: 1181.4990234375\n","Train -> sample/numSamples/epoch: 882/1182/0, Loss: 1276.34912109375\n","Train -> sample/numSamples/epoch: 883/1182/0, Loss: 1298.8839111328125\n","Train -> sample/numSamples/epoch: 884/1182/0, Loss: 862.6925659179688\n","Train -> sample/numSamples/epoch: 885/1182/0, Loss: 477.9959716796875\n","Train -> sample/numSamples/epoch: 886/1182/0, Loss: 993.3087158203125\n","Train -> sample/numSamples/epoch: 887/1182/0, Loss: 620.2146606445312\n","Train -> sample/numSamples/epoch: 888/1182/0, Loss: 588.6771850585938\n","Train -> sample/numSamples/epoch: 889/1182/0, Loss: 1007.5257568359375\n","Train -> sample/numSamples/epoch: 890/1182/0, Loss: 851.5084228515625\n","Train -> sample/numSamples/epoch: 891/1182/0, Loss: 843.1586303710938\n","Train -> sample/numSamples/epoch: 892/1182/0, Loss: 1231.085693359375\n","Train -> sample/numSamples/epoch: 893/1182/0, Loss: 1167.1397705078125\n","Train -> sample/numSamples/epoch: 894/1182/0, Loss: 1100.7535400390625\n","Train -> sample/numSamples/epoch: 895/1182/0, Loss: 985.8776245117188\n","Train -> sample/numSamples/epoch: 896/1182/0, Loss: 748.5481567382812\n","Train -> sample/numSamples/epoch: 897/1182/0, Loss: 1428.5633544921875\n","Train -> sample/numSamples/epoch: 898/1182/0, Loss: 1583.9259033203125\n","Train -> sample/numSamples/epoch: 899/1182/0, Loss: 758.4026489257812\n","imatge guardada\n","Train -> sample/numSamples/epoch: 900/1182/0, Loss: 1059.9637451171875\n","Train -> sample/numSamples/epoch: 901/1182/0, Loss: 1039.154052734375\n","Train -> sample/numSamples/epoch: 902/1182/0, Loss: 1185.9190673828125\n","Train -> sample/numSamples/epoch: 903/1182/0, Loss: 1254.7166748046875\n","Train -> sample/numSamples/epoch: 904/1182/0, Loss: 1673.0035400390625\n","Train -> sample/numSamples/epoch: 905/1182/0, Loss: 796.521484375\n","Train -> sample/numSamples/epoch: 906/1182/0, Loss: 1100.325927734375\n","Train -> sample/numSamples/epoch: 907/1182/0, Loss: 888.3530883789062\n","Train -> sample/numSamples/epoch: 908/1182/0, Loss: 1168.173095703125\n","Train -> sample/numSamples/epoch: 909/1182/0, Loss: 927.7681274414062\n","Train -> sample/numSamples/epoch: 910/1182/0, Loss: 1429.7115478515625\n","Train -> sample/numSamples/epoch: 911/1182/0, Loss: 1387.335693359375\n","Train -> sample/numSamples/epoch: 912/1182/0, Loss: 949.8118896484375\n","Train -> sample/numSamples/epoch: 913/1182/0, Loss: 592.7494506835938\n","Train -> sample/numSamples/epoch: 914/1182/0, Loss: 883.5292358398438\n","Train -> sample/numSamples/epoch: 915/1182/0, Loss: 1997.5447998046875\n","Train -> sample/numSamples/epoch: 916/1182/0, Loss: 760.9664916992188\n","Train -> sample/numSamples/epoch: 917/1182/0, Loss: 1240.3377685546875\n","Train -> sample/numSamples/epoch: 918/1182/0, Loss: 1079.5125732421875\n","Train -> sample/numSamples/epoch: 919/1182/0, Loss: 830.552978515625\n","Train -> sample/numSamples/epoch: 920/1182/0, Loss: 1757.53076171875\n","Train -> sample/numSamples/epoch: 921/1182/0, Loss: 853.6838989257812\n","Train -> sample/numSamples/epoch: 922/1182/0, Loss: 471.54205322265625\n","Train -> sample/numSamples/epoch: 923/1182/0, Loss: 1063.4781494140625\n","Train -> sample/numSamples/epoch: 924/1182/0, Loss: 1014.1990356445312\n","Train -> sample/numSamples/epoch: 925/1182/0, Loss: 1448.0772705078125\n","Train -> sample/numSamples/epoch: 926/1182/0, Loss: 1195.4505615234375\n","Train -> sample/numSamples/epoch: 927/1182/0, Loss: 1138.1279296875\n","Train -> sample/numSamples/epoch: 928/1182/0, Loss: 682.728271484375\n","Train -> sample/numSamples/epoch: 929/1182/0, Loss: 660.5557250976562\n","Train -> sample/numSamples/epoch: 930/1182/0, Loss: 822.1309814453125\n","Train -> sample/numSamples/epoch: 931/1182/0, Loss: 1008.6500244140625\n","Train -> sample/numSamples/epoch: 932/1182/0, Loss: 1845.5538330078125\n","Train -> sample/numSamples/epoch: 933/1182/0, Loss: 913.0366821289062\n","Train -> sample/numSamples/epoch: 934/1182/0, Loss: 915.2095336914062\n","Train -> sample/numSamples/epoch: 935/1182/0, Loss: 796.4186401367188\n","Train -> sample/numSamples/epoch: 936/1182/0, Loss: 809.9364624023438\n","Train -> sample/numSamples/epoch: 937/1182/0, Loss: 1113.0545654296875\n","Train -> sample/numSamples/epoch: 938/1182/0, Loss: 984.840087890625\n","Train -> sample/numSamples/epoch: 939/1182/0, Loss: 1223.0728759765625\n","Train -> sample/numSamples/epoch: 940/1182/0, Loss: 645.8067016601562\n","Train -> sample/numSamples/epoch: 941/1182/0, Loss: 1463.7803955078125\n","Train -> sample/numSamples/epoch: 942/1182/0, Loss: 1017.93408203125\n","Train -> sample/numSamples/epoch: 943/1182/0, Loss: 1241.7496337890625\n","Train -> sample/numSamples/epoch: 944/1182/0, Loss: 998.4857788085938\n","Train -> sample/numSamples/epoch: 945/1182/0, Loss: 566.9794921875\n","Train -> sample/numSamples/epoch: 946/1182/0, Loss: 1035.2930908203125\n","Train -> sample/numSamples/epoch: 947/1182/0, Loss: 812.4669189453125\n","Train -> sample/numSamples/epoch: 948/1182/0, Loss: 942.1444702148438\n","Train -> sample/numSamples/epoch: 949/1182/0, Loss: 1077.7657470703125\n","Train -> sample/numSamples/epoch: 950/1182/0, Loss: 1093.3511962890625\n","Train -> sample/numSamples/epoch: 951/1182/0, Loss: 856.9087524414062\n","Train -> sample/numSamples/epoch: 952/1182/0, Loss: 1442.109619140625\n","Train -> sample/numSamples/epoch: 953/1182/0, Loss: 851.5169067382812\n","Train -> sample/numSamples/epoch: 954/1182/0, Loss: 857.1399536132812\n","Train -> sample/numSamples/epoch: 955/1182/0, Loss: 1634.7174072265625\n","Train -> sample/numSamples/epoch: 956/1182/0, Loss: 884.9392700195312\n","Train -> sample/numSamples/epoch: 957/1182/0, Loss: 1285.1668701171875\n","Train -> sample/numSamples/epoch: 958/1182/0, Loss: 1033.1669921875\n","Train -> sample/numSamples/epoch: 959/1182/0, Loss: 760.7982177734375\n","Train -> sample/numSamples/epoch: 960/1182/0, Loss: 1512.8431396484375\n","Train -> sample/numSamples/epoch: 961/1182/0, Loss: 801.9912719726562\n","Train -> sample/numSamples/epoch: 962/1182/0, Loss: 1221.890625\n","Train -> sample/numSamples/epoch: 963/1182/0, Loss: 1599.8123779296875\n","Train -> sample/numSamples/epoch: 964/1182/0, Loss: 520.0655517578125\n","Train -> sample/numSamples/epoch: 965/1182/0, Loss: 1109.0150146484375\n","Train -> sample/numSamples/epoch: 966/1182/0, Loss: 810.5631713867188\n","Train -> sample/numSamples/epoch: 967/1182/0, Loss: 1001.6355590820312\n","Train -> sample/numSamples/epoch: 968/1182/0, Loss: 872.456298828125\n","Train -> sample/numSamples/epoch: 969/1182/0, Loss: 831.1707153320312\n","Train -> sample/numSamples/epoch: 970/1182/0, Loss: 1696.6298828125\n","Train -> sample/numSamples/epoch: 971/1182/0, Loss: 696.6123046875\n","Train -> sample/numSamples/epoch: 972/1182/0, Loss: 773.655517578125\n","Train -> sample/numSamples/epoch: 973/1182/0, Loss: 1235.19775390625\n","Train -> sample/numSamples/epoch: 974/1182/0, Loss: 1326.3553466796875\n","Train -> sample/numSamples/epoch: 975/1182/0, Loss: 1311.738037109375\n","Train -> sample/numSamples/epoch: 976/1182/0, Loss: 1098.19677734375\n","Train -> sample/numSamples/epoch: 977/1182/0, Loss: 1190.979248046875\n","Train -> sample/numSamples/epoch: 978/1182/0, Loss: 1565.9310302734375\n","Train -> sample/numSamples/epoch: 979/1182/0, Loss: 1267.727783203125\n","Train -> sample/numSamples/epoch: 980/1182/0, Loss: 917.24609375\n","Train -> sample/numSamples/epoch: 981/1182/0, Loss: 1298.234619140625\n","Train -> sample/numSamples/epoch: 982/1182/0, Loss: 931.297119140625\n","Train -> sample/numSamples/epoch: 983/1182/0, Loss: 1128.8311767578125\n","Train -> sample/numSamples/epoch: 984/1182/0, Loss: 828.3184204101562\n","Train -> sample/numSamples/epoch: 985/1182/0, Loss: 1026.23388671875\n","Train -> sample/numSamples/epoch: 986/1182/0, Loss: 812.4710693359375\n","Train -> sample/numSamples/epoch: 987/1182/0, Loss: 1049.575927734375\n","Train -> sample/numSamples/epoch: 988/1182/0, Loss: 1255.7392578125\n","Train -> sample/numSamples/epoch: 989/1182/0, Loss: 642.9020385742188\n","Train -> sample/numSamples/epoch: 990/1182/0, Loss: 765.5\n","Train -> sample/numSamples/epoch: 991/1182/0, Loss: 1000.2513427734375\n","Train -> sample/numSamples/epoch: 992/1182/0, Loss: 1158.5625\n","Train -> sample/numSamples/epoch: 993/1182/0, Loss: 1325.019775390625\n","Train -> sample/numSamples/epoch: 994/1182/0, Loss: 826.683349609375\n","Train -> sample/numSamples/epoch: 995/1182/0, Loss: 1155.1824951171875\n","Train -> sample/numSamples/epoch: 996/1182/0, Loss: 1166.8182373046875\n","Train -> sample/numSamples/epoch: 997/1182/0, Loss: 1357.8421630859375\n","Train -> sample/numSamples/epoch: 998/1182/0, Loss: 897.5845947265625\n","Train -> sample/numSamples/epoch: 999/1182/0, Loss: 1433.146240234375\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1000/1182/0, Loss: 1010.6915893554688\n","Train -> sample/numSamples/epoch: 1001/1182/0, Loss: 1455.2939453125\n","Train -> sample/numSamples/epoch: 1002/1182/0, Loss: 810.5584106445312\n","Train -> sample/numSamples/epoch: 1003/1182/0, Loss: 1129.6461181640625\n","Train -> sample/numSamples/epoch: 1004/1182/0, Loss: 1026.258056640625\n","Train -> sample/numSamples/epoch: 1005/1182/0, Loss: 1077.99658203125\n","Train -> sample/numSamples/epoch: 1006/1182/0, Loss: 993.4760131835938\n","Train -> sample/numSamples/epoch: 1007/1182/0, Loss: 1006.8485107421875\n","Train -> sample/numSamples/epoch: 1008/1182/0, Loss: 707.8353271484375\n","Train -> sample/numSamples/epoch: 1009/1182/0, Loss: 1082.78369140625\n","Train -> sample/numSamples/epoch: 1010/1182/0, Loss: 820.2572631835938\n","Train -> sample/numSamples/epoch: 1011/1182/0, Loss: 1033.529541015625\n","Train -> sample/numSamples/epoch: 1012/1182/0, Loss: 1466.3355712890625\n","Train -> sample/numSamples/epoch: 1013/1182/0, Loss: 1390.2318115234375\n","Train -> sample/numSamples/epoch: 1014/1182/0, Loss: 833.71875\n","Train -> sample/numSamples/epoch: 1015/1182/0, Loss: 1344.81689453125\n","Train -> sample/numSamples/epoch: 1016/1182/0, Loss: 821.9317626953125\n","Train -> sample/numSamples/epoch: 1017/1182/0, Loss: 969.57421875\n","Train -> sample/numSamples/epoch: 1018/1182/0, Loss: 964.5995483398438\n","Train -> sample/numSamples/epoch: 1019/1182/0, Loss: 1022.4613647460938\n","Train -> sample/numSamples/epoch: 1020/1182/0, Loss: 691.7832641601562\n","Train -> sample/numSamples/epoch: 1021/1182/0, Loss: 1335.050537109375\n","Train -> sample/numSamples/epoch: 1022/1182/0, Loss: 883.4658203125\n","Train -> sample/numSamples/epoch: 1023/1182/0, Loss: 1310.69921875\n","Train -> sample/numSamples/epoch: 1024/1182/0, Loss: 1117.4134521484375\n","Train -> sample/numSamples/epoch: 1025/1182/0, Loss: 1145.23486328125\n","Train -> sample/numSamples/epoch: 1026/1182/0, Loss: 1188.5338134765625\n","Train -> sample/numSamples/epoch: 1027/1182/0, Loss: 804.3253173828125\n","Train -> sample/numSamples/epoch: 1028/1182/0, Loss: 1325.194580078125\n","Train -> sample/numSamples/epoch: 1029/1182/0, Loss: 880.1212158203125\n","Train -> sample/numSamples/epoch: 1030/1182/0, Loss: 1184.497314453125\n","Train -> sample/numSamples/epoch: 1031/1182/0, Loss: 1083.27197265625\n","Train -> sample/numSamples/epoch: 1032/1182/0, Loss: 1142.7613525390625\n","Train -> sample/numSamples/epoch: 1033/1182/0, Loss: 1198.5113525390625\n","Train -> sample/numSamples/epoch: 1034/1182/0, Loss: 1002.9957275390625\n","Train -> sample/numSamples/epoch: 1035/1182/0, Loss: 1061.408447265625\n","Train -> sample/numSamples/epoch: 1036/1182/0, Loss: 956.8403930664062\n","Train -> sample/numSamples/epoch: 1037/1182/0, Loss: 855.6781616210938\n","Train -> sample/numSamples/epoch: 1038/1182/0, Loss: 1129.0157470703125\n","Train -> sample/numSamples/epoch: 1039/1182/0, Loss: 1206.3572998046875\n","Train -> sample/numSamples/epoch: 1040/1182/0, Loss: 1400.236083984375\n","Train -> sample/numSamples/epoch: 1041/1182/0, Loss: 1307.41845703125\n","Train -> sample/numSamples/epoch: 1042/1182/0, Loss: 965.25146484375\n","Train -> sample/numSamples/epoch: 1043/1182/0, Loss: 939.0571899414062\n","Train -> sample/numSamples/epoch: 1044/1182/0, Loss: 957.9988403320312\n","Train -> sample/numSamples/epoch: 1045/1182/0, Loss: 1147.9427490234375\n","Train -> sample/numSamples/epoch: 1046/1182/0, Loss: 907.3851928710938\n","Train -> sample/numSamples/epoch: 1047/1182/0, Loss: 988.3204345703125\n","Train -> sample/numSamples/epoch: 1048/1182/0, Loss: 768.2857666015625\n","Train -> sample/numSamples/epoch: 1049/1182/0, Loss: 1310.6107177734375\n","Train -> sample/numSamples/epoch: 1050/1182/0, Loss: 617.8942260742188\n","Train -> sample/numSamples/epoch: 1051/1182/0, Loss: 978.9080200195312\n","Train -> sample/numSamples/epoch: 1052/1182/0, Loss: 1041.625244140625\n","Train -> sample/numSamples/epoch: 1053/1182/0, Loss: 919.679931640625\n","Train -> sample/numSamples/epoch: 1054/1182/0, Loss: 1130.221923828125\n","Train -> sample/numSamples/epoch: 1055/1182/0, Loss: 930.7596435546875\n","Train -> sample/numSamples/epoch: 1056/1182/0, Loss: 671.2095947265625\n","Train -> sample/numSamples/epoch: 1057/1182/0, Loss: 1040.662841796875\n","Train -> sample/numSamples/epoch: 1058/1182/0, Loss: 1041.34912109375\n","Train -> sample/numSamples/epoch: 1059/1182/0, Loss: 689.2227783203125\n","Train -> sample/numSamples/epoch: 1060/1182/0, Loss: 1121.2342529296875\n","Train -> sample/numSamples/epoch: 1061/1182/0, Loss: 1160.60888671875\n","Train -> sample/numSamples/epoch: 1062/1182/0, Loss: 1206.827392578125\n","Train -> sample/numSamples/epoch: 1063/1182/0, Loss: 1400.4677734375\n","Train -> sample/numSamples/epoch: 1064/1182/0, Loss: 1103.526123046875\n","Train -> sample/numSamples/epoch: 1065/1182/0, Loss: 1175.285400390625\n","Train -> sample/numSamples/epoch: 1066/1182/0, Loss: 909.979248046875\n","Train -> sample/numSamples/epoch: 1067/1182/0, Loss: 1174.069580078125\n","Train -> sample/numSamples/epoch: 1068/1182/0, Loss: 829.8046875\n","Train -> sample/numSamples/epoch: 1069/1182/0, Loss: 1343.4471435546875\n","Train -> sample/numSamples/epoch: 1070/1182/0, Loss: 1218.7093505859375\n","Train -> sample/numSamples/epoch: 1071/1182/0, Loss: 1402.4676513671875\n","Train -> sample/numSamples/epoch: 1072/1182/0, Loss: 976.60009765625\n","Train -> sample/numSamples/epoch: 1073/1182/0, Loss: 1178.8551025390625\n","Train -> sample/numSamples/epoch: 1074/1182/0, Loss: 850.5774536132812\n","Train -> sample/numSamples/epoch: 1075/1182/0, Loss: 1286.84375\n","Train -> sample/numSamples/epoch: 1076/1182/0, Loss: 1443.758544921875\n","Train -> sample/numSamples/epoch: 1077/1182/0, Loss: 1328.9276123046875\n","Train -> sample/numSamples/epoch: 1078/1182/0, Loss: 1027.0347900390625\n","Train -> sample/numSamples/epoch: 1079/1182/0, Loss: 605.5805053710938\n","Train -> sample/numSamples/epoch: 1080/1182/0, Loss: 1266.7691650390625\n","Train -> sample/numSamples/epoch: 1081/1182/0, Loss: 1030.6783447265625\n","Train -> sample/numSamples/epoch: 1082/1182/0, Loss: 748.0654907226562\n","Train -> sample/numSamples/epoch: 1083/1182/0, Loss: 997.0913696289062\n","Train -> sample/numSamples/epoch: 1084/1182/0, Loss: 853.1245727539062\n","Train -> sample/numSamples/epoch: 1085/1182/0, Loss: 790.8777465820312\n","Train -> sample/numSamples/epoch: 1086/1182/0, Loss: 995.9820556640625\n","Train -> sample/numSamples/epoch: 1087/1182/0, Loss: 1479.213134765625\n","Train -> sample/numSamples/epoch: 1088/1182/0, Loss: 1003.3024291992188\n","Train -> sample/numSamples/epoch: 1089/1182/0, Loss: 1318.25732421875\n","Train -> sample/numSamples/epoch: 1090/1182/0, Loss: 1465.0552978515625\n","Train -> sample/numSamples/epoch: 1091/1182/0, Loss: 902.4107666015625\n","Train -> sample/numSamples/epoch: 1092/1182/0, Loss: 1181.7705078125\n","Train -> sample/numSamples/epoch: 1093/1182/0, Loss: 1233.651123046875\n","Train -> sample/numSamples/epoch: 1094/1182/0, Loss: 769.7333374023438\n","Train -> sample/numSamples/epoch: 1095/1182/0, Loss: 1173.5545654296875\n","Train -> sample/numSamples/epoch: 1096/1182/0, Loss: 949.9691162109375\n","Train -> sample/numSamples/epoch: 1097/1182/0, Loss: 644.8828735351562\n","Train -> sample/numSamples/epoch: 1098/1182/0, Loss: 1224.046142578125\n","Train -> sample/numSamples/epoch: 1099/1182/0, Loss: 783.9879150390625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1100/1182/0, Loss: 1080.3724365234375\n","Train -> sample/numSamples/epoch: 1101/1182/0, Loss: 1038.2957763671875\n","Train -> sample/numSamples/epoch: 1102/1182/0, Loss: 935.0662841796875\n","Train -> sample/numSamples/epoch: 1103/1182/0, Loss: 869.505859375\n","Train -> sample/numSamples/epoch: 1104/1182/0, Loss: 749.6047973632812\n","Train -> sample/numSamples/epoch: 1105/1182/0, Loss: 722.8695068359375\n","Train -> sample/numSamples/epoch: 1106/1182/0, Loss: 1535.6077880859375\n","Train -> sample/numSamples/epoch: 1107/1182/0, Loss: 1110.0069580078125\n","Train -> sample/numSamples/epoch: 1108/1182/0, Loss: 1024.0487060546875\n","Train -> sample/numSamples/epoch: 1109/1182/0, Loss: 1012.547119140625\n","Train -> sample/numSamples/epoch: 1110/1182/0, Loss: 1176.187744140625\n","Train -> sample/numSamples/epoch: 1111/1182/0, Loss: 735.960693359375\n","Train -> sample/numSamples/epoch: 1112/1182/0, Loss: 1165.9473876953125\n","Train -> sample/numSamples/epoch: 1113/1182/0, Loss: 1430.021240234375\n","Train -> sample/numSamples/epoch: 1114/1182/0, Loss: 1332.1439208984375\n","Train -> sample/numSamples/epoch: 1115/1182/0, Loss: 1225.369140625\n","Train -> sample/numSamples/epoch: 1116/1182/0, Loss: 817.582275390625\n","Train -> sample/numSamples/epoch: 1117/1182/0, Loss: 1410.259765625\n","Train -> sample/numSamples/epoch: 1118/1182/0, Loss: 1309.19677734375\n","Train -> sample/numSamples/epoch: 1119/1182/0, Loss: 1494.587890625\n","Train -> sample/numSamples/epoch: 1120/1182/0, Loss: 1223.4019775390625\n","Train -> sample/numSamples/epoch: 1121/1182/0, Loss: 1191.7210693359375\n","Train -> sample/numSamples/epoch: 1122/1182/0, Loss: 1018.6475219726562\n","Train -> sample/numSamples/epoch: 1123/1182/0, Loss: 567.7591552734375\n","Train -> sample/numSamples/epoch: 1124/1182/0, Loss: 1306.2054443359375\n","Train -> sample/numSamples/epoch: 1125/1182/0, Loss: 857.1235961914062\n","Train -> sample/numSamples/epoch: 1126/1182/0, Loss: 951.9382934570312\n","Train -> sample/numSamples/epoch: 1127/1182/0, Loss: 1352.52880859375\n","Train -> sample/numSamples/epoch: 1128/1182/0, Loss: 707.14111328125\n","Train -> sample/numSamples/epoch: 1129/1182/0, Loss: 1297.1865234375\n","Train -> sample/numSamples/epoch: 1130/1182/0, Loss: 1157.674072265625\n","Train -> sample/numSamples/epoch: 1131/1182/0, Loss: 1053.7056884765625\n","Train -> sample/numSamples/epoch: 1132/1182/0, Loss: 1035.785400390625\n","Train -> sample/numSamples/epoch: 1133/1182/0, Loss: 1328.299560546875\n","Train -> sample/numSamples/epoch: 1134/1182/0, Loss: 776.772705078125\n","Train -> sample/numSamples/epoch: 1135/1182/0, Loss: 875.8829956054688\n","Train -> sample/numSamples/epoch: 1136/1182/0, Loss: 1113.3468017578125\n","Train -> sample/numSamples/epoch: 1137/1182/0, Loss: 1019.6077270507812\n","Train -> sample/numSamples/epoch: 1138/1182/0, Loss: 1300.2510986328125\n","Train -> sample/numSamples/epoch: 1139/1182/0, Loss: 1003.2548828125\n","Train -> sample/numSamples/epoch: 1140/1182/0, Loss: 1154.938720703125\n","Train -> sample/numSamples/epoch: 1141/1182/0, Loss: 662.8380126953125\n","Train -> sample/numSamples/epoch: 1142/1182/0, Loss: 812.0343017578125\n","Train -> sample/numSamples/epoch: 1143/1182/0, Loss: 1144.2962646484375\n","Train -> sample/numSamples/epoch: 1144/1182/0, Loss: 1170.08984375\n","Train -> sample/numSamples/epoch: 1145/1182/0, Loss: 1190.0572509765625\n","Train -> sample/numSamples/epoch: 1146/1182/0, Loss: 974.099609375\n","Train -> sample/numSamples/epoch: 1147/1182/0, Loss: 1169.8963623046875\n","Train -> sample/numSamples/epoch: 1148/1182/0, Loss: 1163.241943359375\n","Train -> sample/numSamples/epoch: 1149/1182/0, Loss: 1328.271484375\n","Train -> sample/numSamples/epoch: 1150/1182/0, Loss: 875.407958984375\n","Train -> sample/numSamples/epoch: 1151/1182/0, Loss: 1445.9219970703125\n","Train -> sample/numSamples/epoch: 1152/1182/0, Loss: 1180.698486328125\n","Train -> sample/numSamples/epoch: 1153/1182/0, Loss: 945.5165405273438\n","Train -> sample/numSamples/epoch: 1154/1182/0, Loss: 1286.870361328125\n","Train -> sample/numSamples/epoch: 1155/1182/0, Loss: 926.2675170898438\n","Train -> sample/numSamples/epoch: 1156/1182/0, Loss: 1184.6783447265625\n","Train -> sample/numSamples/epoch: 1157/1182/0, Loss: 1191.416259765625\n","Train -> sample/numSamples/epoch: 1158/1182/0, Loss: 815.6016845703125\n","Train -> sample/numSamples/epoch: 1159/1182/0, Loss: 1041.86669921875\n","Train -> sample/numSamples/epoch: 1160/1182/0, Loss: 1092.7197265625\n","Train -> sample/numSamples/epoch: 1161/1182/0, Loss: 681.3185424804688\n","Train -> sample/numSamples/epoch: 1162/1182/0, Loss: 1387.21923828125\n","Train -> sample/numSamples/epoch: 1163/1182/0, Loss: 658.032958984375\n","Train -> sample/numSamples/epoch: 1164/1182/0, Loss: 775.0908813476562\n","Train -> sample/numSamples/epoch: 1165/1182/0, Loss: 904.3108520507812\n","Train -> sample/numSamples/epoch: 1166/1182/0, Loss: 1029.52685546875\n","Train -> sample/numSamples/epoch: 1167/1182/0, Loss: 1449.2686767578125\n","Train -> sample/numSamples/epoch: 1168/1182/0, Loss: 1292.6231689453125\n","Train -> sample/numSamples/epoch: 1169/1182/0, Loss: 916.4464111328125\n","Train -> sample/numSamples/epoch: 1170/1182/0, Loss: 732.6680297851562\n","Train -> sample/numSamples/epoch: 1171/1182/0, Loss: 590.7789916992188\n","Train -> sample/numSamples/epoch: 1172/1182/0, Loss: 1024.640380859375\n","Train -> sample/numSamples/epoch: 1173/1182/0, Loss: 1316.954345703125\n","Train -> sample/numSamples/epoch: 1174/1182/0, Loss: 669.9991455078125\n","Train -> sample/numSamples/epoch: 1175/1182/0, Loss: 1166.6390380859375\n","Train -> sample/numSamples/epoch: 1176/1182/0, Loss: 1645.057373046875\n","Train -> sample/numSamples/epoch: 1177/1182/0, Loss: 1214.438232421875\n","Train -> sample/numSamples/epoch: 1178/1182/0, Loss: 1135.9637451171875\n","Train -> sample/numSamples/epoch: 1179/1182/0, Loss: 988.7024536132812\n","Train -> sample/numSamples/epoch: 1180/1182/0, Loss: 1018.4083251953125\n","Train -> sample/numSamples/epoch: 1181/1182/0, Loss: 734.8864135742188\n","Train -> sample/numSamples/epoch: 0/1182/1, Loss: 984.0114135742188\n","Train -> sample/numSamples/epoch: 1/1182/1, Loss: 986.7384643554688\n","Train -> sample/numSamples/epoch: 2/1182/1, Loss: 1641.57177734375\n","Train -> sample/numSamples/epoch: 3/1182/1, Loss: 1223.5948486328125\n","Train -> sample/numSamples/epoch: 4/1182/1, Loss: 1121.7972412109375\n","Train -> sample/numSamples/epoch: 5/1182/1, Loss: 824.400146484375\n","Train -> sample/numSamples/epoch: 6/1182/1, Loss: 731.9407348632812\n","Train -> sample/numSamples/epoch: 7/1182/1, Loss: 911.33837890625\n","Train -> sample/numSamples/epoch: 8/1182/1, Loss: 1350.6585693359375\n","Train -> sample/numSamples/epoch: 9/1182/1, Loss: 912.0798950195312\n","Train -> sample/numSamples/epoch: 10/1182/1, Loss: 1015.19140625\n","Train -> sample/numSamples/epoch: 11/1182/1, Loss: 1216.478271484375\n","Train -> sample/numSamples/epoch: 12/1182/1, Loss: 1068.6622314453125\n","Train -> sample/numSamples/epoch: 13/1182/1, Loss: 929.9959106445312\n","Train -> sample/numSamples/epoch: 14/1182/1, Loss: 879.4833374023438\n","Train -> sample/numSamples/epoch: 15/1182/1, Loss: 1084.0062255859375\n","Train -> sample/numSamples/epoch: 16/1182/1, Loss: 842.096435546875\n","Train -> sample/numSamples/epoch: 17/1182/1, Loss: 1010.78466796875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 18/1182/1, Loss: 960.2583618164062\n","Train -> sample/numSamples/epoch: 19/1182/1, Loss: 1029.1048583984375\n","Train -> sample/numSamples/epoch: 20/1182/1, Loss: 1192.830810546875\n","Train -> sample/numSamples/epoch: 21/1182/1, Loss: 1160.294677734375\n","Train -> sample/numSamples/epoch: 22/1182/1, Loss: 1130.325927734375\n","Train -> sample/numSamples/epoch: 23/1182/1, Loss: 815.6162719726562\n","Train -> sample/numSamples/epoch: 24/1182/1, Loss: 1047.0880126953125\n","Train -> sample/numSamples/epoch: 25/1182/1, Loss: 1121.378173828125\n","Train -> sample/numSamples/epoch: 26/1182/1, Loss: 1348.804443359375\n","Train -> sample/numSamples/epoch: 27/1182/1, Loss: 804.7993774414062\n","Train -> sample/numSamples/epoch: 28/1182/1, Loss: 805.9246826171875\n","Train -> sample/numSamples/epoch: 29/1182/1, Loss: 1119.4427490234375\n","Train -> sample/numSamples/epoch: 30/1182/1, Loss: 1228.8079833984375\n","Train -> sample/numSamples/epoch: 31/1182/1, Loss: 1108.1826171875\n","Train -> sample/numSamples/epoch: 32/1182/1, Loss: 1387.7696533203125\n","Train -> sample/numSamples/epoch: 33/1182/1, Loss: 773.8875122070312\n","Train -> sample/numSamples/epoch: 34/1182/1, Loss: 963.7759399414062\n","Train -> sample/numSamples/epoch: 35/1182/1, Loss: 796.7184448242188\n","Train -> sample/numSamples/epoch: 36/1182/1, Loss: 1072.1866455078125\n","Train -> sample/numSamples/epoch: 37/1182/1, Loss: 869.0538330078125\n","Train -> sample/numSamples/epoch: 38/1182/1, Loss: 1384.5567626953125\n","Train -> sample/numSamples/epoch: 39/1182/1, Loss: 1361.6025390625\n","Train -> sample/numSamples/epoch: 40/1182/1, Loss: 763.6973876953125\n","Train -> sample/numSamples/epoch: 41/1182/1, Loss: 944.6292114257812\n","Train -> sample/numSamples/epoch: 42/1182/1, Loss: 727.404296875\n","Train -> sample/numSamples/epoch: 43/1182/1, Loss: 614.0741577148438\n","Train -> sample/numSamples/epoch: 44/1182/1, Loss: 1129.6805419921875\n","Train -> sample/numSamples/epoch: 45/1182/1, Loss: 965.4786376953125\n","Train -> sample/numSamples/epoch: 46/1182/1, Loss: 806.8079223632812\n","Train -> sample/numSamples/epoch: 47/1182/1, Loss: 774.2273559570312\n","Train -> sample/numSamples/epoch: 48/1182/1, Loss: 902.7349853515625\n","Train -> sample/numSamples/epoch: 49/1182/1, Loss: 926.1146850585938\n","Train -> sample/numSamples/epoch: 50/1182/1, Loss: 506.2784118652344\n","Train -> sample/numSamples/epoch: 51/1182/1, Loss: 962.803466796875\n","Train -> sample/numSamples/epoch: 52/1182/1, Loss: 973.1826171875\n","Train -> sample/numSamples/epoch: 53/1182/1, Loss: 924.5293579101562\n","Train -> sample/numSamples/epoch: 54/1182/1, Loss: 1183.6490478515625\n","Train -> sample/numSamples/epoch: 55/1182/1, Loss: 938.373779296875\n","Train -> sample/numSamples/epoch: 56/1182/1, Loss: 1168.2769775390625\n","Train -> sample/numSamples/epoch: 57/1182/1, Loss: 1246.7918701171875\n","Train -> sample/numSamples/epoch: 58/1182/1, Loss: 984.2424926757812\n","Train -> sample/numSamples/epoch: 59/1182/1, Loss: 1070.34912109375\n","Train -> sample/numSamples/epoch: 60/1182/1, Loss: 1289.3994140625\n","Train -> sample/numSamples/epoch: 61/1182/1, Loss: 2305.625732421875\n","Train -> sample/numSamples/epoch: 62/1182/1, Loss: 1082.10791015625\n","Train -> sample/numSamples/epoch: 63/1182/1, Loss: 1285.5279541015625\n","Train -> sample/numSamples/epoch: 64/1182/1, Loss: 880.4723510742188\n","Train -> sample/numSamples/epoch: 65/1182/1, Loss: 1241.43115234375\n","Train -> sample/numSamples/epoch: 66/1182/1, Loss: 1158.026123046875\n","Train -> sample/numSamples/epoch: 67/1182/1, Loss: 1316.73486328125\n","Train -> sample/numSamples/epoch: 68/1182/1, Loss: 1439.5140380859375\n","Train -> sample/numSamples/epoch: 69/1182/1, Loss: 897.6670532226562\n","Train -> sample/numSamples/epoch: 70/1182/1, Loss: 770.2800903320312\n","Train -> sample/numSamples/epoch: 71/1182/1, Loss: 871.5577392578125\n","Train -> sample/numSamples/epoch: 72/1182/1, Loss: 903.7553100585938\n","Train -> sample/numSamples/epoch: 73/1182/1, Loss: 1024.9276123046875\n","Train -> sample/numSamples/epoch: 74/1182/1, Loss: 951.3358154296875\n","Train -> sample/numSamples/epoch: 75/1182/1, Loss: 1215.349365234375\n","Train -> sample/numSamples/epoch: 76/1182/1, Loss: 961.7355346679688\n","Train -> sample/numSamples/epoch: 77/1182/1, Loss: 973.9180297851562\n","Train -> sample/numSamples/epoch: 78/1182/1, Loss: 1052.0186767578125\n","Train -> sample/numSamples/epoch: 79/1182/1, Loss: 1455.4871826171875\n","Train -> sample/numSamples/epoch: 80/1182/1, Loss: 1476.1829833984375\n","Train -> sample/numSamples/epoch: 81/1182/1, Loss: 1281.260498046875\n","Train -> sample/numSamples/epoch: 82/1182/1, Loss: 837.0503540039062\n","Train -> sample/numSamples/epoch: 83/1182/1, Loss: 873.4219970703125\n","Train -> sample/numSamples/epoch: 84/1182/1, Loss: 774.9329833984375\n","Train -> sample/numSamples/epoch: 85/1182/1, Loss: 1128.5194091796875\n","Train -> sample/numSamples/epoch: 86/1182/1, Loss: 1167.8377685546875\n","Train -> sample/numSamples/epoch: 87/1182/1, Loss: 1595.5579833984375\n","Train -> sample/numSamples/epoch: 88/1182/1, Loss: 742.7137451171875\n","Train -> sample/numSamples/epoch: 89/1182/1, Loss: 1421.982666015625\n","Train -> sample/numSamples/epoch: 90/1182/1, Loss: 804.93505859375\n","Train -> sample/numSamples/epoch: 91/1182/1, Loss: 1317.3089599609375\n","Train -> sample/numSamples/epoch: 92/1182/1, Loss: 872.3999633789062\n","Train -> sample/numSamples/epoch: 93/1182/1, Loss: 990.2999877929688\n","Train -> sample/numSamples/epoch: 94/1182/1, Loss: 1129.4400634765625\n","Train -> sample/numSamples/epoch: 95/1182/1, Loss: 1184.2071533203125\n","Train -> sample/numSamples/epoch: 96/1182/1, Loss: 1029.01611328125\n","Train -> sample/numSamples/epoch: 97/1182/1, Loss: 980.748291015625\n","Train -> sample/numSamples/epoch: 98/1182/1, Loss: 1289.14453125\n","Train -> sample/numSamples/epoch: 99/1182/1, Loss: 1269.5694580078125\n","Train -> sample/numSamples/epoch: 100/1182/1, Loss: 973.064453125\n","Train -> sample/numSamples/epoch: 101/1182/1, Loss: 1206.9039306640625\n","Train -> sample/numSamples/epoch: 102/1182/1, Loss: 619.26171875\n","Train -> sample/numSamples/epoch: 103/1182/1, Loss: 947.867431640625\n","Train -> sample/numSamples/epoch: 104/1182/1, Loss: 969.0405883789062\n","Train -> sample/numSamples/epoch: 105/1182/1, Loss: 807.8085327148438\n","Train -> sample/numSamples/epoch: 106/1182/1, Loss: 974.4728393554688\n","Train -> sample/numSamples/epoch: 107/1182/1, Loss: 1212.427490234375\n","Train -> sample/numSamples/epoch: 108/1182/1, Loss: 1103.355224609375\n","Train -> sample/numSamples/epoch: 109/1182/1, Loss: 790.5304565429688\n","Train -> sample/numSamples/epoch: 110/1182/1, Loss: 1016.7571411132812\n","Train -> sample/numSamples/epoch: 111/1182/1, Loss: 1051.2298583984375\n","Train -> sample/numSamples/epoch: 112/1182/1, Loss: 694.8220825195312\n","Train -> sample/numSamples/epoch: 113/1182/1, Loss: 1180.0172119140625\n","Train -> sample/numSamples/epoch: 114/1182/1, Loss: 724.43212890625\n","Train -> sample/numSamples/epoch: 115/1182/1, Loss: 836.5088500976562\n","Train -> sample/numSamples/epoch: 116/1182/1, Loss: 1046.8790283203125\n","Train -> sample/numSamples/epoch: 117/1182/1, Loss: 1049.137451171875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 118/1182/1, Loss: 850.214111328125\n","Train -> sample/numSamples/epoch: 119/1182/1, Loss: 1060.4951171875\n","Train -> sample/numSamples/epoch: 120/1182/1, Loss: 1061.5740966796875\n","Train -> sample/numSamples/epoch: 121/1182/1, Loss: 1183.9063720703125\n","Train -> sample/numSamples/epoch: 122/1182/1, Loss: 838.032470703125\n","Train -> sample/numSamples/epoch: 123/1182/1, Loss: 991.889404296875\n","Train -> sample/numSamples/epoch: 124/1182/1, Loss: 1092.480224609375\n","Train -> sample/numSamples/epoch: 125/1182/1, Loss: 1390.9837646484375\n","Train -> sample/numSamples/epoch: 126/1182/1, Loss: 1227.783447265625\n","Train -> sample/numSamples/epoch: 127/1182/1, Loss: 1103.58642578125\n","Train -> sample/numSamples/epoch: 128/1182/1, Loss: 1096.8411865234375\n","Train -> sample/numSamples/epoch: 129/1182/1, Loss: 1031.8721923828125\n","Train -> sample/numSamples/epoch: 130/1182/1, Loss: 820.3978271484375\n","Train -> sample/numSamples/epoch: 131/1182/1, Loss: 1297.147705078125\n","Train -> sample/numSamples/epoch: 132/1182/1, Loss: 965.5108642578125\n","Train -> sample/numSamples/epoch: 133/1182/1, Loss: 1614.3365478515625\n","Train -> sample/numSamples/epoch: 134/1182/1, Loss: 983.811279296875\n","Train -> sample/numSamples/epoch: 135/1182/1, Loss: 1268.1397705078125\n","Train -> sample/numSamples/epoch: 136/1182/1, Loss: 1218.3919677734375\n","Train -> sample/numSamples/epoch: 137/1182/1, Loss: 990.2753295898438\n","Train -> sample/numSamples/epoch: 138/1182/1, Loss: 1108.672607421875\n","Train -> sample/numSamples/epoch: 139/1182/1, Loss: 658.9198608398438\n","Train -> sample/numSamples/epoch: 140/1182/1, Loss: 1311.270263671875\n","Train -> sample/numSamples/epoch: 141/1182/1, Loss: 1194.16357421875\n","Train -> sample/numSamples/epoch: 142/1182/1, Loss: 905.2785034179688\n","Train -> sample/numSamples/epoch: 143/1182/1, Loss: 1589.78173828125\n","Train -> sample/numSamples/epoch: 144/1182/1, Loss: 1490.1370849609375\n","Train -> sample/numSamples/epoch: 145/1182/1, Loss: 1289.9771728515625\n","Train -> sample/numSamples/epoch: 146/1182/1, Loss: 1137.0869140625\n","Train -> sample/numSamples/epoch: 147/1182/1, Loss: 764.3809204101562\n","Train -> sample/numSamples/epoch: 148/1182/1, Loss: 802.956298828125\n","Train -> sample/numSamples/epoch: 149/1182/1, Loss: 1055.018310546875\n","Train -> sample/numSamples/epoch: 150/1182/1, Loss: 257.7432861328125\n","Train -> sample/numSamples/epoch: 151/1182/1, Loss: 803.3034057617188\n","Train -> sample/numSamples/epoch: 152/1182/1, Loss: 742.0053100585938\n","Train -> sample/numSamples/epoch: 153/1182/1, Loss: 865.2117309570312\n","Train -> sample/numSamples/epoch: 154/1182/1, Loss: 1094.8492431640625\n","Train -> sample/numSamples/epoch: 155/1182/1, Loss: 1317.474609375\n","Train -> sample/numSamples/epoch: 156/1182/1, Loss: 1004.0123291015625\n","Train -> sample/numSamples/epoch: 157/1182/1, Loss: 1265.759521484375\n","Train -> sample/numSamples/epoch: 158/1182/1, Loss: 954.9034423828125\n","Train -> sample/numSamples/epoch: 159/1182/1, Loss: 970.8609619140625\n","Train -> sample/numSamples/epoch: 160/1182/1, Loss: 926.0137939453125\n","Train -> sample/numSamples/epoch: 161/1182/1, Loss: 489.92578125\n","Train -> sample/numSamples/epoch: 162/1182/1, Loss: 939.62939453125\n","Train -> sample/numSamples/epoch: 163/1182/1, Loss: 809.0250854492188\n","Train -> sample/numSamples/epoch: 164/1182/1, Loss: 963.550048828125\n","Train -> sample/numSamples/epoch: 165/1182/1, Loss: 615.7647705078125\n","Train -> sample/numSamples/epoch: 166/1182/1, Loss: 1230.1435546875\n","Train -> sample/numSamples/epoch: 167/1182/1, Loss: 942.5507202148438\n","Train -> sample/numSamples/epoch: 168/1182/1, Loss: 731.297607421875\n","Train -> sample/numSamples/epoch: 169/1182/1, Loss: 1136.5855712890625\n","Train -> sample/numSamples/epoch: 170/1182/1, Loss: 951.1951904296875\n","Train -> sample/numSamples/epoch: 171/1182/1, Loss: 1421.7503662109375\n","Train -> sample/numSamples/epoch: 172/1182/1, Loss: 562.045166015625\n","Train -> sample/numSamples/epoch: 173/1182/1, Loss: 1106.645751953125\n","Train -> sample/numSamples/epoch: 174/1182/1, Loss: 1128.613525390625\n","Train -> sample/numSamples/epoch: 175/1182/1, Loss: 641.2282104492188\n","Train -> sample/numSamples/epoch: 176/1182/1, Loss: 1106.9886474609375\n","Train -> sample/numSamples/epoch: 177/1182/1, Loss: 1093.5150146484375\n","Train -> sample/numSamples/epoch: 178/1182/1, Loss: 1113.431640625\n","Train -> sample/numSamples/epoch: 179/1182/1, Loss: 821.450927734375\n","Train -> sample/numSamples/epoch: 180/1182/1, Loss: 1339.9158935546875\n","Train -> sample/numSamples/epoch: 181/1182/1, Loss: 1161.0218505859375\n","Train -> sample/numSamples/epoch: 182/1182/1, Loss: 1030.314697265625\n","Train -> sample/numSamples/epoch: 183/1182/1, Loss: 846.9752807617188\n","Train -> sample/numSamples/epoch: 184/1182/1, Loss: 877.6692504882812\n","Train -> sample/numSamples/epoch: 185/1182/1, Loss: 1140.9417724609375\n","Train -> sample/numSamples/epoch: 186/1182/1, Loss: 1004.7979125976562\n","Train -> sample/numSamples/epoch: 187/1182/1, Loss: 928.25927734375\n","Train -> sample/numSamples/epoch: 188/1182/1, Loss: 1335.91552734375\n","Train -> sample/numSamples/epoch: 189/1182/1, Loss: 1070.5589599609375\n","Train -> sample/numSamples/epoch: 190/1182/1, Loss: 1252.2108154296875\n","Train -> sample/numSamples/epoch: 191/1182/1, Loss: 1247.3736572265625\n","Train -> sample/numSamples/epoch: 192/1182/1, Loss: 1262.1395263671875\n","Train -> sample/numSamples/epoch: 193/1182/1, Loss: 919.4628295898438\n","Train -> sample/numSamples/epoch: 194/1182/1, Loss: 1230.999267578125\n","Train -> sample/numSamples/epoch: 195/1182/1, Loss: 1220.1156005859375\n","Train -> sample/numSamples/epoch: 196/1182/1, Loss: 1824.5450439453125\n","Train -> sample/numSamples/epoch: 197/1182/1, Loss: 907.9971313476562\n","Train -> sample/numSamples/epoch: 198/1182/1, Loss: 880.2877197265625\n","Train -> sample/numSamples/epoch: 199/1182/1, Loss: 1103.952392578125\n","Train -> sample/numSamples/epoch: 200/1182/1, Loss: 1589.23193359375\n","Train -> sample/numSamples/epoch: 201/1182/1, Loss: 1025.8521728515625\n","Train -> sample/numSamples/epoch: 202/1182/1, Loss: 984.8797607421875\n","Train -> sample/numSamples/epoch: 203/1182/1, Loss: 691.6121826171875\n","Train -> sample/numSamples/epoch: 204/1182/1, Loss: 1052.196533203125\n","Train -> sample/numSamples/epoch: 205/1182/1, Loss: 1266.4825439453125\n","Train -> sample/numSamples/epoch: 206/1182/1, Loss: 860.0696411132812\n","Train -> sample/numSamples/epoch: 207/1182/1, Loss: 1357.5736083984375\n","Train -> sample/numSamples/epoch: 208/1182/1, Loss: 1365.352294921875\n","Train -> sample/numSamples/epoch: 209/1182/1, Loss: 1439.5120849609375\n","Train -> sample/numSamples/epoch: 210/1182/1, Loss: 1113.259765625\n","Train -> sample/numSamples/epoch: 211/1182/1, Loss: 475.5257263183594\n","Train -> sample/numSamples/epoch: 212/1182/1, Loss: 849.1286010742188\n","Train -> sample/numSamples/epoch: 213/1182/1, Loss: 1035.1981201171875\n","Train -> sample/numSamples/epoch: 214/1182/1, Loss: 1326.2142333984375\n","Train -> sample/numSamples/epoch: 215/1182/1, Loss: 950.5789184570312\n","Train -> sample/numSamples/epoch: 216/1182/1, Loss: 1402.429443359375\n","Train -> sample/numSamples/epoch: 217/1182/1, Loss: 1001.186767578125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 218/1182/1, Loss: 1324.1656494140625\n","Train -> sample/numSamples/epoch: 219/1182/1, Loss: 800.9338989257812\n","Train -> sample/numSamples/epoch: 220/1182/1, Loss: 864.399658203125\n","Train -> sample/numSamples/epoch: 221/1182/1, Loss: 1037.8150634765625\n","Train -> sample/numSamples/epoch: 222/1182/1, Loss: 1442.68408203125\n","Train -> sample/numSamples/epoch: 223/1182/1, Loss: 868.1010131835938\n","Train -> sample/numSamples/epoch: 224/1182/1, Loss: 1147.3778076171875\n","Train -> sample/numSamples/epoch: 225/1182/1, Loss: 1271.26513671875\n","Train -> sample/numSamples/epoch: 226/1182/1, Loss: 667.5831909179688\n","Train -> sample/numSamples/epoch: 227/1182/1, Loss: 1043.581298828125\n","Train -> sample/numSamples/epoch: 228/1182/1, Loss: 1424.976806640625\n","Train -> sample/numSamples/epoch: 229/1182/1, Loss: 947.5433349609375\n","Train -> sample/numSamples/epoch: 230/1182/1, Loss: 1074.7108154296875\n","Train -> sample/numSamples/epoch: 231/1182/1, Loss: 1414.5982666015625\n","Train -> sample/numSamples/epoch: 232/1182/1, Loss: 1133.9715576171875\n","Train -> sample/numSamples/epoch: 233/1182/1, Loss: 1164.037353515625\n","Train -> sample/numSamples/epoch: 234/1182/1, Loss: 1022.6514282226562\n","Train -> sample/numSamples/epoch: 235/1182/1, Loss: 1314.6458740234375\n","Train -> sample/numSamples/epoch: 236/1182/1, Loss: 982.3189697265625\n","Train -> sample/numSamples/epoch: 237/1182/1, Loss: 1356.474365234375\n","Train -> sample/numSamples/epoch: 238/1182/1, Loss: 742.6730346679688\n","Train -> sample/numSamples/epoch: 239/1182/1, Loss: 1474.828857421875\n","Train -> sample/numSamples/epoch: 240/1182/1, Loss: 708.9259033203125\n","Train -> sample/numSamples/epoch: 241/1182/1, Loss: 1218.25634765625\n","Train -> sample/numSamples/epoch: 242/1182/1, Loss: 1439.0068359375\n","Train -> sample/numSamples/epoch: 243/1182/1, Loss: 816.82421875\n","Train -> sample/numSamples/epoch: 244/1182/1, Loss: 673.737548828125\n","Train -> sample/numSamples/epoch: 245/1182/1, Loss: 1167.6298828125\n","Train -> sample/numSamples/epoch: 246/1182/1, Loss: 982.520751953125\n","Train -> sample/numSamples/epoch: 247/1182/1, Loss: 715.0562133789062\n","Train -> sample/numSamples/epoch: 248/1182/1, Loss: 764.6571655273438\n","Train -> sample/numSamples/epoch: 249/1182/1, Loss: 1400.8785400390625\n","Train -> sample/numSamples/epoch: 250/1182/1, Loss: 1182.4417724609375\n","Train -> sample/numSamples/epoch: 251/1182/1, Loss: 702.7678833007812\n","Train -> sample/numSamples/epoch: 252/1182/1, Loss: 1202.1279296875\n","Train -> sample/numSamples/epoch: 253/1182/1, Loss: 676.0816040039062\n","Train -> sample/numSamples/epoch: 254/1182/1, Loss: 1263.3680419921875\n","Train -> sample/numSamples/epoch: 255/1182/1, Loss: 980.7174682617188\n","Train -> sample/numSamples/epoch: 256/1182/1, Loss: 970.5338134765625\n","Train -> sample/numSamples/epoch: 257/1182/1, Loss: 756.1690673828125\n","Train -> sample/numSamples/epoch: 258/1182/1, Loss: 1129.221435546875\n","Train -> sample/numSamples/epoch: 259/1182/1, Loss: 838.4470825195312\n","Train -> sample/numSamples/epoch: 260/1182/1, Loss: 840.7716064453125\n","Train -> sample/numSamples/epoch: 261/1182/1, Loss: 887.5592651367188\n","Train -> sample/numSamples/epoch: 262/1182/1, Loss: 890.1400756835938\n","Train -> sample/numSamples/epoch: 263/1182/1, Loss: 896.3202514648438\n","Train -> sample/numSamples/epoch: 264/1182/1, Loss: 981.6557006835938\n","Train -> sample/numSamples/epoch: 265/1182/1, Loss: 1039.131591796875\n","Train -> sample/numSamples/epoch: 266/1182/1, Loss: 1336.352783203125\n","Train -> sample/numSamples/epoch: 267/1182/1, Loss: 812.094970703125\n","Train -> sample/numSamples/epoch: 268/1182/1, Loss: 1070.080810546875\n","Train -> sample/numSamples/epoch: 269/1182/1, Loss: 697.1983642578125\n","Train -> sample/numSamples/epoch: 270/1182/1, Loss: 1193.012451171875\n","Train -> sample/numSamples/epoch: 271/1182/1, Loss: 877.3833618164062\n","Train -> sample/numSamples/epoch: 272/1182/1, Loss: 1046.55078125\n","Train -> sample/numSamples/epoch: 273/1182/1, Loss: 1566.052490234375\n","Train -> sample/numSamples/epoch: 274/1182/1, Loss: 1234.9632568359375\n","Train -> sample/numSamples/epoch: 275/1182/1, Loss: 1168.154541015625\n","Train -> sample/numSamples/epoch: 276/1182/1, Loss: 981.43798828125\n","Train -> sample/numSamples/epoch: 277/1182/1, Loss: 1161.9049072265625\n","Train -> sample/numSamples/epoch: 278/1182/1, Loss: 1038.5345458984375\n","Train -> sample/numSamples/epoch: 279/1182/1, Loss: 982.4600219726562\n","Train -> sample/numSamples/epoch: 280/1182/1, Loss: 1192.538818359375\n","Train -> sample/numSamples/epoch: 281/1182/1, Loss: 998.9354248046875\n","Train -> sample/numSamples/epoch: 282/1182/1, Loss: 522.8845825195312\n","Train -> sample/numSamples/epoch: 283/1182/1, Loss: 784.7378540039062\n","Train -> sample/numSamples/epoch: 284/1182/1, Loss: 1058.3641357421875\n","Train -> sample/numSamples/epoch: 285/1182/1, Loss: 991.9798583984375\n","Train -> sample/numSamples/epoch: 286/1182/1, Loss: 720.2680053710938\n","Train -> sample/numSamples/epoch: 287/1182/1, Loss: 882.6227416992188\n","Train -> sample/numSamples/epoch: 288/1182/1, Loss: 893.880126953125\n","Train -> sample/numSamples/epoch: 289/1182/1, Loss: 1274.618896484375\n","Train -> sample/numSamples/epoch: 290/1182/1, Loss: 1070.423583984375\n","Train -> sample/numSamples/epoch: 291/1182/1, Loss: 828.794677734375\n","Train -> sample/numSamples/epoch: 292/1182/1, Loss: 1202.9627685546875\n","Train -> sample/numSamples/epoch: 293/1182/1, Loss: 705.0557861328125\n","Train -> sample/numSamples/epoch: 294/1182/1, Loss: 1211.1466064453125\n","Train -> sample/numSamples/epoch: 295/1182/1, Loss: 743.3272705078125\n","Train -> sample/numSamples/epoch: 296/1182/1, Loss: 827.739013671875\n","Train -> sample/numSamples/epoch: 297/1182/1, Loss: 1124.8797607421875\n","Train -> sample/numSamples/epoch: 298/1182/1, Loss: 1193.963623046875\n","Train -> sample/numSamples/epoch: 299/1182/1, Loss: 1174.589111328125\n","Train -> sample/numSamples/epoch: 300/1182/1, Loss: 648.9758911132812\n","Train -> sample/numSamples/epoch: 301/1182/1, Loss: 827.086181640625\n","Train -> sample/numSamples/epoch: 302/1182/1, Loss: 952.9113159179688\n","Train -> sample/numSamples/epoch: 303/1182/1, Loss: 1218.2958984375\n","Train -> sample/numSamples/epoch: 304/1182/1, Loss: 872.3368530273438\n","Train -> sample/numSamples/epoch: 305/1182/1, Loss: 835.2684936523438\n","Train -> sample/numSamples/epoch: 306/1182/1, Loss: 988.4788208007812\n","Train -> sample/numSamples/epoch: 307/1182/1, Loss: 1339.73779296875\n","Train -> sample/numSamples/epoch: 308/1182/1, Loss: 770.109619140625\n","Train -> sample/numSamples/epoch: 309/1182/1, Loss: 1561.1907958984375\n","Train -> sample/numSamples/epoch: 310/1182/1, Loss: 853.565185546875\n","Train -> sample/numSamples/epoch: 311/1182/1, Loss: 799.2352905273438\n","Train -> sample/numSamples/epoch: 312/1182/1, Loss: 1196.111572265625\n","Train -> sample/numSamples/epoch: 313/1182/1, Loss: 894.8043212890625\n","Train -> sample/numSamples/epoch: 314/1182/1, Loss: 819.1971435546875\n","Train -> sample/numSamples/epoch: 315/1182/1, Loss: 1006.3721923828125\n","Train -> sample/numSamples/epoch: 316/1182/1, Loss: 927.42431640625\n","Train -> sample/numSamples/epoch: 317/1182/1, Loss: 1132.718994140625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 318/1182/1, Loss: 862.4453125\n","Train -> sample/numSamples/epoch: 319/1182/1, Loss: 779.5911254882812\n","Train -> sample/numSamples/epoch: 320/1182/1, Loss: 596.2968139648438\n","Train -> sample/numSamples/epoch: 321/1182/1, Loss: 932.8541870117188\n","Train -> sample/numSamples/epoch: 322/1182/1, Loss: 1191.3297119140625\n","Train -> sample/numSamples/epoch: 323/1182/1, Loss: 1110.5850830078125\n","Train -> sample/numSamples/epoch: 324/1182/1, Loss: 627.7561645507812\n","Train -> sample/numSamples/epoch: 325/1182/1, Loss: 1055.5748291015625\n","Train -> sample/numSamples/epoch: 326/1182/1, Loss: 1079.061767578125\n","Train -> sample/numSamples/epoch: 327/1182/1, Loss: 1257.265869140625\n","Train -> sample/numSamples/epoch: 328/1182/1, Loss: 1494.082275390625\n","Train -> sample/numSamples/epoch: 329/1182/1, Loss: 1004.5599975585938\n","Train -> sample/numSamples/epoch: 330/1182/1, Loss: 1164.01025390625\n","Train -> sample/numSamples/epoch: 331/1182/1, Loss: 899.4628295898438\n","Train -> sample/numSamples/epoch: 332/1182/1, Loss: 1063.63330078125\n","Train -> sample/numSamples/epoch: 333/1182/1, Loss: 573.7438354492188\n","Train -> sample/numSamples/epoch: 334/1182/1, Loss: 793.1355590820312\n","Train -> sample/numSamples/epoch: 335/1182/1, Loss: 1159.7481689453125\n","Train -> sample/numSamples/epoch: 336/1182/1, Loss: 876.0870361328125\n","Train -> sample/numSamples/epoch: 337/1182/1, Loss: 828.4765625\n","Train -> sample/numSamples/epoch: 338/1182/1, Loss: 764.069580078125\n","Train -> sample/numSamples/epoch: 339/1182/1, Loss: 1036.3426513671875\n","Train -> sample/numSamples/epoch: 340/1182/1, Loss: 742.5743408203125\n","Train -> sample/numSamples/epoch: 341/1182/1, Loss: 993.0160522460938\n","Train -> sample/numSamples/epoch: 342/1182/1, Loss: 906.1473388671875\n","Train -> sample/numSamples/epoch: 343/1182/1, Loss: 982.0433959960938\n","Train -> sample/numSamples/epoch: 344/1182/1, Loss: 1135.1429443359375\n","Train -> sample/numSamples/epoch: 345/1182/1, Loss: 927.947021484375\n","Train -> sample/numSamples/epoch: 346/1182/1, Loss: 1090.6798095703125\n","Train -> sample/numSamples/epoch: 347/1182/1, Loss: 1322.2066650390625\n","Train -> sample/numSamples/epoch: 348/1182/1, Loss: 1140.011962890625\n","Train -> sample/numSamples/epoch: 349/1182/1, Loss: 878.2054443359375\n","Train -> sample/numSamples/epoch: 350/1182/1, Loss: 1251.6654052734375\n","Train -> sample/numSamples/epoch: 351/1182/1, Loss: 917.6036987304688\n","Train -> sample/numSamples/epoch: 352/1182/1, Loss: 589.8475341796875\n","Train -> sample/numSamples/epoch: 353/1182/1, Loss: 752.0407104492188\n","Train -> sample/numSamples/epoch: 354/1182/1, Loss: 1009.6226196289062\n","Train -> sample/numSamples/epoch: 355/1182/1, Loss: 1032.3905029296875\n","Train -> sample/numSamples/epoch: 356/1182/1, Loss: 753.980224609375\n","Train -> sample/numSamples/epoch: 357/1182/1, Loss: 1492.061767578125\n","Train -> sample/numSamples/epoch: 358/1182/1, Loss: 1135.4547119140625\n","Train -> sample/numSamples/epoch: 359/1182/1, Loss: 1145.760498046875\n","Train -> sample/numSamples/epoch: 360/1182/1, Loss: 888.1997680664062\n","Train -> sample/numSamples/epoch: 361/1182/1, Loss: 1189.4017333984375\n","Train -> sample/numSamples/epoch: 362/1182/1, Loss: 1038.808837890625\n","Train -> sample/numSamples/epoch: 363/1182/1, Loss: 1088.068603515625\n","Train -> sample/numSamples/epoch: 364/1182/1, Loss: 877.1600341796875\n","Train -> sample/numSamples/epoch: 365/1182/1, Loss: 968.0089721679688\n","Train -> sample/numSamples/epoch: 366/1182/1, Loss: 900.2821044921875\n","Train -> sample/numSamples/epoch: 367/1182/1, Loss: 876.1315307617188\n","Train -> sample/numSamples/epoch: 368/1182/1, Loss: 1063.103759765625\n","Train -> sample/numSamples/epoch: 369/1182/1, Loss: 779.8030395507812\n","Train -> sample/numSamples/epoch: 370/1182/1, Loss: 947.4442138671875\n","Train -> sample/numSamples/epoch: 371/1182/1, Loss: 997.4424438476562\n","Train -> sample/numSamples/epoch: 372/1182/1, Loss: 1517.4017333984375\n","Train -> sample/numSamples/epoch: 373/1182/1, Loss: 1169.0064697265625\n","Train -> sample/numSamples/epoch: 374/1182/1, Loss: 1005.3408813476562\n","Train -> sample/numSamples/epoch: 375/1182/1, Loss: 721.7149047851562\n","Train -> sample/numSamples/epoch: 376/1182/1, Loss: 982.3628540039062\n","Train -> sample/numSamples/epoch: 377/1182/1, Loss: 1021.272216796875\n","Train -> sample/numSamples/epoch: 378/1182/1, Loss: 1011.956298828125\n","Train -> sample/numSamples/epoch: 379/1182/1, Loss: 667.149658203125\n","Train -> sample/numSamples/epoch: 380/1182/1, Loss: 815.48388671875\n","Train -> sample/numSamples/epoch: 381/1182/1, Loss: 1155.157958984375\n","Train -> sample/numSamples/epoch: 382/1182/1, Loss: 1294.349609375\n","Train -> sample/numSamples/epoch: 383/1182/1, Loss: 832.1341552734375\n","Train -> sample/numSamples/epoch: 384/1182/1, Loss: 1032.0963134765625\n","Train -> sample/numSamples/epoch: 385/1182/1, Loss: 792.5283813476562\n","Train -> sample/numSamples/epoch: 386/1182/1, Loss: 1422.3995361328125\n","Train -> sample/numSamples/epoch: 387/1182/1, Loss: 823.718017578125\n","Train -> sample/numSamples/epoch: 388/1182/1, Loss: 809.8637084960938\n","Train -> sample/numSamples/epoch: 389/1182/1, Loss: 1155.38232421875\n","Train -> sample/numSamples/epoch: 390/1182/1, Loss: 1003.6463623046875\n","Train -> sample/numSamples/epoch: 391/1182/1, Loss: 1101.130126953125\n","Train -> sample/numSamples/epoch: 392/1182/1, Loss: 1193.0548095703125\n","Train -> sample/numSamples/epoch: 393/1182/1, Loss: 1056.7772216796875\n","Train -> sample/numSamples/epoch: 394/1182/1, Loss: 1193.7235107421875\n","Train -> sample/numSamples/epoch: 395/1182/1, Loss: 1268.8106689453125\n","Train -> sample/numSamples/epoch: 396/1182/1, Loss: 873.108154296875\n","Train -> sample/numSamples/epoch: 397/1182/1, Loss: 977.5358276367188\n","Train -> sample/numSamples/epoch: 398/1182/1, Loss: 929.073974609375\n","Train -> sample/numSamples/epoch: 399/1182/1, Loss: 1339.4053955078125\n","Train -> sample/numSamples/epoch: 400/1182/1, Loss: 1373.5950927734375\n","Train -> sample/numSamples/epoch: 401/1182/1, Loss: 1069.2020263671875\n","Train -> sample/numSamples/epoch: 402/1182/1, Loss: 991.790771484375\n","Train -> sample/numSamples/epoch: 403/1182/1, Loss: 913.8638305664062\n","Train -> sample/numSamples/epoch: 404/1182/1, Loss: 890.587890625\n","Train -> sample/numSamples/epoch: 405/1182/1, Loss: 1266.17626953125\n","Train -> sample/numSamples/epoch: 406/1182/1, Loss: 764.1489868164062\n","Train -> sample/numSamples/epoch: 407/1182/1, Loss: 898.9027099609375\n","Train -> sample/numSamples/epoch: 408/1182/1, Loss: 673.1146850585938\n","Train -> sample/numSamples/epoch: 409/1182/1, Loss: 974.0617065429688\n","Train -> sample/numSamples/epoch: 410/1182/1, Loss: 1205.7855224609375\n","Train -> sample/numSamples/epoch: 411/1182/1, Loss: 1161.8397216796875\n","Train -> sample/numSamples/epoch: 412/1182/1, Loss: 956.8427124023438\n","Train -> sample/numSamples/epoch: 413/1182/1, Loss: 1256.566650390625\n","Train -> sample/numSamples/epoch: 414/1182/1, Loss: 788.96728515625\n","Train -> sample/numSamples/epoch: 415/1182/1, Loss: 1264.5999755859375\n","Train -> sample/numSamples/epoch: 416/1182/1, Loss: 858.4608154296875\n","Train -> sample/numSamples/epoch: 417/1182/1, Loss: 1395.9288330078125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 418/1182/1, Loss: 1160.79443359375\n","Train -> sample/numSamples/epoch: 419/1182/1, Loss: 591.8177490234375\n","Train -> sample/numSamples/epoch: 420/1182/1, Loss: 864.12841796875\n","Train -> sample/numSamples/epoch: 421/1182/1, Loss: 1300.3116455078125\n","Train -> sample/numSamples/epoch: 422/1182/1, Loss: 1519.1207275390625\n","Train -> sample/numSamples/epoch: 423/1182/1, Loss: 796.47607421875\n","Train -> sample/numSamples/epoch: 424/1182/1, Loss: 1136.1630859375\n","Train -> sample/numSamples/epoch: 425/1182/1, Loss: 1443.62646484375\n","Train -> sample/numSamples/epoch: 426/1182/1, Loss: 761.0938110351562\n","Train -> sample/numSamples/epoch: 427/1182/1, Loss: 1015.8719482421875\n","Train -> sample/numSamples/epoch: 428/1182/1, Loss: 1306.8092041015625\n","Train -> sample/numSamples/epoch: 429/1182/1, Loss: 831.5570678710938\n","Train -> sample/numSamples/epoch: 430/1182/1, Loss: 1118.266845703125\n","Train -> sample/numSamples/epoch: 431/1182/1, Loss: 725.84326171875\n","Train -> sample/numSamples/epoch: 432/1182/1, Loss: 656.0505981445312\n","Train -> sample/numSamples/epoch: 433/1182/1, Loss: 1446.7149658203125\n","Train -> sample/numSamples/epoch: 434/1182/1, Loss: 1162.21484375\n","Train -> sample/numSamples/epoch: 435/1182/1, Loss: 1804.2294921875\n","Train -> sample/numSamples/epoch: 436/1182/1, Loss: 995.1417236328125\n","Train -> sample/numSamples/epoch: 437/1182/1, Loss: 1487.589111328125\n","Train -> sample/numSamples/epoch: 438/1182/1, Loss: 868.6187744140625\n","Train -> sample/numSamples/epoch: 439/1182/1, Loss: 1033.11328125\n","Train -> sample/numSamples/epoch: 440/1182/1, Loss: 1443.51806640625\n","Train -> sample/numSamples/epoch: 441/1182/1, Loss: 906.2992553710938\n","Train -> sample/numSamples/epoch: 442/1182/1, Loss: 1565.1334228515625\n","Train -> sample/numSamples/epoch: 443/1182/1, Loss: 833.329345703125\n","Train -> sample/numSamples/epoch: 444/1182/1, Loss: 1114.365966796875\n","Train -> sample/numSamples/epoch: 445/1182/1, Loss: 1655.71484375\n","Train -> sample/numSamples/epoch: 446/1182/1, Loss: 1286.4285888671875\n","Train -> sample/numSamples/epoch: 447/1182/1, Loss: 1179.291748046875\n","Train -> sample/numSamples/epoch: 448/1182/1, Loss: 542.9624633789062\n","Train -> sample/numSamples/epoch: 449/1182/1, Loss: 1012.6412353515625\n","Train -> sample/numSamples/epoch: 450/1182/1, Loss: 1246.4656982421875\n","Train -> sample/numSamples/epoch: 451/1182/1, Loss: 883.8948974609375\n","Train -> sample/numSamples/epoch: 452/1182/1, Loss: 1003.96240234375\n","Train -> sample/numSamples/epoch: 453/1182/1, Loss: 1302.688720703125\n","Train -> sample/numSamples/epoch: 454/1182/1, Loss: 718.0547485351562\n","Train -> sample/numSamples/epoch: 455/1182/1, Loss: 1121.9881591796875\n","Train -> sample/numSamples/epoch: 456/1182/1, Loss: 1246.7542724609375\n","Train -> sample/numSamples/epoch: 457/1182/1, Loss: 959.0894775390625\n","Train -> sample/numSamples/epoch: 458/1182/1, Loss: 1065.2835693359375\n","Train -> sample/numSamples/epoch: 459/1182/1, Loss: 915.124267578125\n","Train -> sample/numSamples/epoch: 460/1182/1, Loss: 1186.9053955078125\n","Train -> sample/numSamples/epoch: 461/1182/1, Loss: 1069.900146484375\n","Train -> sample/numSamples/epoch: 462/1182/1, Loss: 1172.8328857421875\n","Train -> sample/numSamples/epoch: 463/1182/1, Loss: 1283.4866943359375\n","Train -> sample/numSamples/epoch: 464/1182/1, Loss: 1115.9951171875\n","Train -> sample/numSamples/epoch: 465/1182/1, Loss: 1297.671630859375\n","Train -> sample/numSamples/epoch: 466/1182/1, Loss: 1308.1092529296875\n","Train -> sample/numSamples/epoch: 467/1182/1, Loss: 800.34814453125\n","Train -> sample/numSamples/epoch: 468/1182/1, Loss: 1176.4354248046875\n","Train -> sample/numSamples/epoch: 469/1182/1, Loss: 809.1910400390625\n","Train -> sample/numSamples/epoch: 470/1182/1, Loss: 692.495849609375\n","Train -> sample/numSamples/epoch: 471/1182/1, Loss: 1423.17236328125\n","Train -> sample/numSamples/epoch: 472/1182/1, Loss: 1286.443115234375\n","Train -> sample/numSamples/epoch: 473/1182/1, Loss: 1073.14892578125\n","Train -> sample/numSamples/epoch: 474/1182/1, Loss: 1175.327880859375\n","Train -> sample/numSamples/epoch: 475/1182/1, Loss: 1026.236328125\n","Train -> sample/numSamples/epoch: 476/1182/1, Loss: 1113.8177490234375\n","Train -> sample/numSamples/epoch: 477/1182/1, Loss: 852.82421875\n","Train -> sample/numSamples/epoch: 478/1182/1, Loss: 881.2130737304688\n","Train -> sample/numSamples/epoch: 479/1182/1, Loss: 1148.702392578125\n","Train -> sample/numSamples/epoch: 480/1182/1, Loss: 987.2677612304688\n","Train -> sample/numSamples/epoch: 481/1182/1, Loss: 1069.5360107421875\n","Train -> sample/numSamples/epoch: 482/1182/1, Loss: 732.330322265625\n","Train -> sample/numSamples/epoch: 483/1182/1, Loss: 694.3085327148438\n","Train -> sample/numSamples/epoch: 484/1182/1, Loss: 1300.8875732421875\n","Train -> sample/numSamples/epoch: 485/1182/1, Loss: 1325.01611328125\n","Train -> sample/numSamples/epoch: 486/1182/1, Loss: 891.2858276367188\n","Train -> sample/numSamples/epoch: 487/1182/1, Loss: 1006.6849365234375\n","Train -> sample/numSamples/epoch: 488/1182/1, Loss: 1588.1689453125\n","Train -> sample/numSamples/epoch: 489/1182/1, Loss: 677.229736328125\n","Train -> sample/numSamples/epoch: 490/1182/1, Loss: 1345.826904296875\n","Train -> sample/numSamples/epoch: 491/1182/1, Loss: 866.8118286132812\n","Train -> sample/numSamples/epoch: 492/1182/1, Loss: 1566.003662109375\n","Train -> sample/numSamples/epoch: 493/1182/1, Loss: 881.6577758789062\n","Train -> sample/numSamples/epoch: 494/1182/1, Loss: 1387.810546875\n","Train -> sample/numSamples/epoch: 495/1182/1, Loss: 1152.41015625\n","Train -> sample/numSamples/epoch: 496/1182/1, Loss: 1195.6517333984375\n","Train -> sample/numSamples/epoch: 497/1182/1, Loss: 954.7060546875\n","Train -> sample/numSamples/epoch: 498/1182/1, Loss: 1430.6142578125\n","Train -> sample/numSamples/epoch: 499/1182/1, Loss: 1020.3720703125\n","Train -> sample/numSamples/epoch: 500/1182/1, Loss: 989.4695434570312\n","Train -> sample/numSamples/epoch: 501/1182/1, Loss: 1108.8863525390625\n","Train -> sample/numSamples/epoch: 502/1182/1, Loss: 752.6445922851562\n","Train -> sample/numSamples/epoch: 503/1182/1, Loss: 775.25146484375\n","Train -> sample/numSamples/epoch: 504/1182/1, Loss: 1186.46044921875\n","Train -> sample/numSamples/epoch: 505/1182/1, Loss: 1364.1385498046875\n","Train -> sample/numSamples/epoch: 506/1182/1, Loss: 986.7378540039062\n","Train -> sample/numSamples/epoch: 507/1182/1, Loss: 974.5670776367188\n","Train -> sample/numSamples/epoch: 508/1182/1, Loss: 1137.4420166015625\n","Train -> sample/numSamples/epoch: 509/1182/1, Loss: 821.294921875\n","Train -> sample/numSamples/epoch: 510/1182/1, Loss: 706.3790283203125\n","Train -> sample/numSamples/epoch: 511/1182/1, Loss: 1009.6612548828125\n","Train -> sample/numSamples/epoch: 512/1182/1, Loss: 1130.828125\n","Train -> sample/numSamples/epoch: 513/1182/1, Loss: 2008.0511474609375\n","Train -> sample/numSamples/epoch: 514/1182/1, Loss: 1826.0999755859375\n","Train -> sample/numSamples/epoch: 515/1182/1, Loss: 1397.7100830078125\n","Train -> sample/numSamples/epoch: 516/1182/1, Loss: 927.8790893554688\n","Train -> sample/numSamples/epoch: 517/1182/1, Loss: 1097.77392578125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 518/1182/1, Loss: 1088.3253173828125\n","Train -> sample/numSamples/epoch: 519/1182/1, Loss: 833.7384643554688\n","Train -> sample/numSamples/epoch: 520/1182/1, Loss: 1333.9749755859375\n","Train -> sample/numSamples/epoch: 521/1182/1, Loss: 1128.3172607421875\n","Train -> sample/numSamples/epoch: 522/1182/1, Loss: 1478.276611328125\n","Train -> sample/numSamples/epoch: 523/1182/1, Loss: 1051.800537109375\n","Train -> sample/numSamples/epoch: 524/1182/1, Loss: 971.7314453125\n","Train -> sample/numSamples/epoch: 525/1182/1, Loss: 1188.695068359375\n","Train -> sample/numSamples/epoch: 526/1182/1, Loss: 1026.43359375\n","Train -> sample/numSamples/epoch: 527/1182/1, Loss: 1108.980712890625\n","Train -> sample/numSamples/epoch: 528/1182/1, Loss: 1176.34326171875\n","Train -> sample/numSamples/epoch: 529/1182/1, Loss: 1062.482421875\n","Train -> sample/numSamples/epoch: 530/1182/1, Loss: 1036.427490234375\n","Train -> sample/numSamples/epoch: 531/1182/1, Loss: 1259.4891357421875\n","Train -> sample/numSamples/epoch: 532/1182/1, Loss: 935.459716796875\n","Train -> sample/numSamples/epoch: 533/1182/1, Loss: 997.7330322265625\n","Train -> sample/numSamples/epoch: 534/1182/1, Loss: 1059.7913818359375\n","Train -> sample/numSamples/epoch: 535/1182/1, Loss: 1679.904052734375\n","Train -> sample/numSamples/epoch: 536/1182/1, Loss: 1160.7816162109375\n","Train -> sample/numSamples/epoch: 537/1182/1, Loss: 1383.823974609375\n","Train -> sample/numSamples/epoch: 538/1182/1, Loss: 961.7866821289062\n","Train -> sample/numSamples/epoch: 539/1182/1, Loss: 1150.418212890625\n","Train -> sample/numSamples/epoch: 540/1182/1, Loss: 1328.0616455078125\n","Train -> sample/numSamples/epoch: 541/1182/1, Loss: 1295.4290771484375\n","Train -> sample/numSamples/epoch: 542/1182/1, Loss: 731.6985473632812\n","Train -> sample/numSamples/epoch: 543/1182/1, Loss: 1022.041015625\n","Train -> sample/numSamples/epoch: 544/1182/1, Loss: 970.1431884765625\n","Train -> sample/numSamples/epoch: 545/1182/1, Loss: 833.2990112304688\n","Train -> sample/numSamples/epoch: 546/1182/1, Loss: 1049.6060791015625\n","Train -> sample/numSamples/epoch: 547/1182/1, Loss: 723.0516967773438\n","Train -> sample/numSamples/epoch: 548/1182/1, Loss: 863.9127197265625\n","Train -> sample/numSamples/epoch: 549/1182/1, Loss: 1207.1666259765625\n","Train -> sample/numSamples/epoch: 550/1182/1, Loss: 1592.4898681640625\n","Train -> sample/numSamples/epoch: 551/1182/1, Loss: 1091.3216552734375\n","Train -> sample/numSamples/epoch: 552/1182/1, Loss: 904.23681640625\n","Train -> sample/numSamples/epoch: 553/1182/1, Loss: 1167.7073974609375\n","Train -> sample/numSamples/epoch: 554/1182/1, Loss: 963.3838500976562\n","Train -> sample/numSamples/epoch: 555/1182/1, Loss: 978.0234985351562\n","Train -> sample/numSamples/epoch: 556/1182/1, Loss: 1219.569580078125\n","Train -> sample/numSamples/epoch: 557/1182/1, Loss: 1263.260498046875\n","Train -> sample/numSamples/epoch: 558/1182/1, Loss: 1176.705810546875\n","Train -> sample/numSamples/epoch: 559/1182/1, Loss: 1213.798095703125\n","Train -> sample/numSamples/epoch: 560/1182/1, Loss: 1183.551513671875\n","Train -> sample/numSamples/epoch: 561/1182/1, Loss: 1294.65283203125\n","Train -> sample/numSamples/epoch: 562/1182/1, Loss: 1409.9210205078125\n","Train -> sample/numSamples/epoch: 563/1182/1, Loss: 1058.980712890625\n","Train -> sample/numSamples/epoch: 564/1182/1, Loss: 1494.6500244140625\n","Train -> sample/numSamples/epoch: 565/1182/1, Loss: 1013.772216796875\n","Train -> sample/numSamples/epoch: 566/1182/1, Loss: 1021.2936401367188\n","Train -> sample/numSamples/epoch: 567/1182/1, Loss: 821.2880249023438\n","Train -> sample/numSamples/epoch: 568/1182/1, Loss: 986.6604614257812\n","Train -> sample/numSamples/epoch: 569/1182/1, Loss: 1278.64453125\n","Train -> sample/numSamples/epoch: 570/1182/1, Loss: 1699.681884765625\n","Train -> sample/numSamples/epoch: 571/1182/1, Loss: 1326.714111328125\n","Train -> sample/numSamples/epoch: 572/1182/1, Loss: 1112.7025146484375\n","Train -> sample/numSamples/epoch: 573/1182/1, Loss: 788.5360107421875\n","Train -> sample/numSamples/epoch: 574/1182/1, Loss: 1086.6373291015625\n","Train -> sample/numSamples/epoch: 575/1182/1, Loss: 1403.890380859375\n","Train -> sample/numSamples/epoch: 576/1182/1, Loss: 1152.7950439453125\n","Train -> sample/numSamples/epoch: 577/1182/1, Loss: 961.2163696289062\n","Train -> sample/numSamples/epoch: 578/1182/1, Loss: 867.7538452148438\n","Train -> sample/numSamples/epoch: 579/1182/1, Loss: 793.9408569335938\n","Train -> sample/numSamples/epoch: 580/1182/1, Loss: 769.1373901367188\n","Train -> sample/numSamples/epoch: 581/1182/1, Loss: 1320.3433837890625\n","Train -> sample/numSamples/epoch: 582/1182/1, Loss: 980.3765258789062\n","Train -> sample/numSamples/epoch: 583/1182/1, Loss: 1377.97998046875\n","Train -> sample/numSamples/epoch: 584/1182/1, Loss: 693.6443481445312\n","Train -> sample/numSamples/epoch: 585/1182/1, Loss: 1090.1583251953125\n","Train -> sample/numSamples/epoch: 586/1182/1, Loss: 964.0814819335938\n","Train -> sample/numSamples/epoch: 587/1182/1, Loss: 911.1382446289062\n","Train -> sample/numSamples/epoch: 588/1182/1, Loss: 878.630615234375\n","Train -> sample/numSamples/epoch: 589/1182/1, Loss: 1220.4661865234375\n","Train -> sample/numSamples/epoch: 590/1182/1, Loss: 966.187744140625\n","Train -> sample/numSamples/epoch: 591/1182/1, Loss: 1222.3651123046875\n","Train -> sample/numSamples/epoch: 592/1182/1, Loss: 1064.6689453125\n","Train -> sample/numSamples/epoch: 593/1182/1, Loss: 1522.84423828125\n","Train -> sample/numSamples/epoch: 594/1182/1, Loss: 1033.945556640625\n","Train -> sample/numSamples/epoch: 595/1182/1, Loss: 1414.67578125\n","Train -> sample/numSamples/epoch: 596/1182/1, Loss: 945.357666015625\n","Train -> sample/numSamples/epoch: 597/1182/1, Loss: 1135.885498046875\n","Train -> sample/numSamples/epoch: 598/1182/1, Loss: 1219.4752197265625\n","Train -> sample/numSamples/epoch: 599/1182/1, Loss: 895.2193603515625\n","Train -> sample/numSamples/epoch: 600/1182/1, Loss: 926.770751953125\n","Train -> sample/numSamples/epoch: 601/1182/1, Loss: 841.5941162109375\n","Train -> sample/numSamples/epoch: 602/1182/1, Loss: 950.85888671875\n","Train -> sample/numSamples/epoch: 603/1182/1, Loss: 1138.287353515625\n","Train -> sample/numSamples/epoch: 604/1182/1, Loss: 1156.4227294921875\n","Train -> sample/numSamples/epoch: 605/1182/1, Loss: 906.1345825195312\n","Train -> sample/numSamples/epoch: 606/1182/1, Loss: 960.7095336914062\n","Train -> sample/numSamples/epoch: 607/1182/1, Loss: 1151.78759765625\n","Train -> sample/numSamples/epoch: 608/1182/1, Loss: 1116.7264404296875\n","Train -> sample/numSamples/epoch: 609/1182/1, Loss: 1069.6759033203125\n","Train -> sample/numSamples/epoch: 610/1182/1, Loss: 1238.36962890625\n","Train -> sample/numSamples/epoch: 611/1182/1, Loss: 893.0797729492188\n","Train -> sample/numSamples/epoch: 612/1182/1, Loss: 738.9015502929688\n","Train -> sample/numSamples/epoch: 613/1182/1, Loss: 846.6611328125\n","Train -> sample/numSamples/epoch: 614/1182/1, Loss: 873.311279296875\n","Train -> sample/numSamples/epoch: 615/1182/1, Loss: 1176.6700439453125\n","Train -> sample/numSamples/epoch: 616/1182/1, Loss: 1229.2030029296875\n","Train -> sample/numSamples/epoch: 617/1182/1, Loss: 1035.3082275390625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 618/1182/1, Loss: 1026.294189453125\n","Train -> sample/numSamples/epoch: 619/1182/1, Loss: 706.9053344726562\n","Train -> sample/numSamples/epoch: 620/1182/1, Loss: 1114.626953125\n","Train -> sample/numSamples/epoch: 621/1182/1, Loss: 1241.654296875\n","Train -> sample/numSamples/epoch: 622/1182/1, Loss: 1234.7843017578125\n","Train -> sample/numSamples/epoch: 623/1182/1, Loss: 817.1203002929688\n","Train -> sample/numSamples/epoch: 624/1182/1, Loss: 973.1859130859375\n","Train -> sample/numSamples/epoch: 625/1182/1, Loss: 463.757568359375\n","Train -> sample/numSamples/epoch: 626/1182/1, Loss: 1702.24365234375\n","Train -> sample/numSamples/epoch: 627/1182/1, Loss: 780.4935302734375\n","Train -> sample/numSamples/epoch: 628/1182/1, Loss: 808.8692626953125\n","Train -> sample/numSamples/epoch: 629/1182/1, Loss: 553.3155517578125\n","Train -> sample/numSamples/epoch: 630/1182/1, Loss: 1458.8431396484375\n","Train -> sample/numSamples/epoch: 631/1182/1, Loss: 903.0120239257812\n","Train -> sample/numSamples/epoch: 632/1182/1, Loss: 1188.01171875\n","Train -> sample/numSamples/epoch: 633/1182/1, Loss: 1570.0787353515625\n","Train -> sample/numSamples/epoch: 634/1182/1, Loss: 1011.6351928710938\n","Train -> sample/numSamples/epoch: 635/1182/1, Loss: 811.96533203125\n","Train -> sample/numSamples/epoch: 636/1182/1, Loss: 1267.8057861328125\n","Train -> sample/numSamples/epoch: 637/1182/1, Loss: 601.7241821289062\n","Train -> sample/numSamples/epoch: 638/1182/1, Loss: 1216.7769775390625\n","Train -> sample/numSamples/epoch: 639/1182/1, Loss: 1166.7264404296875\n","Train -> sample/numSamples/epoch: 640/1182/1, Loss: 1156.46240234375\n","Train -> sample/numSamples/epoch: 641/1182/1, Loss: 1576.2410888671875\n","Train -> sample/numSamples/epoch: 642/1182/1, Loss: 1023.381591796875\n","Train -> sample/numSamples/epoch: 643/1182/1, Loss: 990.8967895507812\n","Train -> sample/numSamples/epoch: 644/1182/1, Loss: 1335.2073974609375\n","Train -> sample/numSamples/epoch: 645/1182/1, Loss: 1399.5599365234375\n","Train -> sample/numSamples/epoch: 646/1182/1, Loss: 906.4385986328125\n","Train -> sample/numSamples/epoch: 647/1182/1, Loss: 1320.2672119140625\n","Train -> sample/numSamples/epoch: 648/1182/1, Loss: 1003.55712890625\n","Train -> sample/numSamples/epoch: 649/1182/1, Loss: 982.730224609375\n","Train -> sample/numSamples/epoch: 650/1182/1, Loss: 832.7374267578125\n","Train -> sample/numSamples/epoch: 651/1182/1, Loss: 916.4733276367188\n","Train -> sample/numSamples/epoch: 652/1182/1, Loss: 735.085693359375\n","Train -> sample/numSamples/epoch: 653/1182/1, Loss: 1161.502685546875\n","Train -> sample/numSamples/epoch: 654/1182/1, Loss: 792.9551391601562\n","Train -> sample/numSamples/epoch: 655/1182/1, Loss: 885.1378784179688\n","Train -> sample/numSamples/epoch: 656/1182/1, Loss: 1709.6910400390625\n","Train -> sample/numSamples/epoch: 657/1182/1, Loss: 933.5662231445312\n","Train -> sample/numSamples/epoch: 658/1182/1, Loss: 879.0913696289062\n","Train -> sample/numSamples/epoch: 659/1182/1, Loss: 687.3450317382812\n","Train -> sample/numSamples/epoch: 660/1182/1, Loss: 1126.321044921875\n","Train -> sample/numSamples/epoch: 661/1182/1, Loss: 949.084716796875\n","Train -> sample/numSamples/epoch: 662/1182/1, Loss: 1093.4207763671875\n","Train -> sample/numSamples/epoch: 663/1182/1, Loss: 995.613037109375\n","Train -> sample/numSamples/epoch: 664/1182/1, Loss: 596.260498046875\n","Train -> sample/numSamples/epoch: 665/1182/1, Loss: 1192.8631591796875\n","Train -> sample/numSamples/epoch: 666/1182/1, Loss: 1088.323486328125\n","Train -> sample/numSamples/epoch: 667/1182/1, Loss: 1010.3156127929688\n","Train -> sample/numSamples/epoch: 668/1182/1, Loss: 1156.5235595703125\n","Train -> sample/numSamples/epoch: 669/1182/1, Loss: 930.3165893554688\n","Train -> sample/numSamples/epoch: 670/1182/1, Loss: 929.7546997070312\n","Train -> sample/numSamples/epoch: 671/1182/1, Loss: 1350.345458984375\n","Train -> sample/numSamples/epoch: 672/1182/1, Loss: 1360.267822265625\n","Train -> sample/numSamples/epoch: 673/1182/1, Loss: 867.8165893554688\n","Train -> sample/numSamples/epoch: 674/1182/1, Loss: 1344.843017578125\n","Train -> sample/numSamples/epoch: 675/1182/1, Loss: 1180.7049560546875\n","Train -> sample/numSamples/epoch: 676/1182/1, Loss: 868.9295654296875\n","Train -> sample/numSamples/epoch: 677/1182/1, Loss: 1015.8162231445312\n","Train -> sample/numSamples/epoch: 678/1182/1, Loss: 1204.041015625\n","Train -> sample/numSamples/epoch: 679/1182/1, Loss: 1054.991455078125\n","Train -> sample/numSamples/epoch: 680/1182/1, Loss: 1072.30615234375\n","Train -> sample/numSamples/epoch: 681/1182/1, Loss: 996.295654296875\n","Train -> sample/numSamples/epoch: 682/1182/1, Loss: 887.5023193359375\n","Train -> sample/numSamples/epoch: 683/1182/1, Loss: 904.6514892578125\n","Train -> sample/numSamples/epoch: 684/1182/1, Loss: 1434.7552490234375\n","Train -> sample/numSamples/epoch: 685/1182/1, Loss: 1090.647216796875\n","Train -> sample/numSamples/epoch: 686/1182/1, Loss: 1218.2730712890625\n","Train -> sample/numSamples/epoch: 687/1182/1, Loss: 1384.4830322265625\n","Train -> sample/numSamples/epoch: 688/1182/1, Loss: 1187.0635986328125\n","Train -> sample/numSamples/epoch: 689/1182/1, Loss: 1224.31640625\n","Train -> sample/numSamples/epoch: 690/1182/1, Loss: 668.8927001953125\n","Train -> sample/numSamples/epoch: 691/1182/1, Loss: 1096.572509765625\n","Train -> sample/numSamples/epoch: 692/1182/1, Loss: 845.5408325195312\n","Train -> sample/numSamples/epoch: 693/1182/1, Loss: 1285.829833984375\n","Train -> sample/numSamples/epoch: 694/1182/1, Loss: 797.147705078125\n","Train -> sample/numSamples/epoch: 695/1182/1, Loss: 1217.9539794921875\n","Train -> sample/numSamples/epoch: 696/1182/1, Loss: 1486.3197021484375\n","Train -> sample/numSamples/epoch: 697/1182/1, Loss: 1455.5723876953125\n","Train -> sample/numSamples/epoch: 698/1182/1, Loss: 663.3604125976562\n","Train -> sample/numSamples/epoch: 699/1182/1, Loss: 1519.359619140625\n","Train -> sample/numSamples/epoch: 700/1182/1, Loss: 1321.370849609375\n","Train -> sample/numSamples/epoch: 701/1182/1, Loss: 766.303466796875\n","Train -> sample/numSamples/epoch: 702/1182/1, Loss: 810.9021606445312\n","Train -> sample/numSamples/epoch: 703/1182/1, Loss: 1375.737060546875\n","Train -> sample/numSamples/epoch: 704/1182/1, Loss: 969.2156982421875\n","Train -> sample/numSamples/epoch: 705/1182/1, Loss: 1554.1514892578125\n","Train -> sample/numSamples/epoch: 706/1182/1, Loss: 1098.180419921875\n","Train -> sample/numSamples/epoch: 707/1182/1, Loss: 1144.641357421875\n","Train -> sample/numSamples/epoch: 708/1182/1, Loss: 969.4603881835938\n","Train -> sample/numSamples/epoch: 709/1182/1, Loss: 1349.0615234375\n","Train -> sample/numSamples/epoch: 710/1182/1, Loss: 1424.0103759765625\n","Train -> sample/numSamples/epoch: 711/1182/1, Loss: 990.7615356445312\n","Train -> sample/numSamples/epoch: 712/1182/1, Loss: 1000.8281860351562\n","Train -> sample/numSamples/epoch: 713/1182/1, Loss: 1193.438232421875\n","Train -> sample/numSamples/epoch: 714/1182/1, Loss: 865.1674194335938\n","Train -> sample/numSamples/epoch: 715/1182/1, Loss: 1116.1820068359375\n","Train -> sample/numSamples/epoch: 716/1182/1, Loss: 1832.015869140625\n","Train -> sample/numSamples/epoch: 717/1182/1, Loss: 1144.03076171875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 718/1182/1, Loss: 856.1005859375\n","Train -> sample/numSamples/epoch: 719/1182/1, Loss: 961.0178833007812\n","Train -> sample/numSamples/epoch: 720/1182/1, Loss: 958.9866333007812\n","Train -> sample/numSamples/epoch: 721/1182/1, Loss: 1189.798583984375\n","Train -> sample/numSamples/epoch: 722/1182/1, Loss: 1090.68115234375\n","Train -> sample/numSamples/epoch: 723/1182/1, Loss: 705.9878540039062\n","Train -> sample/numSamples/epoch: 724/1182/1, Loss: 1103.3309326171875\n","Train -> sample/numSamples/epoch: 725/1182/1, Loss: 1369.7969970703125\n","Train -> sample/numSamples/epoch: 726/1182/1, Loss: 1117.39111328125\n","Train -> sample/numSamples/epoch: 727/1182/1, Loss: 1221.32275390625\n","Train -> sample/numSamples/epoch: 728/1182/1, Loss: 1297.4239501953125\n","Train -> sample/numSamples/epoch: 729/1182/1, Loss: 1002.6864013671875\n","Train -> sample/numSamples/epoch: 730/1182/1, Loss: 960.6581420898438\n","Train -> sample/numSamples/epoch: 731/1182/1, Loss: 1313.322509765625\n","Train -> sample/numSamples/epoch: 732/1182/1, Loss: 1199.302001953125\n","Train -> sample/numSamples/epoch: 733/1182/1, Loss: 775.9691162109375\n","Train -> sample/numSamples/epoch: 734/1182/1, Loss: 1094.0634765625\n","Train -> sample/numSamples/epoch: 735/1182/1, Loss: 1248.955810546875\n","Train -> sample/numSamples/epoch: 736/1182/1, Loss: 1277.1204833984375\n","Train -> sample/numSamples/epoch: 737/1182/1, Loss: 1299.56201171875\n","Train -> sample/numSamples/epoch: 738/1182/1, Loss: 744.7559814453125\n","Train -> sample/numSamples/epoch: 739/1182/1, Loss: 440.7496032714844\n","Train -> sample/numSamples/epoch: 740/1182/1, Loss: 816.7122192382812\n","Train -> sample/numSamples/epoch: 741/1182/1, Loss: 1123.0592041015625\n","Train -> sample/numSamples/epoch: 742/1182/1, Loss: 913.1653442382812\n","Train -> sample/numSamples/epoch: 743/1182/1, Loss: 1346.412353515625\n","Train -> sample/numSamples/epoch: 744/1182/1, Loss: 783.4452514648438\n","Train -> sample/numSamples/epoch: 745/1182/1, Loss: 1612.1158447265625\n","Train -> sample/numSamples/epoch: 746/1182/1, Loss: 762.703369140625\n","Train -> sample/numSamples/epoch: 747/1182/1, Loss: 769.0943603515625\n","Train -> sample/numSamples/epoch: 748/1182/1, Loss: 1265.7010498046875\n","Train -> sample/numSamples/epoch: 749/1182/1, Loss: 776.1437377929688\n","Train -> sample/numSamples/epoch: 750/1182/1, Loss: 810.01611328125\n","Train -> sample/numSamples/epoch: 751/1182/1, Loss: 1271.9227294921875\n","Train -> sample/numSamples/epoch: 752/1182/1, Loss: 936.4967041015625\n","Train -> sample/numSamples/epoch: 753/1182/1, Loss: 651.7271728515625\n","Train -> sample/numSamples/epoch: 754/1182/1, Loss: 1657.3170166015625\n","Train -> sample/numSamples/epoch: 755/1182/1, Loss: 1149.8082275390625\n","Train -> sample/numSamples/epoch: 756/1182/1, Loss: 1480.8846435546875\n","Train -> sample/numSamples/epoch: 757/1182/1, Loss: 1012.8092041015625\n","Train -> sample/numSamples/epoch: 758/1182/1, Loss: 1016.796142578125\n","Train -> sample/numSamples/epoch: 759/1182/1, Loss: 909.8749389648438\n","Train -> sample/numSamples/epoch: 760/1182/1, Loss: 1123.0635986328125\n","Train -> sample/numSamples/epoch: 761/1182/1, Loss: 1123.028076171875\n","Train -> sample/numSamples/epoch: 762/1182/1, Loss: 1382.02197265625\n","Train -> sample/numSamples/epoch: 763/1182/1, Loss: 502.33734130859375\n","Train -> sample/numSamples/epoch: 764/1182/1, Loss: 1497.7626953125\n","Train -> sample/numSamples/epoch: 765/1182/1, Loss: 1208.7371826171875\n","Train -> sample/numSamples/epoch: 766/1182/1, Loss: 727.21875\n","Train -> sample/numSamples/epoch: 767/1182/1, Loss: 955.9370727539062\n","Train -> sample/numSamples/epoch: 768/1182/1, Loss: 609.0631713867188\n","Train -> sample/numSamples/epoch: 769/1182/1, Loss: 626.2775268554688\n","Train -> sample/numSamples/epoch: 770/1182/1, Loss: 636.96630859375\n","Train -> sample/numSamples/epoch: 771/1182/1, Loss: 721.5236206054688\n","Train -> sample/numSamples/epoch: 772/1182/1, Loss: 943.0491333007812\n","Train -> sample/numSamples/epoch: 773/1182/1, Loss: 1320.9696044921875\n","Train -> sample/numSamples/epoch: 774/1182/1, Loss: 1665.2398681640625\n","Train -> sample/numSamples/epoch: 775/1182/1, Loss: 1016.969970703125\n","Train -> sample/numSamples/epoch: 776/1182/1, Loss: 1032.5147705078125\n","Train -> sample/numSamples/epoch: 777/1182/1, Loss: 893.0245361328125\n","Train -> sample/numSamples/epoch: 778/1182/1, Loss: 1150.306396484375\n","Train -> sample/numSamples/epoch: 779/1182/1, Loss: 1233.781005859375\n","Train -> sample/numSamples/epoch: 780/1182/1, Loss: 844.785888671875\n","Train -> sample/numSamples/epoch: 781/1182/1, Loss: 673.5604248046875\n","Train -> sample/numSamples/epoch: 782/1182/1, Loss: 1044.89111328125\n","Train -> sample/numSamples/epoch: 783/1182/1, Loss: 673.782470703125\n","Train -> sample/numSamples/epoch: 784/1182/1, Loss: 1410.8927001953125\n","Train -> sample/numSamples/epoch: 785/1182/1, Loss: 1767.353759765625\n","Train -> sample/numSamples/epoch: 786/1182/1, Loss: 872.0620727539062\n","Train -> sample/numSamples/epoch: 787/1182/1, Loss: 1226.996826171875\n","Train -> sample/numSamples/epoch: 788/1182/1, Loss: 970.699462890625\n","Train -> sample/numSamples/epoch: 789/1182/1, Loss: 1272.365966796875\n","Train -> sample/numSamples/epoch: 790/1182/1, Loss: 1131.379150390625\n","Train -> sample/numSamples/epoch: 791/1182/1, Loss: 1247.3563232421875\n","Train -> sample/numSamples/epoch: 792/1182/1, Loss: 935.5728149414062\n","Train -> sample/numSamples/epoch: 793/1182/1, Loss: 1019.0405883789062\n","Train -> sample/numSamples/epoch: 794/1182/1, Loss: 880.783203125\n","Train -> sample/numSamples/epoch: 795/1182/1, Loss: 918.88671875\n","Train -> sample/numSamples/epoch: 796/1182/1, Loss: 740.9091186523438\n","Train -> sample/numSamples/epoch: 797/1182/1, Loss: 987.0313720703125\n","Train -> sample/numSamples/epoch: 798/1182/1, Loss: 1424.58447265625\n","Train -> sample/numSamples/epoch: 799/1182/1, Loss: 1348.55322265625\n","Train -> sample/numSamples/epoch: 800/1182/1, Loss: 1354.8194580078125\n","Train -> sample/numSamples/epoch: 801/1182/1, Loss: 1110.8072509765625\n","Train -> sample/numSamples/epoch: 802/1182/1, Loss: 1079.3055419921875\n","Train -> sample/numSamples/epoch: 803/1182/1, Loss: 1140.4306640625\n","Train -> sample/numSamples/epoch: 804/1182/1, Loss: 983.3217163085938\n","Train -> sample/numSamples/epoch: 805/1182/1, Loss: 1171.2401123046875\n","Train -> sample/numSamples/epoch: 806/1182/1, Loss: 925.833984375\n","Train -> sample/numSamples/epoch: 807/1182/1, Loss: 1016.3673706054688\n","Train -> sample/numSamples/epoch: 808/1182/1, Loss: 771.3804321289062\n","Train -> sample/numSamples/epoch: 809/1182/1, Loss: 1022.8079833984375\n","Train -> sample/numSamples/epoch: 810/1182/1, Loss: 1180.2279052734375\n","Train -> sample/numSamples/epoch: 811/1182/1, Loss: 1245.716552734375\n","Train -> sample/numSamples/epoch: 812/1182/1, Loss: 1148.98486328125\n","Train -> sample/numSamples/epoch: 813/1182/1, Loss: 950.103271484375\n","Train -> sample/numSamples/epoch: 814/1182/1, Loss: 780.8666381835938\n","Train -> sample/numSamples/epoch: 815/1182/1, Loss: 754.6892700195312\n","Train -> sample/numSamples/epoch: 816/1182/1, Loss: 1297.2548828125\n","Train -> sample/numSamples/epoch: 817/1182/1, Loss: 364.1066589355469\n","imatge guardada\n","Train -> sample/numSamples/epoch: 818/1182/1, Loss: 1073.1583251953125\n","Train -> sample/numSamples/epoch: 819/1182/1, Loss: 676.8936767578125\n","Train -> sample/numSamples/epoch: 820/1182/1, Loss: 1148.8802490234375\n","Train -> sample/numSamples/epoch: 821/1182/1, Loss: 742.9113159179688\n","Train -> sample/numSamples/epoch: 822/1182/1, Loss: 1054.9410400390625\n","Train -> sample/numSamples/epoch: 823/1182/1, Loss: 685.9893798828125\n","Train -> sample/numSamples/epoch: 824/1182/1, Loss: 844.8565673828125\n","Train -> sample/numSamples/epoch: 825/1182/1, Loss: 650.5950317382812\n","Train -> sample/numSamples/epoch: 826/1182/1, Loss: 873.6060791015625\n","Train -> sample/numSamples/epoch: 827/1182/1, Loss: 611.0376586914062\n","Train -> sample/numSamples/epoch: 828/1182/1, Loss: 1284.04541015625\n","Train -> sample/numSamples/epoch: 829/1182/1, Loss: 1129.9482421875\n","Train -> sample/numSamples/epoch: 830/1182/1, Loss: 1279.47900390625\n","Train -> sample/numSamples/epoch: 831/1182/1, Loss: 1398.86962890625\n","Train -> sample/numSamples/epoch: 832/1182/1, Loss: 1556.19140625\n","Train -> sample/numSamples/epoch: 833/1182/1, Loss: 1267.5135498046875\n","Train -> sample/numSamples/epoch: 834/1182/1, Loss: 944.078125\n","Train -> sample/numSamples/epoch: 835/1182/1, Loss: 1137.939453125\n","Train -> sample/numSamples/epoch: 836/1182/1, Loss: 762.78955078125\n","Train -> sample/numSamples/epoch: 837/1182/1, Loss: 819.2632446289062\n","Train -> sample/numSamples/epoch: 838/1182/1, Loss: 575.6212158203125\n","Train -> sample/numSamples/epoch: 839/1182/1, Loss: 995.6268920898438\n","Train -> sample/numSamples/epoch: 840/1182/1, Loss: 1092.050048828125\n","Train -> sample/numSamples/epoch: 841/1182/1, Loss: 1223.881103515625\n","Train -> sample/numSamples/epoch: 842/1182/1, Loss: 1180.674560546875\n","Train -> sample/numSamples/epoch: 843/1182/1, Loss: 1732.4605712890625\n","Train -> sample/numSamples/epoch: 844/1182/1, Loss: 1132.194580078125\n","Train -> sample/numSamples/epoch: 845/1182/1, Loss: 1000.270263671875\n","Train -> sample/numSamples/epoch: 846/1182/1, Loss: 773.8929443359375\n","Train -> sample/numSamples/epoch: 847/1182/1, Loss: 739.1336059570312\n","Train -> sample/numSamples/epoch: 848/1182/1, Loss: 990.5950317382812\n","Train -> sample/numSamples/epoch: 849/1182/1, Loss: 737.0988159179688\n","Train -> sample/numSamples/epoch: 850/1182/1, Loss: 1063.505126953125\n","Train -> sample/numSamples/epoch: 851/1182/1, Loss: 941.2658081054688\n","Train -> sample/numSamples/epoch: 852/1182/1, Loss: 998.2640991210938\n","Train -> sample/numSamples/epoch: 853/1182/1, Loss: 871.43359375\n","Train -> sample/numSamples/epoch: 854/1182/1, Loss: 744.6592407226562\n","Train -> sample/numSamples/epoch: 855/1182/1, Loss: 962.9824829101562\n","Train -> sample/numSamples/epoch: 856/1182/1, Loss: 1272.2301025390625\n","Train -> sample/numSamples/epoch: 857/1182/1, Loss: 834.9844360351562\n","Train -> sample/numSamples/epoch: 858/1182/1, Loss: 1091.045654296875\n","Train -> sample/numSamples/epoch: 859/1182/1, Loss: 770.3953247070312\n","Train -> sample/numSamples/epoch: 860/1182/1, Loss: 996.154541015625\n","Train -> sample/numSamples/epoch: 861/1182/1, Loss: 1154.3052978515625\n","Train -> sample/numSamples/epoch: 862/1182/1, Loss: 620.08154296875\n","Train -> sample/numSamples/epoch: 863/1182/1, Loss: 799.23388671875\n","Train -> sample/numSamples/epoch: 864/1182/1, Loss: 1284.0078125\n","Train -> sample/numSamples/epoch: 865/1182/1, Loss: 337.57293701171875\n","Train -> sample/numSamples/epoch: 866/1182/1, Loss: 1129.488037109375\n","Train -> sample/numSamples/epoch: 867/1182/1, Loss: 944.3157348632812\n","Train -> sample/numSamples/epoch: 868/1182/1, Loss: 1075.355712890625\n","Train -> sample/numSamples/epoch: 869/1182/1, Loss: 1063.506103515625\n","Train -> sample/numSamples/epoch: 870/1182/1, Loss: 1319.174560546875\n","Train -> sample/numSamples/epoch: 871/1182/1, Loss: 692.2982788085938\n","Train -> sample/numSamples/epoch: 872/1182/1, Loss: 1311.69775390625\n","Train -> sample/numSamples/epoch: 873/1182/1, Loss: 1211.6103515625\n","Train -> sample/numSamples/epoch: 874/1182/1, Loss: 755.8682861328125\n","Train -> sample/numSamples/epoch: 875/1182/1, Loss: 655.8226928710938\n","Train -> sample/numSamples/epoch: 876/1182/1, Loss: 967.6489868164062\n","Train -> sample/numSamples/epoch: 877/1182/1, Loss: 1070.17431640625\n","Train -> sample/numSamples/epoch: 878/1182/1, Loss: 1177.3817138671875\n","Train -> sample/numSamples/epoch: 879/1182/1, Loss: 1261.625\n","Train -> sample/numSamples/epoch: 880/1182/1, Loss: 1059.54638671875\n","Train -> sample/numSamples/epoch: 881/1182/1, Loss: 1193.3316650390625\n","Train -> sample/numSamples/epoch: 882/1182/1, Loss: 1133.4356689453125\n","Train -> sample/numSamples/epoch: 883/1182/1, Loss: 1330.4219970703125\n","Train -> sample/numSamples/epoch: 884/1182/1, Loss: 700.411376953125\n","Train -> sample/numSamples/epoch: 885/1182/1, Loss: 1075.0577392578125\n","Train -> sample/numSamples/epoch: 886/1182/1, Loss: 1572.3006591796875\n","Train -> sample/numSamples/epoch: 887/1182/1, Loss: 1423.5228271484375\n","Train -> sample/numSamples/epoch: 888/1182/1, Loss: 1066.675048828125\n","Train -> sample/numSamples/epoch: 889/1182/1, Loss: 727.0806274414062\n","Train -> sample/numSamples/epoch: 890/1182/1, Loss: 655.8685302734375\n","Train -> sample/numSamples/epoch: 891/1182/1, Loss: 1004.312255859375\n","Train -> sample/numSamples/epoch: 892/1182/1, Loss: 1114.779541015625\n","Train -> sample/numSamples/epoch: 893/1182/1, Loss: 996.4284057617188\n","Train -> sample/numSamples/epoch: 894/1182/1, Loss: 1358.9166259765625\n","Train -> sample/numSamples/epoch: 895/1182/1, Loss: 973.421630859375\n","Train -> sample/numSamples/epoch: 896/1182/1, Loss: 1007.81494140625\n","Train -> sample/numSamples/epoch: 897/1182/1, Loss: 1402.997802734375\n","Train -> sample/numSamples/epoch: 898/1182/1, Loss: 1089.3699951171875\n","Train -> sample/numSamples/epoch: 899/1182/1, Loss: 918.0162963867188\n","Train -> sample/numSamples/epoch: 900/1182/1, Loss: 1262.09521484375\n","Train -> sample/numSamples/epoch: 901/1182/1, Loss: 1345.7957763671875\n","Train -> sample/numSamples/epoch: 902/1182/1, Loss: 590.5205078125\n","Train -> sample/numSamples/epoch: 903/1182/1, Loss: 1176.0516357421875\n","Train -> sample/numSamples/epoch: 904/1182/1, Loss: 1139.2764892578125\n","Train -> sample/numSamples/epoch: 905/1182/1, Loss: 674.2838134765625\n","Train -> sample/numSamples/epoch: 906/1182/1, Loss: 1250.0155029296875\n","Train -> sample/numSamples/epoch: 907/1182/1, Loss: 816.27392578125\n","Train -> sample/numSamples/epoch: 908/1182/1, Loss: 1208.1605224609375\n","Train -> sample/numSamples/epoch: 909/1182/1, Loss: 692.622314453125\n","Train -> sample/numSamples/epoch: 910/1182/1, Loss: 1528.7147216796875\n","Train -> sample/numSamples/epoch: 911/1182/1, Loss: 822.0716552734375\n","Train -> sample/numSamples/epoch: 912/1182/1, Loss: 1049.7227783203125\n","Train -> sample/numSamples/epoch: 913/1182/1, Loss: 1030.214111328125\n","Train -> sample/numSamples/epoch: 914/1182/1, Loss: 1072.4395751953125\n","Train -> sample/numSamples/epoch: 915/1182/1, Loss: 1222.6192626953125\n","Train -> sample/numSamples/epoch: 916/1182/1, Loss: 734.068603515625\n","Train -> sample/numSamples/epoch: 917/1182/1, Loss: 1059.043212890625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 918/1182/1, Loss: 1008.349365234375\n","Train -> sample/numSamples/epoch: 919/1182/1, Loss: 1205.330078125\n","Train -> sample/numSamples/epoch: 920/1182/1, Loss: 1265.4232177734375\n","Train -> sample/numSamples/epoch: 921/1182/1, Loss: 1024.4644775390625\n","Train -> sample/numSamples/epoch: 922/1182/1, Loss: 868.1305541992188\n","Train -> sample/numSamples/epoch: 923/1182/1, Loss: 1221.439453125\n","Train -> sample/numSamples/epoch: 924/1182/1, Loss: 1312.7982177734375\n","Train -> sample/numSamples/epoch: 925/1182/1, Loss: 752.4508056640625\n","Train -> sample/numSamples/epoch: 926/1182/1, Loss: 1201.03076171875\n","Train -> sample/numSamples/epoch: 927/1182/1, Loss: 1069.34912109375\n","Train -> sample/numSamples/epoch: 928/1182/1, Loss: 928.936279296875\n","Train -> sample/numSamples/epoch: 929/1182/1, Loss: 1146.105712890625\n","Train -> sample/numSamples/epoch: 930/1182/1, Loss: 776.2240600585938\n","Train -> sample/numSamples/epoch: 931/1182/1, Loss: 1225.3056640625\n","Train -> sample/numSamples/epoch: 932/1182/1, Loss: 1124.320556640625\n","Train -> sample/numSamples/epoch: 933/1182/1, Loss: 702.4744873046875\n","Train -> sample/numSamples/epoch: 934/1182/1, Loss: 945.0753784179688\n","Train -> sample/numSamples/epoch: 935/1182/1, Loss: 640.6561889648438\n","Train -> sample/numSamples/epoch: 936/1182/1, Loss: 1349.3172607421875\n","Train -> sample/numSamples/epoch: 937/1182/1, Loss: 1131.2158203125\n","Train -> sample/numSamples/epoch: 938/1182/1, Loss: 1123.6610107421875\n","Train -> sample/numSamples/epoch: 939/1182/1, Loss: 1032.8302001953125\n","Train -> sample/numSamples/epoch: 940/1182/1, Loss: 1259.0323486328125\n","Train -> sample/numSamples/epoch: 941/1182/1, Loss: 872.4395141601562\n","Train -> sample/numSamples/epoch: 942/1182/1, Loss: 1135.995361328125\n","Train -> sample/numSamples/epoch: 943/1182/1, Loss: 861.8125610351562\n","Train -> sample/numSamples/epoch: 944/1182/1, Loss: 1101.8267822265625\n","Train -> sample/numSamples/epoch: 945/1182/1, Loss: 779.528564453125\n","Train -> sample/numSamples/epoch: 946/1182/1, Loss: 971.9688720703125\n","Train -> sample/numSamples/epoch: 947/1182/1, Loss: 864.5828247070312\n","Train -> sample/numSamples/epoch: 948/1182/1, Loss: 1217.8516845703125\n","Train -> sample/numSamples/epoch: 949/1182/1, Loss: 743.1304321289062\n","Train -> sample/numSamples/epoch: 950/1182/1, Loss: 847.9199829101562\n","Train -> sample/numSamples/epoch: 951/1182/1, Loss: 1146.5733642578125\n","Train -> sample/numSamples/epoch: 952/1182/1, Loss: 882.77587890625\n","Train -> sample/numSamples/epoch: 953/1182/1, Loss: 816.553955078125\n","Train -> sample/numSamples/epoch: 954/1182/1, Loss: 1103.438232421875\n","Train -> sample/numSamples/epoch: 955/1182/1, Loss: 1031.068603515625\n","Train -> sample/numSamples/epoch: 956/1182/1, Loss: 890.4847412109375\n","Train -> sample/numSamples/epoch: 957/1182/1, Loss: 1089.65673828125\n","Train -> sample/numSamples/epoch: 958/1182/1, Loss: 1167.7127685546875\n","Train -> sample/numSamples/epoch: 959/1182/1, Loss: 1099.09130859375\n","Train -> sample/numSamples/epoch: 960/1182/1, Loss: 816.6691284179688\n","Train -> sample/numSamples/epoch: 961/1182/1, Loss: 1147.1949462890625\n","Train -> sample/numSamples/epoch: 962/1182/1, Loss: 1143.280517578125\n","Train -> sample/numSamples/epoch: 963/1182/1, Loss: 1036.62060546875\n","Train -> sample/numSamples/epoch: 964/1182/1, Loss: 1110.12255859375\n","Train -> sample/numSamples/epoch: 965/1182/1, Loss: 722.1119995117188\n","Train -> sample/numSamples/epoch: 966/1182/1, Loss: 928.4910888671875\n","Train -> sample/numSamples/epoch: 967/1182/1, Loss: 541.6458129882812\n","Train -> sample/numSamples/epoch: 968/1182/1, Loss: 1390.894287109375\n","Train -> sample/numSamples/epoch: 969/1182/1, Loss: 1104.354248046875\n","Train -> sample/numSamples/epoch: 970/1182/1, Loss: 1227.5325927734375\n","Train -> sample/numSamples/epoch: 971/1182/1, Loss: 702.9254150390625\n","Train -> sample/numSamples/epoch: 972/1182/1, Loss: 1477.8082275390625\n","Train -> sample/numSamples/epoch: 973/1182/1, Loss: 884.6990356445312\n","Train -> sample/numSamples/epoch: 974/1182/1, Loss: 1276.65673828125\n","Train -> sample/numSamples/epoch: 975/1182/1, Loss: 1289.760498046875\n","Train -> sample/numSamples/epoch: 976/1182/1, Loss: 1226.5272216796875\n","Train -> sample/numSamples/epoch: 977/1182/1, Loss: 922.8722534179688\n","Train -> sample/numSamples/epoch: 978/1182/1, Loss: 1549.4169921875\n","Train -> sample/numSamples/epoch: 979/1182/1, Loss: 1236.8505859375\n","Train -> sample/numSamples/epoch: 980/1182/1, Loss: 1494.849365234375\n","Train -> sample/numSamples/epoch: 981/1182/1, Loss: 1062.0145263671875\n","Train -> sample/numSamples/epoch: 982/1182/1, Loss: 936.1161499023438\n","Train -> sample/numSamples/epoch: 983/1182/1, Loss: 1119.6962890625\n","Train -> sample/numSamples/epoch: 984/1182/1, Loss: 904.505126953125\n","Train -> sample/numSamples/epoch: 985/1182/1, Loss: 734.1046142578125\n","Train -> sample/numSamples/epoch: 986/1182/1, Loss: 1047.9915771484375\n","Train -> sample/numSamples/epoch: 987/1182/1, Loss: 946.5399780273438\n","Train -> sample/numSamples/epoch: 988/1182/1, Loss: 562.2286376953125\n","Train -> sample/numSamples/epoch: 989/1182/1, Loss: 1169.4149169921875\n","Train -> sample/numSamples/epoch: 990/1182/1, Loss: 1334.4017333984375\n","Train -> sample/numSamples/epoch: 991/1182/1, Loss: 892.0428466796875\n","Train -> sample/numSamples/epoch: 992/1182/1, Loss: 1167.730712890625\n","Train -> sample/numSamples/epoch: 993/1182/1, Loss: 1060.8736572265625\n","Train -> sample/numSamples/epoch: 994/1182/1, Loss: 946.8030395507812\n","Train -> sample/numSamples/epoch: 995/1182/1, Loss: 1224.0635986328125\n","Train -> sample/numSamples/epoch: 996/1182/1, Loss: 1240.525146484375\n","Train -> sample/numSamples/epoch: 997/1182/1, Loss: 800.507568359375\n","Train -> sample/numSamples/epoch: 998/1182/1, Loss: 946.4009399414062\n","Train -> sample/numSamples/epoch: 999/1182/1, Loss: 1041.596435546875\n","Train -> sample/numSamples/epoch: 1000/1182/1, Loss: 621.6591186523438\n","Train -> sample/numSamples/epoch: 1001/1182/1, Loss: 575.6359252929688\n","Train -> sample/numSamples/epoch: 1002/1182/1, Loss: 1305.740478515625\n","Train -> sample/numSamples/epoch: 1003/1182/1, Loss: 871.1514282226562\n","Train -> sample/numSamples/epoch: 1004/1182/1, Loss: 861.0530395507812\n","Train -> sample/numSamples/epoch: 1005/1182/1, Loss: 769.515625\n","Train -> sample/numSamples/epoch: 1006/1182/1, Loss: 1015.6232299804688\n","Train -> sample/numSamples/epoch: 1007/1182/1, Loss: 1042.606689453125\n","Train -> sample/numSamples/epoch: 1008/1182/1, Loss: 1401.5142822265625\n","Train -> sample/numSamples/epoch: 1009/1182/1, Loss: 945.932373046875\n","Train -> sample/numSamples/epoch: 1010/1182/1, Loss: 1096.2291259765625\n","Train -> sample/numSamples/epoch: 1011/1182/1, Loss: 1467.5325927734375\n","Train -> sample/numSamples/epoch: 1012/1182/1, Loss: 805.7169189453125\n","Train -> sample/numSamples/epoch: 1013/1182/1, Loss: 1072.556640625\n","Train -> sample/numSamples/epoch: 1014/1182/1, Loss: 1212.2186279296875\n","Train -> sample/numSamples/epoch: 1015/1182/1, Loss: 966.216552734375\n","Train -> sample/numSamples/epoch: 1016/1182/1, Loss: 1111.229736328125\n","Train -> sample/numSamples/epoch: 1017/1182/1, Loss: 1484.9027099609375\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1018/1182/1, Loss: 989.5770874023438\n","Train -> sample/numSamples/epoch: 1019/1182/1, Loss: 718.5745849609375\n","Train -> sample/numSamples/epoch: 1020/1182/1, Loss: 698.4681396484375\n","Train -> sample/numSamples/epoch: 1021/1182/1, Loss: 1028.2198486328125\n","Train -> sample/numSamples/epoch: 1022/1182/1, Loss: 1118.295166015625\n","Train -> sample/numSamples/epoch: 1023/1182/1, Loss: 1443.544189453125\n","Train -> sample/numSamples/epoch: 1024/1182/1, Loss: 1291.8360595703125\n","Train -> sample/numSamples/epoch: 1025/1182/1, Loss: 1186.7232666015625\n","Train -> sample/numSamples/epoch: 1026/1182/1, Loss: 1294.4649658203125\n","Train -> sample/numSamples/epoch: 1027/1182/1, Loss: 1016.5231323242188\n","Train -> sample/numSamples/epoch: 1028/1182/1, Loss: 990.8948364257812\n","Train -> sample/numSamples/epoch: 1029/1182/1, Loss: 993.7921142578125\n","Train -> sample/numSamples/epoch: 1030/1182/1, Loss: 971.5172119140625\n","Train -> sample/numSamples/epoch: 1031/1182/1, Loss: 1579.8905029296875\n","Train -> sample/numSamples/epoch: 1032/1182/1, Loss: 1238.8642578125\n","Train -> sample/numSamples/epoch: 1033/1182/1, Loss: 811.4696655273438\n","Train -> sample/numSamples/epoch: 1034/1182/1, Loss: 1058.784423828125\n","Train -> sample/numSamples/epoch: 1035/1182/1, Loss: 909.2630615234375\n","Train -> sample/numSamples/epoch: 1036/1182/1, Loss: 1162.9552001953125\n","Train -> sample/numSamples/epoch: 1037/1182/1, Loss: 1608.580078125\n","Train -> sample/numSamples/epoch: 1038/1182/1, Loss: 1345.48291015625\n","Train -> sample/numSamples/epoch: 1039/1182/1, Loss: 1107.5096435546875\n","Train -> sample/numSamples/epoch: 1040/1182/1, Loss: 568.3107299804688\n","Train -> sample/numSamples/epoch: 1041/1182/1, Loss: 1220.85302734375\n","Train -> sample/numSamples/epoch: 1042/1182/1, Loss: 947.9198608398438\n","Train -> sample/numSamples/epoch: 1043/1182/1, Loss: 1415.6044921875\n","Train -> sample/numSamples/epoch: 1044/1182/1, Loss: 1243.374267578125\n","Train -> sample/numSamples/epoch: 1045/1182/1, Loss: 1107.4058837890625\n","Train -> sample/numSamples/epoch: 1046/1182/1, Loss: 777.5349731445312\n","Train -> sample/numSamples/epoch: 1047/1182/1, Loss: 1158.669921875\n","Train -> sample/numSamples/epoch: 1048/1182/1, Loss: 810.1195068359375\n","Train -> sample/numSamples/epoch: 1049/1182/1, Loss: 758.708251953125\n","Train -> sample/numSamples/epoch: 1050/1182/1, Loss: 814.95703125\n","Train -> sample/numSamples/epoch: 1051/1182/1, Loss: 1083.94580078125\n","Train -> sample/numSamples/epoch: 1052/1182/1, Loss: 1030.750244140625\n","Train -> sample/numSamples/epoch: 1053/1182/1, Loss: 915.1792602539062\n","Train -> sample/numSamples/epoch: 1054/1182/1, Loss: 1315.365478515625\n","Train -> sample/numSamples/epoch: 1055/1182/1, Loss: 931.1685180664062\n","Train -> sample/numSamples/epoch: 1056/1182/1, Loss: 1170.151611328125\n","Train -> sample/numSamples/epoch: 1057/1182/1, Loss: 775.110595703125\n","Train -> sample/numSamples/epoch: 1058/1182/1, Loss: 935.7865600585938\n","Train -> sample/numSamples/epoch: 1059/1182/1, Loss: 1321.14599609375\n","Train -> sample/numSamples/epoch: 1060/1182/1, Loss: 1144.0316162109375\n","Train -> sample/numSamples/epoch: 1061/1182/1, Loss: 893.4437866210938\n","Train -> sample/numSamples/epoch: 1062/1182/1, Loss: 1256.8349609375\n","Train -> sample/numSamples/epoch: 1063/1182/1, Loss: 1043.435791015625\n","Train -> sample/numSamples/epoch: 1064/1182/1, Loss: 1076.29052734375\n","Train -> sample/numSamples/epoch: 1065/1182/1, Loss: 802.1119384765625\n","Train -> sample/numSamples/epoch: 1066/1182/1, Loss: 1082.9803466796875\n","Train -> sample/numSamples/epoch: 1067/1182/1, Loss: 1057.0399169921875\n","Train -> sample/numSamples/epoch: 1068/1182/1, Loss: 1313.1551513671875\n","Train -> sample/numSamples/epoch: 1069/1182/1, Loss: 1130.3955078125\n","Train -> sample/numSamples/epoch: 1070/1182/1, Loss: 1171.671630859375\n","Train -> sample/numSamples/epoch: 1071/1182/1, Loss: 1323.061767578125\n","Train -> sample/numSamples/epoch: 1072/1182/1, Loss: 1444.9881591796875\n","Train -> sample/numSamples/epoch: 1073/1182/1, Loss: 1359.0416259765625\n","Train -> sample/numSamples/epoch: 1074/1182/1, Loss: 1164.2169189453125\n","Train -> sample/numSamples/epoch: 1075/1182/1, Loss: 1332.463134765625\n","Train -> sample/numSamples/epoch: 1076/1182/1, Loss: 805.45703125\n","Train -> sample/numSamples/epoch: 1077/1182/1, Loss: 808.0606689453125\n","Train -> sample/numSamples/epoch: 1078/1182/1, Loss: 1532.4107666015625\n","Train -> sample/numSamples/epoch: 1079/1182/1, Loss: 529.6957397460938\n","Train -> sample/numSamples/epoch: 1080/1182/1, Loss: 1283.734375\n","Train -> sample/numSamples/epoch: 1081/1182/1, Loss: 824.0961303710938\n","Train -> sample/numSamples/epoch: 1082/1182/1, Loss: 897.0552978515625\n","Train -> sample/numSamples/epoch: 1083/1182/1, Loss: 1065.9442138671875\n","Train -> sample/numSamples/epoch: 1084/1182/1, Loss: 1065.9346923828125\n","Train -> sample/numSamples/epoch: 1085/1182/1, Loss: 1037.5069580078125\n","Train -> sample/numSamples/epoch: 1086/1182/1, Loss: 1183.1761474609375\n","Train -> sample/numSamples/epoch: 1087/1182/1, Loss: 1106.8575439453125\n","Train -> sample/numSamples/epoch: 1088/1182/1, Loss: 782.9285278320312\n","Train -> sample/numSamples/epoch: 1089/1182/1, Loss: 971.258544921875\n","Train -> sample/numSamples/epoch: 1090/1182/1, Loss: 1452.1097412109375\n","Train -> sample/numSamples/epoch: 1091/1182/1, Loss: 855.027099609375\n","Train -> sample/numSamples/epoch: 1092/1182/1, Loss: 960.2784423828125\n","Train -> sample/numSamples/epoch: 1093/1182/1, Loss: 1091.2152099609375\n","Train -> sample/numSamples/epoch: 1094/1182/1, Loss: 850.3990478515625\n","Train -> sample/numSamples/epoch: 1095/1182/1, Loss: 1124.7366943359375\n","Train -> sample/numSamples/epoch: 1096/1182/1, Loss: 1214.912841796875\n","Train -> sample/numSamples/epoch: 1097/1182/1, Loss: 1065.560791015625\n","Train -> sample/numSamples/epoch: 1098/1182/1, Loss: 775.0760498046875\n","Train -> sample/numSamples/epoch: 1099/1182/1, Loss: 443.09228515625\n","Train -> sample/numSamples/epoch: 1100/1182/1, Loss: 1149.8599853515625\n","Train -> sample/numSamples/epoch: 1101/1182/1, Loss: 636.9529418945312\n","Train -> sample/numSamples/epoch: 1102/1182/1, Loss: 1121.714599609375\n","Train -> sample/numSamples/epoch: 1103/1182/1, Loss: 873.7650756835938\n","Train -> sample/numSamples/epoch: 1104/1182/1, Loss: 1040.09912109375\n","Train -> sample/numSamples/epoch: 1105/1182/1, Loss: 1170.7689208984375\n","Train -> sample/numSamples/epoch: 1106/1182/1, Loss: 1074.31787109375\n","Train -> sample/numSamples/epoch: 1107/1182/1, Loss: 1002.8402099609375\n","Train -> sample/numSamples/epoch: 1108/1182/1, Loss: 805.235107421875\n","Train -> sample/numSamples/epoch: 1109/1182/1, Loss: 795.4061889648438\n","Train -> sample/numSamples/epoch: 1110/1182/1, Loss: 747.5072631835938\n","Train -> sample/numSamples/epoch: 1111/1182/1, Loss: 654.4591674804688\n","Train -> sample/numSamples/epoch: 1112/1182/1, Loss: 936.5809326171875\n","Train -> sample/numSamples/epoch: 1113/1182/1, Loss: 818.7071533203125\n","Train -> sample/numSamples/epoch: 1114/1182/1, Loss: 703.1482543945312\n","Train -> sample/numSamples/epoch: 1115/1182/1, Loss: 507.89166259765625\n","Train -> sample/numSamples/epoch: 1116/1182/1, Loss: 1266.3558349609375\n","Train -> sample/numSamples/epoch: 1117/1182/1, Loss: 940.4498901367188\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1118/1182/1, Loss: 779.5360107421875\n","Train -> sample/numSamples/epoch: 1119/1182/1, Loss: 960.851318359375\n","Train -> sample/numSamples/epoch: 1120/1182/1, Loss: 767.8851318359375\n","Train -> sample/numSamples/epoch: 1121/1182/1, Loss: 436.4019775390625\n","Train -> sample/numSamples/epoch: 1122/1182/1, Loss: 613.9981689453125\n","Train -> sample/numSamples/epoch: 1123/1182/1, Loss: 1740.6982421875\n","Train -> sample/numSamples/epoch: 1124/1182/1, Loss: 916.5896606445312\n","Train -> sample/numSamples/epoch: 1125/1182/1, Loss: 1300.116455078125\n","Train -> sample/numSamples/epoch: 1126/1182/1, Loss: 975.6607055664062\n","Train -> sample/numSamples/epoch: 1127/1182/1, Loss: 1250.7059326171875\n","Train -> sample/numSamples/epoch: 1128/1182/1, Loss: 1260.0263671875\n","Train -> sample/numSamples/epoch: 1129/1182/1, Loss: 1228.2911376953125\n","Train -> sample/numSamples/epoch: 1130/1182/1, Loss: 1484.584716796875\n","Train -> sample/numSamples/epoch: 1131/1182/1, Loss: 964.2374877929688\n","Train -> sample/numSamples/epoch: 1132/1182/1, Loss: 958.3191528320312\n","Train -> sample/numSamples/epoch: 1133/1182/1, Loss: 890.9999389648438\n","Train -> sample/numSamples/epoch: 1134/1182/1, Loss: 1090.0491943359375\n","Train -> sample/numSamples/epoch: 1135/1182/1, Loss: 1037.0877685546875\n","Train -> sample/numSamples/epoch: 1136/1182/1, Loss: 1396.668212890625\n","Train -> sample/numSamples/epoch: 1137/1182/1, Loss: 661.576171875\n","Train -> sample/numSamples/epoch: 1138/1182/1, Loss: 751.5031127929688\n","Train -> sample/numSamples/epoch: 1139/1182/1, Loss: 1463.262939453125\n","Train -> sample/numSamples/epoch: 1140/1182/1, Loss: 646.0614013671875\n","Train -> sample/numSamples/epoch: 1141/1182/1, Loss: 865.5842895507812\n","Train -> sample/numSamples/epoch: 1142/1182/1, Loss: 1380.7135009765625\n","Train -> sample/numSamples/epoch: 1143/1182/1, Loss: 1084.79638671875\n","Train -> sample/numSamples/epoch: 1144/1182/1, Loss: 868.656005859375\n","Train -> sample/numSamples/epoch: 1145/1182/1, Loss: 790.067138671875\n","Train -> sample/numSamples/epoch: 1146/1182/1, Loss: 1191.991943359375\n","Train -> sample/numSamples/epoch: 1147/1182/1, Loss: 859.0722045898438\n","Train -> sample/numSamples/epoch: 1148/1182/1, Loss: 1052.05908203125\n","Train -> sample/numSamples/epoch: 1149/1182/1, Loss: 820.9889526367188\n","Train -> sample/numSamples/epoch: 1150/1182/1, Loss: 830.68359375\n","Train -> sample/numSamples/epoch: 1151/1182/1, Loss: 811.3702392578125\n","Train -> sample/numSamples/epoch: 1152/1182/1, Loss: 1165.0865478515625\n","Train -> sample/numSamples/epoch: 1153/1182/1, Loss: 888.7147827148438\n","Train -> sample/numSamples/epoch: 1154/1182/1, Loss: 1045.2330322265625\n","Train -> sample/numSamples/epoch: 1155/1182/1, Loss: 880.146728515625\n","Train -> sample/numSamples/epoch: 1156/1182/1, Loss: 806.3555297851562\n","Train -> sample/numSamples/epoch: 1157/1182/1, Loss: 1190.9036865234375\n","Train -> sample/numSamples/epoch: 1158/1182/1, Loss: 1219.70703125\n","Train -> sample/numSamples/epoch: 1159/1182/1, Loss: 656.5230102539062\n","Train -> sample/numSamples/epoch: 1160/1182/1, Loss: 1401.0592041015625\n","Train -> sample/numSamples/epoch: 1161/1182/1, Loss: 1052.7235107421875\n","Train -> sample/numSamples/epoch: 1162/1182/1, Loss: 934.9556884765625\n","Train -> sample/numSamples/epoch: 1163/1182/1, Loss: 1141.8079833984375\n","Train -> sample/numSamples/epoch: 1164/1182/1, Loss: 1063.4967041015625\n","Train -> sample/numSamples/epoch: 1165/1182/1, Loss: 951.0933227539062\n","Train -> sample/numSamples/epoch: 1166/1182/1, Loss: 1155.7294921875\n","Train -> sample/numSamples/epoch: 1167/1182/1, Loss: 1086.263671875\n","Train -> sample/numSamples/epoch: 1168/1182/1, Loss: 747.3674926757812\n","Train -> sample/numSamples/epoch: 1169/1182/1, Loss: 898.5624389648438\n","Train -> sample/numSamples/epoch: 1170/1182/1, Loss: 1212.84765625\n","Train -> sample/numSamples/epoch: 1171/1182/1, Loss: 767.2594604492188\n","Train -> sample/numSamples/epoch: 1172/1182/1, Loss: 738.5274047851562\n","Train -> sample/numSamples/epoch: 1173/1182/1, Loss: 1233.8197021484375\n","Train -> sample/numSamples/epoch: 1174/1182/1, Loss: 1195.039794921875\n","Train -> sample/numSamples/epoch: 1175/1182/1, Loss: 1224.9576416015625\n","Train -> sample/numSamples/epoch: 1176/1182/1, Loss: 711.3955688476562\n","Train -> sample/numSamples/epoch: 1177/1182/1, Loss: 1001.8981323242188\n","Train -> sample/numSamples/epoch: 1178/1182/1, Loss: 901.2731323242188\n","Train -> sample/numSamples/epoch: 1179/1182/1, Loss: 947.976318359375\n","Train -> sample/numSamples/epoch: 1180/1182/1, Loss: 1043.0074462890625\n","Train -> sample/numSamples/epoch: 1181/1182/1, Loss: 1313.2281494140625\n","Train -> sample/numSamples/epoch: 0/1182/2, Loss: 822.6229858398438\n","Train -> sample/numSamples/epoch: 1/1182/2, Loss: 872.1729736328125\n","Train -> sample/numSamples/epoch: 2/1182/2, Loss: 785.2619018554688\n","Train -> sample/numSamples/epoch: 3/1182/2, Loss: 720.1600952148438\n","Train -> sample/numSamples/epoch: 4/1182/2, Loss: 765.4024047851562\n","Train -> sample/numSamples/epoch: 5/1182/2, Loss: 860.2425537109375\n","Train -> sample/numSamples/epoch: 6/1182/2, Loss: 742.90478515625\n","Train -> sample/numSamples/epoch: 7/1182/2, Loss: 1213.3853759765625\n","Train -> sample/numSamples/epoch: 8/1182/2, Loss: 1228.1444091796875\n","Train -> sample/numSamples/epoch: 9/1182/2, Loss: 1342.8304443359375\n","Train -> sample/numSamples/epoch: 10/1182/2, Loss: 642.6654052734375\n","Train -> sample/numSamples/epoch: 11/1182/2, Loss: 976.2324829101562\n","Train -> sample/numSamples/epoch: 12/1182/2, Loss: 1157.8160400390625\n","Train -> sample/numSamples/epoch: 13/1182/2, Loss: 1192.634033203125\n","Train -> sample/numSamples/epoch: 14/1182/2, Loss: 474.2647399902344\n","Train -> sample/numSamples/epoch: 15/1182/2, Loss: 1467.4005126953125\n","Train -> sample/numSamples/epoch: 16/1182/2, Loss: 874.871337890625\n","Train -> sample/numSamples/epoch: 17/1182/2, Loss: 1217.17431640625\n","Train -> sample/numSamples/epoch: 18/1182/2, Loss: 952.337158203125\n","Train -> sample/numSamples/epoch: 19/1182/2, Loss: 1419.0704345703125\n","Train -> sample/numSamples/epoch: 20/1182/2, Loss: 1017.9164428710938\n","Train -> sample/numSamples/epoch: 21/1182/2, Loss: 823.5917358398438\n","Train -> sample/numSamples/epoch: 22/1182/2, Loss: 1185.85205078125\n","Train -> sample/numSamples/epoch: 23/1182/2, Loss: 838.042724609375\n","Train -> sample/numSamples/epoch: 24/1182/2, Loss: 956.5585327148438\n","Train -> sample/numSamples/epoch: 25/1182/2, Loss: 1200.6234130859375\n","Train -> sample/numSamples/epoch: 26/1182/2, Loss: 1130.087646484375\n","Train -> sample/numSamples/epoch: 27/1182/2, Loss: 1109.1927490234375\n","Train -> sample/numSamples/epoch: 28/1182/2, Loss: 992.1808471679688\n","Train -> sample/numSamples/epoch: 29/1182/2, Loss: 1114.3792724609375\n","Train -> sample/numSamples/epoch: 30/1182/2, Loss: 648.5380249023438\n","Train -> sample/numSamples/epoch: 31/1182/2, Loss: 918.2647094726562\n","Train -> sample/numSamples/epoch: 32/1182/2, Loss: 1168.143798828125\n","Train -> sample/numSamples/epoch: 33/1182/2, Loss: 857.7708129882812\n","Train -> sample/numSamples/epoch: 34/1182/2, Loss: 1228.422607421875\n","Train -> sample/numSamples/epoch: 35/1182/2, Loss: 664.9890747070312\n","imatge guardada\n","Train -> sample/numSamples/epoch: 36/1182/2, Loss: 968.6359252929688\n","Train -> sample/numSamples/epoch: 37/1182/2, Loss: 1187.470947265625\n","Train -> sample/numSamples/epoch: 38/1182/2, Loss: 1025.4454345703125\n","Train -> sample/numSamples/epoch: 39/1182/2, Loss: 1350.4937744140625\n","Train -> sample/numSamples/epoch: 40/1182/2, Loss: 1061.582763671875\n","Train -> sample/numSamples/epoch: 41/1182/2, Loss: 1196.2052001953125\n","Train -> sample/numSamples/epoch: 42/1182/2, Loss: 875.9561767578125\n","Train -> sample/numSamples/epoch: 43/1182/2, Loss: 1214.8145751953125\n","Train -> sample/numSamples/epoch: 44/1182/2, Loss: 777.942626953125\n","Train -> sample/numSamples/epoch: 45/1182/2, Loss: 1085.8525390625\n","Train -> sample/numSamples/epoch: 46/1182/2, Loss: 748.1589965820312\n","Train -> sample/numSamples/epoch: 47/1182/2, Loss: 1183.67138671875\n","Train -> sample/numSamples/epoch: 48/1182/2, Loss: 1150.263427734375\n","Train -> sample/numSamples/epoch: 49/1182/2, Loss: 777.9640502929688\n","Train -> sample/numSamples/epoch: 50/1182/2, Loss: 761.9606323242188\n","Train -> sample/numSamples/epoch: 51/1182/2, Loss: 1106.6741943359375\n","Train -> sample/numSamples/epoch: 52/1182/2, Loss: 989.492431640625\n","Train -> sample/numSamples/epoch: 53/1182/2, Loss: 1184.7501220703125\n","Train -> sample/numSamples/epoch: 54/1182/2, Loss: 827.251220703125\n","Train -> sample/numSamples/epoch: 55/1182/2, Loss: 679.8050537109375\n","Train -> sample/numSamples/epoch: 56/1182/2, Loss: 881.3182983398438\n","Train -> sample/numSamples/epoch: 57/1182/2, Loss: 864.0369873046875\n","Train -> sample/numSamples/epoch: 58/1182/2, Loss: 1172.5877685546875\n","Train -> sample/numSamples/epoch: 59/1182/2, Loss: 952.61083984375\n","Train -> sample/numSamples/epoch: 60/1182/2, Loss: 922.0498657226562\n","Train -> sample/numSamples/epoch: 61/1182/2, Loss: 1210.604248046875\n","Train -> sample/numSamples/epoch: 62/1182/2, Loss: 1016.4852905273438\n","Train -> sample/numSamples/epoch: 63/1182/2, Loss: 860.1863403320312\n","Train -> sample/numSamples/epoch: 64/1182/2, Loss: 1006.6043701171875\n","Train -> sample/numSamples/epoch: 65/1182/2, Loss: 1336.57470703125\n","Train -> sample/numSamples/epoch: 66/1182/2, Loss: 1534.055908203125\n","Train -> sample/numSamples/epoch: 67/1182/2, Loss: 1434.736328125\n","Train -> sample/numSamples/epoch: 68/1182/2, Loss: 1103.2359619140625\n","Train -> sample/numSamples/epoch: 69/1182/2, Loss: 923.97119140625\n","Train -> sample/numSamples/epoch: 70/1182/2, Loss: 781.3807373046875\n","Train -> sample/numSamples/epoch: 71/1182/2, Loss: 625.9835815429688\n","Train -> sample/numSamples/epoch: 72/1182/2, Loss: 1196.37109375\n","Train -> sample/numSamples/epoch: 73/1182/2, Loss: 861.2671508789062\n","Train -> sample/numSamples/epoch: 74/1182/2, Loss: 1343.4219970703125\n","Train -> sample/numSamples/epoch: 75/1182/2, Loss: 1182.480712890625\n","Train -> sample/numSamples/epoch: 76/1182/2, Loss: 1077.743408203125\n","Train -> sample/numSamples/epoch: 77/1182/2, Loss: 750.616455078125\n","Train -> sample/numSamples/epoch: 78/1182/2, Loss: 896.6417846679688\n","Train -> sample/numSamples/epoch: 79/1182/2, Loss: 1108.8846435546875\n","Train -> sample/numSamples/epoch: 80/1182/2, Loss: 1038.1455078125\n","Train -> sample/numSamples/epoch: 81/1182/2, Loss: 860.105224609375\n","Train -> sample/numSamples/epoch: 82/1182/2, Loss: 1643.2467041015625\n","Train -> sample/numSamples/epoch: 83/1182/2, Loss: 860.1527099609375\n","Train -> sample/numSamples/epoch: 84/1182/2, Loss: 1178.605712890625\n","Train -> sample/numSamples/epoch: 85/1182/2, Loss: 1251.5849609375\n","Train -> sample/numSamples/epoch: 86/1182/2, Loss: 854.9535522460938\n","Train -> sample/numSamples/epoch: 87/1182/2, Loss: 940.5712280273438\n","Train -> sample/numSamples/epoch: 88/1182/2, Loss: 859.7572631835938\n","Train -> sample/numSamples/epoch: 89/1182/2, Loss: 1085.1656494140625\n","Train -> sample/numSamples/epoch: 90/1182/2, Loss: 885.1670532226562\n","Train -> sample/numSamples/epoch: 91/1182/2, Loss: 588.1998291015625\n","Train -> sample/numSamples/epoch: 92/1182/2, Loss: 868.0501708984375\n","Train -> sample/numSamples/epoch: 93/1182/2, Loss: 954.8253173828125\n","Train -> sample/numSamples/epoch: 94/1182/2, Loss: 923.6165161132812\n","Train -> sample/numSamples/epoch: 95/1182/2, Loss: 963.8531494140625\n","Train -> sample/numSamples/epoch: 96/1182/2, Loss: 1705.4425048828125\n","Train -> sample/numSamples/epoch: 97/1182/2, Loss: 1112.1572265625\n","Train -> sample/numSamples/epoch: 98/1182/2, Loss: 1352.6494140625\n","Train -> sample/numSamples/epoch: 99/1182/2, Loss: 1518.1290283203125\n","Train -> sample/numSamples/epoch: 100/1182/2, Loss: 765.0427856445312\n","Train -> sample/numSamples/epoch: 101/1182/2, Loss: 1158.229248046875\n","Train -> sample/numSamples/epoch: 102/1182/2, Loss: 692.78466796875\n","Train -> sample/numSamples/epoch: 103/1182/2, Loss: 870.8812866210938\n","Train -> sample/numSamples/epoch: 104/1182/2, Loss: 1230.4996337890625\n","Train -> sample/numSamples/epoch: 105/1182/2, Loss: 951.6679077148438\n","Train -> sample/numSamples/epoch: 106/1182/2, Loss: 836.06494140625\n","Train -> sample/numSamples/epoch: 107/1182/2, Loss: 866.6231079101562\n","Train -> sample/numSamples/epoch: 108/1182/2, Loss: 927.441162109375\n","Train -> sample/numSamples/epoch: 109/1182/2, Loss: 833.6312866210938\n","Train -> sample/numSamples/epoch: 110/1182/2, Loss: 783.283447265625\n","Train -> sample/numSamples/epoch: 111/1182/2, Loss: 625.3024291992188\n","Train -> sample/numSamples/epoch: 112/1182/2, Loss: 1104.75830078125\n","Train -> sample/numSamples/epoch: 113/1182/2, Loss: 670.6299438476562\n","Train -> sample/numSamples/epoch: 114/1182/2, Loss: 1141.8853759765625\n","Train -> sample/numSamples/epoch: 115/1182/2, Loss: 1379.7703857421875\n","Train -> sample/numSamples/epoch: 116/1182/2, Loss: 1011.2764282226562\n","Train -> sample/numSamples/epoch: 117/1182/2, Loss: 1233.6112060546875\n","Train -> sample/numSamples/epoch: 118/1182/2, Loss: 1198.0491943359375\n","Train -> sample/numSamples/epoch: 119/1182/2, Loss: 626.245849609375\n","Train -> sample/numSamples/epoch: 120/1182/2, Loss: 1437.7965087890625\n","Train -> sample/numSamples/epoch: 121/1182/2, Loss: 1072.9052734375\n","Train -> sample/numSamples/epoch: 122/1182/2, Loss: 1319.7593994140625\n","Train -> sample/numSamples/epoch: 123/1182/2, Loss: 770.2323608398438\n","Train -> sample/numSamples/epoch: 124/1182/2, Loss: 1305.9566650390625\n","Train -> sample/numSamples/epoch: 125/1182/2, Loss: 1122.9510498046875\n","Train -> sample/numSamples/epoch: 126/1182/2, Loss: 1420.5518798828125\n","Train -> sample/numSamples/epoch: 127/1182/2, Loss: 1079.974365234375\n","Train -> sample/numSamples/epoch: 128/1182/2, Loss: 834.7265625\n","Train -> sample/numSamples/epoch: 129/1182/2, Loss: 1174.55126953125\n","Train -> sample/numSamples/epoch: 130/1182/2, Loss: 790.1927490234375\n","Train -> sample/numSamples/epoch: 131/1182/2, Loss: 1321.724609375\n","Train -> sample/numSamples/epoch: 132/1182/2, Loss: 1121.873291015625\n","Train -> sample/numSamples/epoch: 133/1182/2, Loss: 1204.1932373046875\n","Train -> sample/numSamples/epoch: 134/1182/2, Loss: 1069.9266357421875\n","Train -> sample/numSamples/epoch: 135/1182/2, Loss: 914.9724731445312\n","imatge guardada\n","Train -> sample/numSamples/epoch: 136/1182/2, Loss: 914.3773803710938\n","Train -> sample/numSamples/epoch: 137/1182/2, Loss: 1194.33740234375\n","Train -> sample/numSamples/epoch: 138/1182/2, Loss: 1416.244140625\n","Train -> sample/numSamples/epoch: 139/1182/2, Loss: 842.4612426757812\n","Train -> sample/numSamples/epoch: 140/1182/2, Loss: 1093.5992431640625\n","Train -> sample/numSamples/epoch: 141/1182/2, Loss: 1025.4019775390625\n","Train -> sample/numSamples/epoch: 142/1182/2, Loss: 632.1849365234375\n","Train -> sample/numSamples/epoch: 143/1182/2, Loss: 1362.2034912109375\n","Train -> sample/numSamples/epoch: 144/1182/2, Loss: 714.7288208007812\n","Train -> sample/numSamples/epoch: 145/1182/2, Loss: 950.7990112304688\n","Train -> sample/numSamples/epoch: 146/1182/2, Loss: 1359.2301025390625\n","Train -> sample/numSamples/epoch: 147/1182/2, Loss: 967.8032836914062\n","Train -> sample/numSamples/epoch: 148/1182/2, Loss: 686.4439086914062\n","Train -> sample/numSamples/epoch: 149/1182/2, Loss: 1452.1546630859375\n","Train -> sample/numSamples/epoch: 150/1182/2, Loss: 1324.5662841796875\n","Train -> sample/numSamples/epoch: 151/1182/2, Loss: 1048.6600341796875\n","Train -> sample/numSamples/epoch: 152/1182/2, Loss: 647.1396484375\n","Train -> sample/numSamples/epoch: 153/1182/2, Loss: 719.092041015625\n","Train -> sample/numSamples/epoch: 154/1182/2, Loss: 888.3417358398438\n","Train -> sample/numSamples/epoch: 155/1182/2, Loss: 1309.6839599609375\n","Train -> sample/numSamples/epoch: 156/1182/2, Loss: 1149.8760986328125\n","Train -> sample/numSamples/epoch: 157/1182/2, Loss: 847.7398681640625\n","Train -> sample/numSamples/epoch: 158/1182/2, Loss: 1641.4635009765625\n","Train -> sample/numSamples/epoch: 159/1182/2, Loss: 1274.90966796875\n","Train -> sample/numSamples/epoch: 160/1182/2, Loss: 704.133056640625\n","Train -> sample/numSamples/epoch: 161/1182/2, Loss: 831.9274291992188\n","Train -> sample/numSamples/epoch: 162/1182/2, Loss: 1247.4664306640625\n","Train -> sample/numSamples/epoch: 163/1182/2, Loss: 1111.581787109375\n","Train -> sample/numSamples/epoch: 164/1182/2, Loss: 1079.978759765625\n","Train -> sample/numSamples/epoch: 165/1182/2, Loss: 991.7056884765625\n","Train -> sample/numSamples/epoch: 166/1182/2, Loss: 797.9097290039062\n","Train -> sample/numSamples/epoch: 167/1182/2, Loss: 923.8973999023438\n","Train -> sample/numSamples/epoch: 168/1182/2, Loss: 815.8440551757812\n","Train -> sample/numSamples/epoch: 169/1182/2, Loss: 930.6015014648438\n","Train -> sample/numSamples/epoch: 170/1182/2, Loss: 1234.9576416015625\n","Train -> sample/numSamples/epoch: 171/1182/2, Loss: 1029.2786865234375\n","Train -> sample/numSamples/epoch: 172/1182/2, Loss: 1268.3153076171875\n","Train -> sample/numSamples/epoch: 173/1182/2, Loss: 1067.6328125\n","Train -> sample/numSamples/epoch: 174/1182/2, Loss: 765.5921630859375\n","Train -> sample/numSamples/epoch: 175/1182/2, Loss: 1127.6363525390625\n","Train -> sample/numSamples/epoch: 176/1182/2, Loss: 1190.165771484375\n","Train -> sample/numSamples/epoch: 177/1182/2, Loss: 1106.047119140625\n","Train -> sample/numSamples/epoch: 178/1182/2, Loss: 1501.44384765625\n","Train -> sample/numSamples/epoch: 179/1182/2, Loss: 982.9500122070312\n","Train -> sample/numSamples/epoch: 180/1182/2, Loss: 1483.1424560546875\n","Train -> sample/numSamples/epoch: 181/1182/2, Loss: 1037.885009765625\n","Train -> sample/numSamples/epoch: 182/1182/2, Loss: 982.2337036132812\n","Train -> sample/numSamples/epoch: 183/1182/2, Loss: 1012.045166015625\n","Train -> sample/numSamples/epoch: 184/1182/2, Loss: 960.7139282226562\n","Train -> sample/numSamples/epoch: 185/1182/2, Loss: 781.6210327148438\n","Train -> sample/numSamples/epoch: 186/1182/2, Loss: 718.5037231445312\n","Train -> sample/numSamples/epoch: 187/1182/2, Loss: 1033.2208251953125\n","Train -> sample/numSamples/epoch: 188/1182/2, Loss: 894.9527587890625\n","Train -> sample/numSamples/epoch: 189/1182/2, Loss: 1219.30224609375\n","Train -> sample/numSamples/epoch: 190/1182/2, Loss: 1061.83984375\n","Train -> sample/numSamples/epoch: 191/1182/2, Loss: 856.8656616210938\n","Train -> sample/numSamples/epoch: 192/1182/2, Loss: 1369.2723388671875\n","Train -> sample/numSamples/epoch: 193/1182/2, Loss: 1032.0286865234375\n","Train -> sample/numSamples/epoch: 194/1182/2, Loss: 853.1612548828125\n","Train -> sample/numSamples/epoch: 195/1182/2, Loss: 976.0245361328125\n","Train -> sample/numSamples/epoch: 196/1182/2, Loss: 1349.5565185546875\n","Train -> sample/numSamples/epoch: 197/1182/2, Loss: 1350.2738037109375\n","Train -> sample/numSamples/epoch: 198/1182/2, Loss: 1358.1195068359375\n","Train -> sample/numSamples/epoch: 199/1182/2, Loss: 872.2222900390625\n","Train -> sample/numSamples/epoch: 200/1182/2, Loss: 1152.0343017578125\n","Train -> sample/numSamples/epoch: 201/1182/2, Loss: 1048.003173828125\n","Train -> sample/numSamples/epoch: 202/1182/2, Loss: 837.1851806640625\n","Train -> sample/numSamples/epoch: 203/1182/2, Loss: 1127.5120849609375\n","Train -> sample/numSamples/epoch: 204/1182/2, Loss: 887.6334228515625\n","Train -> sample/numSamples/epoch: 205/1182/2, Loss: 695.9930419921875\n","Train -> sample/numSamples/epoch: 206/1182/2, Loss: 916.9559936523438\n","Train -> sample/numSamples/epoch: 207/1182/2, Loss: 1272.9432373046875\n","Train -> sample/numSamples/epoch: 208/1182/2, Loss: 973.0906982421875\n","Train -> sample/numSamples/epoch: 209/1182/2, Loss: 1462.9228515625\n","Train -> sample/numSamples/epoch: 210/1182/2, Loss: 1110.034912109375\n","Train -> sample/numSamples/epoch: 211/1182/2, Loss: 800.52099609375\n","Train -> sample/numSamples/epoch: 212/1182/2, Loss: 1283.6939697265625\n","Train -> sample/numSamples/epoch: 213/1182/2, Loss: 1379.8983154296875\n","Train -> sample/numSamples/epoch: 214/1182/2, Loss: 1201.8321533203125\n","Train -> sample/numSamples/epoch: 215/1182/2, Loss: 1239.3236083984375\n","Train -> sample/numSamples/epoch: 216/1182/2, Loss: 1271.2412109375\n","Train -> sample/numSamples/epoch: 217/1182/2, Loss: 448.95721435546875\n","Train -> sample/numSamples/epoch: 218/1182/2, Loss: 912.1422119140625\n","Train -> sample/numSamples/epoch: 219/1182/2, Loss: 814.3448486328125\n","Train -> sample/numSamples/epoch: 220/1182/2, Loss: 749.0684204101562\n","Train -> sample/numSamples/epoch: 221/1182/2, Loss: 1415.939697265625\n","Train -> sample/numSamples/epoch: 222/1182/2, Loss: 1437.5992431640625\n","Train -> sample/numSamples/epoch: 223/1182/2, Loss: 1119.147216796875\n","Train -> sample/numSamples/epoch: 224/1182/2, Loss: 1209.893310546875\n","Train -> sample/numSamples/epoch: 225/1182/2, Loss: 835.8005981445312\n","Train -> sample/numSamples/epoch: 226/1182/2, Loss: 1075.8079833984375\n","Train -> sample/numSamples/epoch: 227/1182/2, Loss: 1026.7640380859375\n","Train -> sample/numSamples/epoch: 228/1182/2, Loss: 1447.762939453125\n","Train -> sample/numSamples/epoch: 229/1182/2, Loss: 668.227294921875\n","Train -> sample/numSamples/epoch: 230/1182/2, Loss: 957.911376953125\n","Train -> sample/numSamples/epoch: 231/1182/2, Loss: 1133.35107421875\n","Train -> sample/numSamples/epoch: 232/1182/2, Loss: 793.22607421875\n","Train -> sample/numSamples/epoch: 233/1182/2, Loss: 1271.5943603515625\n","Train -> sample/numSamples/epoch: 234/1182/2, Loss: 1159.5721435546875\n","Train -> sample/numSamples/epoch: 235/1182/2, Loss: 1169.15185546875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 236/1182/2, Loss: 1383.964599609375\n","Train -> sample/numSamples/epoch: 237/1182/2, Loss: 1390.615234375\n","Train -> sample/numSamples/epoch: 238/1182/2, Loss: 1162.1893310546875\n","Train -> sample/numSamples/epoch: 239/1182/2, Loss: 1378.5728759765625\n","Train -> sample/numSamples/epoch: 240/1182/2, Loss: 1125.2259521484375\n","Train -> sample/numSamples/epoch: 241/1182/2, Loss: 1195.0518798828125\n","Train -> sample/numSamples/epoch: 242/1182/2, Loss: 811.1011962890625\n","Train -> sample/numSamples/epoch: 243/1182/2, Loss: 1013.0202026367188\n","Train -> sample/numSamples/epoch: 244/1182/2, Loss: 1401.2037353515625\n","Train -> sample/numSamples/epoch: 245/1182/2, Loss: 1122.3912353515625\n","Train -> sample/numSamples/epoch: 246/1182/2, Loss: 1128.7603759765625\n","Train -> sample/numSamples/epoch: 247/1182/2, Loss: 1151.126708984375\n","Train -> sample/numSamples/epoch: 248/1182/2, Loss: 853.9212036132812\n","Train -> sample/numSamples/epoch: 249/1182/2, Loss: 840.8599853515625\n","Train -> sample/numSamples/epoch: 250/1182/2, Loss: 875.8655395507812\n","Train -> sample/numSamples/epoch: 251/1182/2, Loss: 988.859619140625\n","Train -> sample/numSamples/epoch: 252/1182/2, Loss: 803.1384887695312\n","Train -> sample/numSamples/epoch: 253/1182/2, Loss: 971.7291259765625\n","Train -> sample/numSamples/epoch: 254/1182/2, Loss: 1153.5218505859375\n","Train -> sample/numSamples/epoch: 255/1182/2, Loss: 1002.0988159179688\n","Train -> sample/numSamples/epoch: 256/1182/2, Loss: 1232.7327880859375\n","Train -> sample/numSamples/epoch: 257/1182/2, Loss: 1050.2767333984375\n","Train -> sample/numSamples/epoch: 258/1182/2, Loss: 1017.6494750976562\n","Train -> sample/numSamples/epoch: 259/1182/2, Loss: 824.97607421875\n","Train -> sample/numSamples/epoch: 260/1182/2, Loss: 875.1976318359375\n","Train -> sample/numSamples/epoch: 261/1182/2, Loss: 804.575927734375\n","Train -> sample/numSamples/epoch: 262/1182/2, Loss: 1149.0567626953125\n","Train -> sample/numSamples/epoch: 263/1182/2, Loss: 1079.2247314453125\n","Train -> sample/numSamples/epoch: 264/1182/2, Loss: 822.1991577148438\n","Train -> sample/numSamples/epoch: 265/1182/2, Loss: 1018.0507202148438\n","Train -> sample/numSamples/epoch: 266/1182/2, Loss: 1237.3760986328125\n","Train -> sample/numSamples/epoch: 267/1182/2, Loss: 934.7959594726562\n","Train -> sample/numSamples/epoch: 268/1182/2, Loss: 725.8095703125\n","Train -> sample/numSamples/epoch: 269/1182/2, Loss: 1307.95166015625\n","Train -> sample/numSamples/epoch: 270/1182/2, Loss: 1022.402099609375\n","Train -> sample/numSamples/epoch: 271/1182/2, Loss: 797.4011840820312\n","Train -> sample/numSamples/epoch: 272/1182/2, Loss: 757.69189453125\n","Train -> sample/numSamples/epoch: 273/1182/2, Loss: 943.5726318359375\n","Train -> sample/numSamples/epoch: 274/1182/2, Loss: 849.2247314453125\n","Train -> sample/numSamples/epoch: 275/1182/2, Loss: 1168.7923583984375\n","Train -> sample/numSamples/epoch: 276/1182/2, Loss: 1222.5859375\n","Train -> sample/numSamples/epoch: 277/1182/2, Loss: 1551.2327880859375\n","Train -> sample/numSamples/epoch: 278/1182/2, Loss: 956.3275146484375\n","Train -> sample/numSamples/epoch: 279/1182/2, Loss: 878.43994140625\n","Train -> sample/numSamples/epoch: 280/1182/2, Loss: 1036.3560791015625\n","Train -> sample/numSamples/epoch: 281/1182/2, Loss: 1325.910400390625\n","Train -> sample/numSamples/epoch: 282/1182/2, Loss: 946.1739501953125\n","Train -> sample/numSamples/epoch: 283/1182/2, Loss: 1281.695068359375\n","Train -> sample/numSamples/epoch: 284/1182/2, Loss: 533.9570922851562\n","Train -> sample/numSamples/epoch: 285/1182/2, Loss: 932.2821655273438\n","Train -> sample/numSamples/epoch: 286/1182/2, Loss: 573.203369140625\n","Train -> sample/numSamples/epoch: 287/1182/2, Loss: 832.254638671875\n","Train -> sample/numSamples/epoch: 288/1182/2, Loss: 697.2183837890625\n","Train -> sample/numSamples/epoch: 289/1182/2, Loss: 508.48895263671875\n","Train -> sample/numSamples/epoch: 290/1182/2, Loss: 1544.5675048828125\n","Train -> sample/numSamples/epoch: 291/1182/2, Loss: 609.060302734375\n","Train -> sample/numSamples/epoch: 292/1182/2, Loss: 1305.7725830078125\n","Train -> sample/numSamples/epoch: 293/1182/2, Loss: 708.9317016601562\n","Train -> sample/numSamples/epoch: 294/1182/2, Loss: 967.868896484375\n","Train -> sample/numSamples/epoch: 295/1182/2, Loss: 1189.826171875\n","Train -> sample/numSamples/epoch: 296/1182/2, Loss: 1282.7884521484375\n","Train -> sample/numSamples/epoch: 297/1182/2, Loss: 866.041748046875\n","Train -> sample/numSamples/epoch: 298/1182/2, Loss: 1164.7911376953125\n","Train -> sample/numSamples/epoch: 299/1182/2, Loss: 1158.378662109375\n","Train -> sample/numSamples/epoch: 300/1182/2, Loss: 737.0848388671875\n","Train -> sample/numSamples/epoch: 301/1182/2, Loss: 892.533203125\n","Train -> sample/numSamples/epoch: 302/1182/2, Loss: 651.3198852539062\n","Train -> sample/numSamples/epoch: 303/1182/2, Loss: 1613.1876220703125\n","Train -> sample/numSamples/epoch: 304/1182/2, Loss: 1513.2371826171875\n","Train -> sample/numSamples/epoch: 305/1182/2, Loss: 1228.51953125\n","Train -> sample/numSamples/epoch: 306/1182/2, Loss: 865.990234375\n","Train -> sample/numSamples/epoch: 307/1182/2, Loss: 958.0953369140625\n","Train -> sample/numSamples/epoch: 308/1182/2, Loss: 1060.346435546875\n","Train -> sample/numSamples/epoch: 309/1182/2, Loss: 842.2991943359375\n","Train -> sample/numSamples/epoch: 310/1182/2, Loss: 676.7701416015625\n","Train -> sample/numSamples/epoch: 311/1182/2, Loss: 639.0313110351562\n","Train -> sample/numSamples/epoch: 312/1182/2, Loss: 1001.0140991210938\n","Train -> sample/numSamples/epoch: 313/1182/2, Loss: 1059.6585693359375\n","Train -> sample/numSamples/epoch: 314/1182/2, Loss: 859.0166625976562\n","Train -> sample/numSamples/epoch: 315/1182/2, Loss: 765.8142700195312\n","Train -> sample/numSamples/epoch: 316/1182/2, Loss: 606.6127319335938\n","Train -> sample/numSamples/epoch: 317/1182/2, Loss: 1127.8072509765625\n","Train -> sample/numSamples/epoch: 318/1182/2, Loss: 652.9345703125\n","Train -> sample/numSamples/epoch: 319/1182/2, Loss: 734.6734619140625\n","Train -> sample/numSamples/epoch: 320/1182/2, Loss: 847.652099609375\n","Train -> sample/numSamples/epoch: 321/1182/2, Loss: 1357.780517578125\n","Train -> sample/numSamples/epoch: 322/1182/2, Loss: 873.0753173828125\n","Train -> sample/numSamples/epoch: 323/1182/2, Loss: 702.9622192382812\n","Train -> sample/numSamples/epoch: 324/1182/2, Loss: 1493.5614013671875\n","Train -> sample/numSamples/epoch: 325/1182/2, Loss: 1605.193115234375\n","Train -> sample/numSamples/epoch: 326/1182/2, Loss: 904.2705078125\n","Train -> sample/numSamples/epoch: 327/1182/2, Loss: 536.73974609375\n","Train -> sample/numSamples/epoch: 328/1182/2, Loss: 1130.6025390625\n","Train -> sample/numSamples/epoch: 329/1182/2, Loss: 1249.9676513671875\n","Train -> sample/numSamples/epoch: 330/1182/2, Loss: 1634.9541015625\n","Train -> sample/numSamples/epoch: 331/1182/2, Loss: 625.7479248046875\n","Train -> sample/numSamples/epoch: 332/1182/2, Loss: 1164.7044677734375\n","Train -> sample/numSamples/epoch: 333/1182/2, Loss: 1180.2630615234375\n","Train -> sample/numSamples/epoch: 334/1182/2, Loss: 884.5633544921875\n","Train -> sample/numSamples/epoch: 335/1182/2, Loss: 1103.826904296875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 336/1182/2, Loss: 899.0220336914062\n","Train -> sample/numSamples/epoch: 337/1182/2, Loss: 1268.3221435546875\n","Train -> sample/numSamples/epoch: 338/1182/2, Loss: 1251.1064453125\n","Train -> sample/numSamples/epoch: 339/1182/2, Loss: 1420.8350830078125\n","Train -> sample/numSamples/epoch: 340/1182/2, Loss: 1156.7113037109375\n","Train -> sample/numSamples/epoch: 341/1182/2, Loss: 1123.646728515625\n","Train -> sample/numSamples/epoch: 342/1182/2, Loss: 1069.17626953125\n","Train -> sample/numSamples/epoch: 343/1182/2, Loss: 681.1085205078125\n","Train -> sample/numSamples/epoch: 344/1182/2, Loss: 892.3025512695312\n","Train -> sample/numSamples/epoch: 345/1182/2, Loss: 1206.30322265625\n","Train -> sample/numSamples/epoch: 346/1182/2, Loss: 1105.8487548828125\n","Train -> sample/numSamples/epoch: 347/1182/2, Loss: 1457.069091796875\n","Train -> sample/numSamples/epoch: 348/1182/2, Loss: 853.77099609375\n","Train -> sample/numSamples/epoch: 349/1182/2, Loss: 1450.627685546875\n","Train -> sample/numSamples/epoch: 350/1182/2, Loss: 761.77197265625\n","Train -> sample/numSamples/epoch: 351/1182/2, Loss: 1592.3553466796875\n","Train -> sample/numSamples/epoch: 352/1182/2, Loss: 1082.4810791015625\n","Train -> sample/numSamples/epoch: 353/1182/2, Loss: 1065.688232421875\n","Train -> sample/numSamples/epoch: 354/1182/2, Loss: 962.81103515625\n","Train -> sample/numSamples/epoch: 355/1182/2, Loss: 1439.8826904296875\n","Train -> sample/numSamples/epoch: 356/1182/2, Loss: 1272.02880859375\n","Train -> sample/numSamples/epoch: 357/1182/2, Loss: 1058.5162353515625\n","Train -> sample/numSamples/epoch: 358/1182/2, Loss: 855.0227661132812\n","Train -> sample/numSamples/epoch: 359/1182/2, Loss: 1616.303466796875\n","Train -> sample/numSamples/epoch: 360/1182/2, Loss: 1063.9375\n","Train -> sample/numSamples/epoch: 361/1182/2, Loss: 855.9915161132812\n","Train -> sample/numSamples/epoch: 362/1182/2, Loss: 876.1026000976562\n","Train -> sample/numSamples/epoch: 363/1182/2, Loss: 598.0316772460938\n","Train -> sample/numSamples/epoch: 364/1182/2, Loss: 767.1322021484375\n","Train -> sample/numSamples/epoch: 365/1182/2, Loss: 719.0840454101562\n","Train -> sample/numSamples/epoch: 366/1182/2, Loss: 1360.7861328125\n","Train -> sample/numSamples/epoch: 367/1182/2, Loss: 992.16357421875\n","Train -> sample/numSamples/epoch: 368/1182/2, Loss: 807.5806274414062\n","Train -> sample/numSamples/epoch: 369/1182/2, Loss: 960.2901611328125\n","Train -> sample/numSamples/epoch: 370/1182/2, Loss: 420.217041015625\n","Train -> sample/numSamples/epoch: 371/1182/2, Loss: 648.1114501953125\n","Train -> sample/numSamples/epoch: 372/1182/2, Loss: 1001.7908325195312\n","Train -> sample/numSamples/epoch: 373/1182/2, Loss: 616.815673828125\n","Train -> sample/numSamples/epoch: 374/1182/2, Loss: 1084.1234130859375\n","Train -> sample/numSamples/epoch: 375/1182/2, Loss: 1123.1424560546875\n","Train -> sample/numSamples/epoch: 376/1182/2, Loss: 890.7599487304688\n","Train -> sample/numSamples/epoch: 377/1182/2, Loss: 792.3836669921875\n","Train -> sample/numSamples/epoch: 378/1182/2, Loss: 1191.03955078125\n","Train -> sample/numSamples/epoch: 379/1182/2, Loss: 1121.30517578125\n","Train -> sample/numSamples/epoch: 380/1182/2, Loss: 960.5818481445312\n","Train -> sample/numSamples/epoch: 381/1182/2, Loss: 805.5281982421875\n","Train -> sample/numSamples/epoch: 382/1182/2, Loss: 962.758544921875\n","Train -> sample/numSamples/epoch: 383/1182/2, Loss: 921.5936889648438\n","Train -> sample/numSamples/epoch: 384/1182/2, Loss: 1284.2886962890625\n","Train -> sample/numSamples/epoch: 385/1182/2, Loss: 1057.965087890625\n","Train -> sample/numSamples/epoch: 386/1182/2, Loss: 760.4581909179688\n","Train -> sample/numSamples/epoch: 387/1182/2, Loss: 1071.8331298828125\n","Train -> sample/numSamples/epoch: 388/1182/2, Loss: 824.3750610351562\n","Train -> sample/numSamples/epoch: 389/1182/2, Loss: 1223.822998046875\n","Train -> sample/numSamples/epoch: 390/1182/2, Loss: 1099.6673583984375\n","Train -> sample/numSamples/epoch: 391/1182/2, Loss: 855.3145141601562\n","Train -> sample/numSamples/epoch: 392/1182/2, Loss: 692.8948364257812\n","Train -> sample/numSamples/epoch: 393/1182/2, Loss: 733.3196411132812\n","Train -> sample/numSamples/epoch: 394/1182/2, Loss: 1079.473388671875\n","Train -> sample/numSamples/epoch: 395/1182/2, Loss: 815.1784057617188\n","Train -> sample/numSamples/epoch: 396/1182/2, Loss: 1030.2476806640625\n","Train -> sample/numSamples/epoch: 397/1182/2, Loss: 867.2945556640625\n","Train -> sample/numSamples/epoch: 398/1182/2, Loss: 1056.5654296875\n","Train -> sample/numSamples/epoch: 399/1182/2, Loss: 1324.7684326171875\n","Train -> sample/numSamples/epoch: 400/1182/2, Loss: 514.9888916015625\n","Train -> sample/numSamples/epoch: 401/1182/2, Loss: 1151.1812744140625\n","Train -> sample/numSamples/epoch: 402/1182/2, Loss: 844.817138671875\n","Train -> sample/numSamples/epoch: 403/1182/2, Loss: 1025.489013671875\n","Train -> sample/numSamples/epoch: 404/1182/2, Loss: 1086.7376708984375\n","Train -> sample/numSamples/epoch: 405/1182/2, Loss: 1363.5977783203125\n","Train -> sample/numSamples/epoch: 406/1182/2, Loss: 1283.0250244140625\n","Train -> sample/numSamples/epoch: 407/1182/2, Loss: 1046.1455078125\n","Train -> sample/numSamples/epoch: 408/1182/2, Loss: 1410.669677734375\n","Train -> sample/numSamples/epoch: 409/1182/2, Loss: 727.841552734375\n","Train -> sample/numSamples/epoch: 410/1182/2, Loss: 933.6931762695312\n","Train -> sample/numSamples/epoch: 411/1182/2, Loss: 497.7083435058594\n","Train -> sample/numSamples/epoch: 412/1182/2, Loss: 1066.104736328125\n","Train -> sample/numSamples/epoch: 413/1182/2, Loss: 1115.348388671875\n","Train -> sample/numSamples/epoch: 414/1182/2, Loss: 1392.5501708984375\n","Train -> sample/numSamples/epoch: 415/1182/2, Loss: 1051.8271484375\n","Train -> sample/numSamples/epoch: 416/1182/2, Loss: 886.2455444335938\n","Train -> sample/numSamples/epoch: 417/1182/2, Loss: 733.0632934570312\n","Train -> sample/numSamples/epoch: 418/1182/2, Loss: 916.6629028320312\n","Train -> sample/numSamples/epoch: 419/1182/2, Loss: 1244.2728271484375\n","Train -> sample/numSamples/epoch: 420/1182/2, Loss: 772.8014526367188\n","Train -> sample/numSamples/epoch: 421/1182/2, Loss: 720.3497314453125\n","Train -> sample/numSamples/epoch: 422/1182/2, Loss: 1094.3685302734375\n","Train -> sample/numSamples/epoch: 423/1182/2, Loss: 1016.7119750976562\n","Train -> sample/numSamples/epoch: 424/1182/2, Loss: 749.478271484375\n","Train -> sample/numSamples/epoch: 425/1182/2, Loss: 1211.4976806640625\n","Train -> sample/numSamples/epoch: 426/1182/2, Loss: 979.1746826171875\n","Train -> sample/numSamples/epoch: 427/1182/2, Loss: 1055.0789794921875\n","Train -> sample/numSamples/epoch: 428/1182/2, Loss: 447.06634521484375\n","Train -> sample/numSamples/epoch: 429/1182/2, Loss: 1242.0780029296875\n","Train -> sample/numSamples/epoch: 430/1182/2, Loss: 1133.56396484375\n","Train -> sample/numSamples/epoch: 431/1182/2, Loss: 1008.2090454101562\n","Train -> sample/numSamples/epoch: 432/1182/2, Loss: 1155.5543212890625\n","Train -> sample/numSamples/epoch: 433/1182/2, Loss: 1154.6517333984375\n","Train -> sample/numSamples/epoch: 434/1182/2, Loss: 1176.280517578125\n","Train -> sample/numSamples/epoch: 435/1182/2, Loss: 899.216796875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 436/1182/2, Loss: 950.964599609375\n","Train -> sample/numSamples/epoch: 437/1182/2, Loss: 923.0250854492188\n","Train -> sample/numSamples/epoch: 438/1182/2, Loss: 1206.368896484375\n","Train -> sample/numSamples/epoch: 439/1182/2, Loss: 961.5296020507812\n","Train -> sample/numSamples/epoch: 440/1182/2, Loss: 627.3720703125\n","Train -> sample/numSamples/epoch: 441/1182/2, Loss: 1283.0074462890625\n","Train -> sample/numSamples/epoch: 442/1182/2, Loss: 693.930419921875\n","Train -> sample/numSamples/epoch: 443/1182/2, Loss: 792.1279907226562\n","Train -> sample/numSamples/epoch: 444/1182/2, Loss: 879.728515625\n","Train -> sample/numSamples/epoch: 445/1182/2, Loss: 1429.9659423828125\n","Train -> sample/numSamples/epoch: 446/1182/2, Loss: 1105.940185546875\n","Train -> sample/numSamples/epoch: 447/1182/2, Loss: 685.2750854492188\n","Train -> sample/numSamples/epoch: 448/1182/2, Loss: 973.2345581054688\n","Train -> sample/numSamples/epoch: 449/1182/2, Loss: 1074.4654541015625\n","Train -> sample/numSamples/epoch: 450/1182/2, Loss: 1064.27685546875\n","Train -> sample/numSamples/epoch: 451/1182/2, Loss: 1212.8719482421875\n","Train -> sample/numSamples/epoch: 452/1182/2, Loss: 1263.6119384765625\n","Train -> sample/numSamples/epoch: 453/1182/2, Loss: 958.4724731445312\n","Train -> sample/numSamples/epoch: 454/1182/2, Loss: 1449.1541748046875\n","Train -> sample/numSamples/epoch: 455/1182/2, Loss: 594.2386474609375\n","Train -> sample/numSamples/epoch: 456/1182/2, Loss: 746.0521850585938\n","Train -> sample/numSamples/epoch: 457/1182/2, Loss: 1119.9903564453125\n","Train -> sample/numSamples/epoch: 458/1182/2, Loss: 1352.31298828125\n","Train -> sample/numSamples/epoch: 459/1182/2, Loss: 704.7341918945312\n","Train -> sample/numSamples/epoch: 460/1182/2, Loss: 820.6773071289062\n","Train -> sample/numSamples/epoch: 461/1182/2, Loss: 1361.931884765625\n","Train -> sample/numSamples/epoch: 462/1182/2, Loss: 751.3587036132812\n","Train -> sample/numSamples/epoch: 463/1182/2, Loss: 808.1768798828125\n","Train -> sample/numSamples/epoch: 464/1182/2, Loss: 1070.43896484375\n","Train -> sample/numSamples/epoch: 465/1182/2, Loss: 975.7791137695312\n","Train -> sample/numSamples/epoch: 466/1182/2, Loss: 905.8505859375\n","Train -> sample/numSamples/epoch: 467/1182/2, Loss: 636.5675659179688\n","Train -> sample/numSamples/epoch: 468/1182/2, Loss: 1142.2198486328125\n","Train -> sample/numSamples/epoch: 469/1182/2, Loss: 1141.5958251953125\n","Train -> sample/numSamples/epoch: 470/1182/2, Loss: 818.8370361328125\n","Train -> sample/numSamples/epoch: 471/1182/2, Loss: 724.55224609375\n","Train -> sample/numSamples/epoch: 472/1182/2, Loss: 782.3416137695312\n","Train -> sample/numSamples/epoch: 473/1182/2, Loss: 1512.2938232421875\n","Train -> sample/numSamples/epoch: 474/1182/2, Loss: 913.8040771484375\n","Train -> sample/numSamples/epoch: 475/1182/2, Loss: 1013.112548828125\n","Train -> sample/numSamples/epoch: 476/1182/2, Loss: 976.2902221679688\n","Train -> sample/numSamples/epoch: 477/1182/2, Loss: 1002.56591796875\n","Train -> sample/numSamples/epoch: 478/1182/2, Loss: 623.913330078125\n","Train -> sample/numSamples/epoch: 479/1182/2, Loss: 1192.8792724609375\n","Train -> sample/numSamples/epoch: 480/1182/2, Loss: 1190.437744140625\n","Train -> sample/numSamples/epoch: 481/1182/2, Loss: 1189.8184814453125\n","Train -> sample/numSamples/epoch: 482/1182/2, Loss: 1344.9747314453125\n","Train -> sample/numSamples/epoch: 483/1182/2, Loss: 927.1427001953125\n","Train -> sample/numSamples/epoch: 484/1182/2, Loss: 1044.220703125\n","Train -> sample/numSamples/epoch: 485/1182/2, Loss: 1437.319580078125\n","Train -> sample/numSamples/epoch: 486/1182/2, Loss: 1100.125732421875\n","Train -> sample/numSamples/epoch: 487/1182/2, Loss: 985.1818237304688\n","Train -> sample/numSamples/epoch: 488/1182/2, Loss: 1036.71484375\n","Train -> sample/numSamples/epoch: 489/1182/2, Loss: 742.502197265625\n","Train -> sample/numSamples/epoch: 490/1182/2, Loss: 1162.16943359375\n","Train -> sample/numSamples/epoch: 491/1182/2, Loss: 955.1455688476562\n","Train -> sample/numSamples/epoch: 492/1182/2, Loss: 699.9132690429688\n","Train -> sample/numSamples/epoch: 493/1182/2, Loss: 688.7103881835938\n","Train -> sample/numSamples/epoch: 494/1182/2, Loss: 1057.045654296875\n","Train -> sample/numSamples/epoch: 495/1182/2, Loss: 839.9371337890625\n","Train -> sample/numSamples/epoch: 496/1182/2, Loss: 1176.1632080078125\n","Train -> sample/numSamples/epoch: 497/1182/2, Loss: 965.0753784179688\n","Train -> sample/numSamples/epoch: 498/1182/2, Loss: 1064.2894287109375\n","Train -> sample/numSamples/epoch: 499/1182/2, Loss: 1127.8143310546875\n","Train -> sample/numSamples/epoch: 500/1182/2, Loss: 881.0910034179688\n","Train -> sample/numSamples/epoch: 501/1182/2, Loss: 1075.5830078125\n","Train -> sample/numSamples/epoch: 502/1182/2, Loss: 973.2802734375\n","Train -> sample/numSamples/epoch: 503/1182/2, Loss: 1488.3375244140625\n","Train -> sample/numSamples/epoch: 504/1182/2, Loss: 992.0997314453125\n","Train -> sample/numSamples/epoch: 505/1182/2, Loss: 1016.271484375\n","Train -> sample/numSamples/epoch: 506/1182/2, Loss: 1006.9601440429688\n","Train -> sample/numSamples/epoch: 507/1182/2, Loss: 1339.6304931640625\n","Train -> sample/numSamples/epoch: 508/1182/2, Loss: 1342.370361328125\n","Train -> sample/numSamples/epoch: 509/1182/2, Loss: 1204.3182373046875\n","Train -> sample/numSamples/epoch: 510/1182/2, Loss: 1200.5238037109375\n","Train -> sample/numSamples/epoch: 511/1182/2, Loss: 1065.928955078125\n","Train -> sample/numSamples/epoch: 512/1182/2, Loss: 1740.4842529296875\n","Train -> sample/numSamples/epoch: 513/1182/2, Loss: 977.1474609375\n","Train -> sample/numSamples/epoch: 514/1182/2, Loss: 1292.5936279296875\n","Train -> sample/numSamples/epoch: 515/1182/2, Loss: 1138.2257080078125\n","Train -> sample/numSamples/epoch: 516/1182/2, Loss: 943.010986328125\n","Train -> sample/numSamples/epoch: 517/1182/2, Loss: 1090.9080810546875\n","Train -> sample/numSamples/epoch: 518/1182/2, Loss: 1168.5087890625\n","Train -> sample/numSamples/epoch: 519/1182/2, Loss: 1426.1876220703125\n","Train -> sample/numSamples/epoch: 520/1182/2, Loss: 777.2088623046875\n","Train -> sample/numSamples/epoch: 521/1182/2, Loss: 1165.9307861328125\n","Train -> sample/numSamples/epoch: 522/1182/2, Loss: 776.2432250976562\n","Train -> sample/numSamples/epoch: 523/1182/2, Loss: 720.1529541015625\n","Train -> sample/numSamples/epoch: 524/1182/2, Loss: 765.5286865234375\n","Train -> sample/numSamples/epoch: 525/1182/2, Loss: 979.7943725585938\n","Train -> sample/numSamples/epoch: 526/1182/2, Loss: 1245.2950439453125\n","Train -> sample/numSamples/epoch: 527/1182/2, Loss: 1412.0404052734375\n","Train -> sample/numSamples/epoch: 528/1182/2, Loss: 1052.0330810546875\n","Train -> sample/numSamples/epoch: 529/1182/2, Loss: 1416.2628173828125\n","Train -> sample/numSamples/epoch: 530/1182/2, Loss: 1122.6917724609375\n","Train -> sample/numSamples/epoch: 531/1182/2, Loss: 998.207763671875\n","Train -> sample/numSamples/epoch: 532/1182/2, Loss: 930.0613403320312\n","Train -> sample/numSamples/epoch: 533/1182/2, Loss: 982.3866577148438\n","Train -> sample/numSamples/epoch: 534/1182/2, Loss: 947.3978881835938\n","Train -> sample/numSamples/epoch: 535/1182/2, Loss: 1031.2537841796875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 536/1182/2, Loss: 867.8536987304688\n","Train -> sample/numSamples/epoch: 537/1182/2, Loss: 1323.43505859375\n","Train -> sample/numSamples/epoch: 538/1182/2, Loss: 1159.808349609375\n","Train -> sample/numSamples/epoch: 539/1182/2, Loss: 1214.946533203125\n","Train -> sample/numSamples/epoch: 540/1182/2, Loss: 732.1784057617188\n","Train -> sample/numSamples/epoch: 541/1182/2, Loss: 1568.415283203125\n","Train -> sample/numSamples/epoch: 542/1182/2, Loss: 986.4894409179688\n","Train -> sample/numSamples/epoch: 543/1182/2, Loss: 1440.022216796875\n","Train -> sample/numSamples/epoch: 544/1182/2, Loss: 928.7448120117188\n","Train -> sample/numSamples/epoch: 545/1182/2, Loss: 1346.093994140625\n","Train -> sample/numSamples/epoch: 546/1182/2, Loss: 830.5275268554688\n","Train -> sample/numSamples/epoch: 547/1182/2, Loss: 1172.2928466796875\n","Train -> sample/numSamples/epoch: 548/1182/2, Loss: 613.1109619140625\n","Train -> sample/numSamples/epoch: 549/1182/2, Loss: 689.1844482421875\n","Train -> sample/numSamples/epoch: 550/1182/2, Loss: 1088.7691650390625\n","Train -> sample/numSamples/epoch: 551/1182/2, Loss: 1089.987548828125\n","Train -> sample/numSamples/epoch: 552/1182/2, Loss: 736.2945556640625\n","Train -> sample/numSamples/epoch: 553/1182/2, Loss: 707.3759155273438\n","Train -> sample/numSamples/epoch: 554/1182/2, Loss: 1108.6160888671875\n","Train -> sample/numSamples/epoch: 555/1182/2, Loss: 1086.5155029296875\n","Train -> sample/numSamples/epoch: 556/1182/2, Loss: 1602.9635009765625\n","Train -> sample/numSamples/epoch: 557/1182/2, Loss: 1003.8505859375\n","Train -> sample/numSamples/epoch: 558/1182/2, Loss: 592.7301025390625\n","Train -> sample/numSamples/epoch: 559/1182/2, Loss: 1275.2445068359375\n","Train -> sample/numSamples/epoch: 560/1182/2, Loss: 962.9855346679688\n","Train -> sample/numSamples/epoch: 561/1182/2, Loss: 985.9094848632812\n","Train -> sample/numSamples/epoch: 562/1182/2, Loss: 589.7311401367188\n","Train -> sample/numSamples/epoch: 563/1182/2, Loss: 1060.7520751953125\n","Train -> sample/numSamples/epoch: 564/1182/2, Loss: 833.2333984375\n","Train -> sample/numSamples/epoch: 565/1182/2, Loss: 904.4186401367188\n","Train -> sample/numSamples/epoch: 566/1182/2, Loss: 853.4896850585938\n","Train -> sample/numSamples/epoch: 567/1182/2, Loss: 1101.262451171875\n","Train -> sample/numSamples/epoch: 568/1182/2, Loss: 1162.973876953125\n","Train -> sample/numSamples/epoch: 569/1182/2, Loss: 1510.1868896484375\n","Train -> sample/numSamples/epoch: 570/1182/2, Loss: 909.7229614257812\n","Train -> sample/numSamples/epoch: 571/1182/2, Loss: 915.7088623046875\n","Train -> sample/numSamples/epoch: 572/1182/2, Loss: 1118.95361328125\n","Train -> sample/numSamples/epoch: 573/1182/2, Loss: 902.2339477539062\n","Train -> sample/numSamples/epoch: 574/1182/2, Loss: 572.4365844726562\n","Train -> sample/numSamples/epoch: 575/1182/2, Loss: 1532.2501220703125\n","Train -> sample/numSamples/epoch: 576/1182/2, Loss: 968.3953247070312\n","Train -> sample/numSamples/epoch: 577/1182/2, Loss: 1033.54638671875\n","Train -> sample/numSamples/epoch: 578/1182/2, Loss: 900.820556640625\n","Train -> sample/numSamples/epoch: 579/1182/2, Loss: 1415.500244140625\n","Train -> sample/numSamples/epoch: 580/1182/2, Loss: 763.1021118164062\n","Train -> sample/numSamples/epoch: 581/1182/2, Loss: 1041.1849365234375\n","Train -> sample/numSamples/epoch: 582/1182/2, Loss: 1415.2379150390625\n","Train -> sample/numSamples/epoch: 583/1182/2, Loss: 1016.03466796875\n","Train -> sample/numSamples/epoch: 584/1182/2, Loss: 902.0278930664062\n","Train -> sample/numSamples/epoch: 585/1182/2, Loss: 1341.2781982421875\n","Train -> sample/numSamples/epoch: 586/1182/2, Loss: 1081.635986328125\n","Train -> sample/numSamples/epoch: 587/1182/2, Loss: 872.0636596679688\n","Train -> sample/numSamples/epoch: 588/1182/2, Loss: 1736.677001953125\n","Train -> sample/numSamples/epoch: 589/1182/2, Loss: 1472.5157470703125\n","Train -> sample/numSamples/epoch: 590/1182/2, Loss: 785.7448120117188\n","Train -> sample/numSamples/epoch: 591/1182/2, Loss: 832.7553100585938\n","Train -> sample/numSamples/epoch: 592/1182/2, Loss: 732.1947021484375\n","Train -> sample/numSamples/epoch: 593/1182/2, Loss: 1068.281982421875\n","Train -> sample/numSamples/epoch: 594/1182/2, Loss: 1017.75732421875\n","Train -> sample/numSamples/epoch: 595/1182/2, Loss: 992.8040161132812\n","Train -> sample/numSamples/epoch: 596/1182/2, Loss: 1052.2486572265625\n","Train -> sample/numSamples/epoch: 597/1182/2, Loss: 1095.4599609375\n","Train -> sample/numSamples/epoch: 598/1182/2, Loss: 853.8311767578125\n","Train -> sample/numSamples/epoch: 599/1182/2, Loss: 1102.914306640625\n","Train -> sample/numSamples/epoch: 600/1182/2, Loss: 1212.12890625\n","Train -> sample/numSamples/epoch: 601/1182/2, Loss: 815.3631591796875\n","Train -> sample/numSamples/epoch: 602/1182/2, Loss: 1040.4019775390625\n","Train -> sample/numSamples/epoch: 603/1182/2, Loss: 1190.9715576171875\n","Train -> sample/numSamples/epoch: 604/1182/2, Loss: 1080.9915771484375\n","Train -> sample/numSamples/epoch: 605/1182/2, Loss: 1030.572998046875\n","Train -> sample/numSamples/epoch: 606/1182/2, Loss: 1185.2818603515625\n","Train -> sample/numSamples/epoch: 607/1182/2, Loss: 1249.337646484375\n","Train -> sample/numSamples/epoch: 608/1182/2, Loss: 1105.7528076171875\n","Train -> sample/numSamples/epoch: 609/1182/2, Loss: 1205.9237060546875\n","Train -> sample/numSamples/epoch: 610/1182/2, Loss: 1345.5118408203125\n","Train -> sample/numSamples/epoch: 611/1182/2, Loss: 938.863525390625\n","Train -> sample/numSamples/epoch: 612/1182/2, Loss: 933.1466674804688\n","Train -> sample/numSamples/epoch: 613/1182/2, Loss: 827.9614868164062\n","Train -> sample/numSamples/epoch: 614/1182/2, Loss: 1185.662841796875\n","Train -> sample/numSamples/epoch: 615/1182/2, Loss: 863.9522094726562\n","Train -> sample/numSamples/epoch: 616/1182/2, Loss: 712.5078735351562\n","Train -> sample/numSamples/epoch: 617/1182/2, Loss: 1345.489013671875\n","Train -> sample/numSamples/epoch: 618/1182/2, Loss: 924.5477905273438\n","Train -> sample/numSamples/epoch: 619/1182/2, Loss: 625.2945556640625\n","Train -> sample/numSamples/epoch: 620/1182/2, Loss: 645.063720703125\n","Train -> sample/numSamples/epoch: 621/1182/2, Loss: 1034.93359375\n","Train -> sample/numSamples/epoch: 622/1182/2, Loss: 1039.2076416015625\n","Train -> sample/numSamples/epoch: 623/1182/2, Loss: 857.5034790039062\n","Train -> sample/numSamples/epoch: 624/1182/2, Loss: 884.6555786132812\n","Train -> sample/numSamples/epoch: 625/1182/2, Loss: 787.3004150390625\n","Train -> sample/numSamples/epoch: 626/1182/2, Loss: 855.423583984375\n","Train -> sample/numSamples/epoch: 627/1182/2, Loss: 828.619384765625\n","Train -> sample/numSamples/epoch: 628/1182/2, Loss: 1019.2366333007812\n","Train -> sample/numSamples/epoch: 629/1182/2, Loss: 1239.8343505859375\n","Train -> sample/numSamples/epoch: 630/1182/2, Loss: 1329.4298095703125\n","Train -> sample/numSamples/epoch: 631/1182/2, Loss: 1291.6876220703125\n","Train -> sample/numSamples/epoch: 632/1182/2, Loss: 1250.376953125\n","Train -> sample/numSamples/epoch: 633/1182/2, Loss: 1414.0423583984375\n","Train -> sample/numSamples/epoch: 634/1182/2, Loss: 991.2698974609375\n","Train -> sample/numSamples/epoch: 635/1182/2, Loss: 997.1273193359375\n","imatge guardada\n","Train -> sample/numSamples/epoch: 636/1182/2, Loss: 946.8467407226562\n","Train -> sample/numSamples/epoch: 637/1182/2, Loss: 1330.396484375\n","Train -> sample/numSamples/epoch: 638/1182/2, Loss: 764.176025390625\n","Train -> sample/numSamples/epoch: 639/1182/2, Loss: 1125.0184326171875\n","Train -> sample/numSamples/epoch: 640/1182/2, Loss: 893.064208984375\n","Train -> sample/numSamples/epoch: 641/1182/2, Loss: 627.4972534179688\n","Train -> sample/numSamples/epoch: 642/1182/2, Loss: 898.4520263671875\n","Train -> sample/numSamples/epoch: 643/1182/2, Loss: 618.3557739257812\n","Train -> sample/numSamples/epoch: 644/1182/2, Loss: 999.1444091796875\n","Train -> sample/numSamples/epoch: 645/1182/2, Loss: 838.0421142578125\n","Train -> sample/numSamples/epoch: 646/1182/2, Loss: 1396.285400390625\n","Train -> sample/numSamples/epoch: 647/1182/2, Loss: 862.3538818359375\n","Train -> sample/numSamples/epoch: 648/1182/2, Loss: 1401.4893798828125\n","Train -> sample/numSamples/epoch: 649/1182/2, Loss: 714.4953002929688\n","Train -> sample/numSamples/epoch: 650/1182/2, Loss: 931.0438842773438\n","Train -> sample/numSamples/epoch: 651/1182/2, Loss: 1047.792236328125\n","Train -> sample/numSamples/epoch: 652/1182/2, Loss: 1356.94189453125\n","Train -> sample/numSamples/epoch: 653/1182/2, Loss: 839.4124145507812\n","Train -> sample/numSamples/epoch: 654/1182/2, Loss: 1188.651611328125\n","Train -> sample/numSamples/epoch: 655/1182/2, Loss: 907.4789428710938\n","Train -> sample/numSamples/epoch: 656/1182/2, Loss: 859.4619750976562\n","Train -> sample/numSamples/epoch: 657/1182/2, Loss: 995.8778076171875\n","Train -> sample/numSamples/epoch: 658/1182/2, Loss: 900.0466918945312\n","Train -> sample/numSamples/epoch: 659/1182/2, Loss: 1223.4007568359375\n","Train -> sample/numSamples/epoch: 660/1182/2, Loss: 1241.139892578125\n","Train -> sample/numSamples/epoch: 661/1182/2, Loss: 1130.5364990234375\n","Train -> sample/numSamples/epoch: 662/1182/2, Loss: 798.347900390625\n","Train -> sample/numSamples/epoch: 663/1182/2, Loss: 1407.71240234375\n","Train -> sample/numSamples/epoch: 664/1182/2, Loss: 840.5402221679688\n","Train -> sample/numSamples/epoch: 665/1182/2, Loss: 1074.221923828125\n","Train -> sample/numSamples/epoch: 666/1182/2, Loss: 960.7332153320312\n","Train -> sample/numSamples/epoch: 667/1182/2, Loss: 872.8052978515625\n","Train -> sample/numSamples/epoch: 668/1182/2, Loss: 1161.5989990234375\n","Train -> sample/numSamples/epoch: 669/1182/2, Loss: 643.2500610351562\n","Train -> sample/numSamples/epoch: 670/1182/2, Loss: 862.5635986328125\n","Train -> sample/numSamples/epoch: 671/1182/2, Loss: 1112.09033203125\n","Train -> sample/numSamples/epoch: 672/1182/2, Loss: 791.4814453125\n","Train -> sample/numSamples/epoch: 673/1182/2, Loss: 1188.923828125\n","Train -> sample/numSamples/epoch: 674/1182/2, Loss: 1148.413330078125\n","Train -> sample/numSamples/epoch: 675/1182/2, Loss: 1126.392578125\n","Train -> sample/numSamples/epoch: 676/1182/2, Loss: 691.8578491210938\n","Train -> sample/numSamples/epoch: 677/1182/2, Loss: 983.6321411132812\n","Train -> sample/numSamples/epoch: 678/1182/2, Loss: 1483.9642333984375\n","Train -> sample/numSamples/epoch: 679/1182/2, Loss: 998.4179077148438\n","Train -> sample/numSamples/epoch: 680/1182/2, Loss: 1114.874267578125\n","Train -> sample/numSamples/epoch: 681/1182/2, Loss: 737.095947265625\n","Train -> sample/numSamples/epoch: 682/1182/2, Loss: 1147.8453369140625\n","Train -> sample/numSamples/epoch: 683/1182/2, Loss: 1462.1197509765625\n","Train -> sample/numSamples/epoch: 684/1182/2, Loss: 1251.103759765625\n","Train -> sample/numSamples/epoch: 685/1182/2, Loss: 937.2640380859375\n","Train -> sample/numSamples/epoch: 686/1182/2, Loss: 1176.5589599609375\n","Train -> sample/numSamples/epoch: 687/1182/2, Loss: 771.0075073242188\n","Train -> sample/numSamples/epoch: 688/1182/2, Loss: 933.627685546875\n","Train -> sample/numSamples/epoch: 689/1182/2, Loss: 1145.6575927734375\n","Train -> sample/numSamples/epoch: 690/1182/2, Loss: 690.9320678710938\n","Train -> sample/numSamples/epoch: 691/1182/2, Loss: 1242.4478759765625\n","Train -> sample/numSamples/epoch: 692/1182/2, Loss: 1167.5980224609375\n","Train -> sample/numSamples/epoch: 693/1182/2, Loss: 819.824462890625\n","Train -> sample/numSamples/epoch: 694/1182/2, Loss: 1512.1246337890625\n","Train -> sample/numSamples/epoch: 695/1182/2, Loss: 574.0313720703125\n","Train -> sample/numSamples/epoch: 696/1182/2, Loss: 781.82763671875\n","Train -> sample/numSamples/epoch: 697/1182/2, Loss: 1104.9415283203125\n","Train -> sample/numSamples/epoch: 698/1182/2, Loss: 812.281494140625\n","Train -> sample/numSamples/epoch: 699/1182/2, Loss: 1098.509765625\n","Train -> sample/numSamples/epoch: 700/1182/2, Loss: 1310.788818359375\n","Train -> sample/numSamples/epoch: 701/1182/2, Loss: 1019.8307495117188\n","Train -> sample/numSamples/epoch: 702/1182/2, Loss: 1207.9146728515625\n","Train -> sample/numSamples/epoch: 703/1182/2, Loss: 906.883056640625\n","Train -> sample/numSamples/epoch: 704/1182/2, Loss: 1287.983154296875\n","Train -> sample/numSamples/epoch: 705/1182/2, Loss: 1453.1094970703125\n","Train -> sample/numSamples/epoch: 706/1182/2, Loss: 1112.129638671875\n","Train -> sample/numSamples/epoch: 707/1182/2, Loss: 717.2393798828125\n","Train -> sample/numSamples/epoch: 708/1182/2, Loss: 1050.9669189453125\n","Train -> sample/numSamples/epoch: 709/1182/2, Loss: 1338.8206787109375\n","Train -> sample/numSamples/epoch: 710/1182/2, Loss: 421.95654296875\n","Train -> sample/numSamples/epoch: 711/1182/2, Loss: 1093.3424072265625\n","Train -> sample/numSamples/epoch: 712/1182/2, Loss: 827.085693359375\n","Train -> sample/numSamples/epoch: 713/1182/2, Loss: 1249.5574951171875\n","Train -> sample/numSamples/epoch: 714/1182/2, Loss: 1269.3765869140625\n","Train -> sample/numSamples/epoch: 715/1182/2, Loss: 1223.5360107421875\n","Train -> sample/numSamples/epoch: 716/1182/2, Loss: 940.1802368164062\n","Train -> sample/numSamples/epoch: 717/1182/2, Loss: 1090.976318359375\n","Train -> sample/numSamples/epoch: 718/1182/2, Loss: 1412.0233154296875\n","Train -> sample/numSamples/epoch: 719/1182/2, Loss: 1181.0235595703125\n","Train -> sample/numSamples/epoch: 720/1182/2, Loss: 715.0025634765625\n","Train -> sample/numSamples/epoch: 721/1182/2, Loss: 1270.571533203125\n","Train -> sample/numSamples/epoch: 722/1182/2, Loss: 1126.1484375\n","Train -> sample/numSamples/epoch: 723/1182/2, Loss: 1264.4454345703125\n","Train -> sample/numSamples/epoch: 724/1182/2, Loss: 1123.0052490234375\n","Train -> sample/numSamples/epoch: 725/1182/2, Loss: 495.6087341308594\n","Train -> sample/numSamples/epoch: 726/1182/2, Loss: 1036.15673828125\n","Train -> sample/numSamples/epoch: 727/1182/2, Loss: 1673.77001953125\n","Train -> sample/numSamples/epoch: 728/1182/2, Loss: 1018.1019897460938\n","Train -> sample/numSamples/epoch: 729/1182/2, Loss: 1065.962646484375\n","Train -> sample/numSamples/epoch: 730/1182/2, Loss: 1235.2061767578125\n","Train -> sample/numSamples/epoch: 731/1182/2, Loss: 1141.678955078125\n","Train -> sample/numSamples/epoch: 732/1182/2, Loss: 841.9798583984375\n","Train -> sample/numSamples/epoch: 733/1182/2, Loss: 1192.586669921875\n","Train -> sample/numSamples/epoch: 734/1182/2, Loss: 669.4401245117188\n","Train -> sample/numSamples/epoch: 735/1182/2, Loss: 493.3788146972656\n","imatge guardada\n","Train -> sample/numSamples/epoch: 736/1182/2, Loss: 860.9678955078125\n","Train -> sample/numSamples/epoch: 737/1182/2, Loss: 727.8621215820312\n","Train -> sample/numSamples/epoch: 738/1182/2, Loss: 1175.4337158203125\n","Train -> sample/numSamples/epoch: 739/1182/2, Loss: 811.5363159179688\n","Train -> sample/numSamples/epoch: 740/1182/2, Loss: 873.3776245117188\n","Train -> sample/numSamples/epoch: 741/1182/2, Loss: 1151.81884765625\n","Train -> sample/numSamples/epoch: 742/1182/2, Loss: 939.4374389648438\n","Train -> sample/numSamples/epoch: 743/1182/2, Loss: 734.1395263671875\n","Train -> sample/numSamples/epoch: 744/1182/2, Loss: 1166.267578125\n","Train -> sample/numSamples/epoch: 745/1182/2, Loss: 943.4732055664062\n","Train -> sample/numSamples/epoch: 746/1182/2, Loss: 776.7593994140625\n","Train -> sample/numSamples/epoch: 747/1182/2, Loss: 1258.52587890625\n","Train -> sample/numSamples/epoch: 748/1182/2, Loss: 1145.6043701171875\n","Train -> sample/numSamples/epoch: 749/1182/2, Loss: 1200.369384765625\n","Train -> sample/numSamples/epoch: 750/1182/2, Loss: 1155.232177734375\n","Train -> sample/numSamples/epoch: 751/1182/2, Loss: 1047.870361328125\n","Train -> sample/numSamples/epoch: 752/1182/2, Loss: 894.990966796875\n","Train -> sample/numSamples/epoch: 753/1182/2, Loss: 928.9224243164062\n","Train -> sample/numSamples/epoch: 754/1182/2, Loss: 1098.8416748046875\n","Train -> sample/numSamples/epoch: 755/1182/2, Loss: 1059.626708984375\n","Train -> sample/numSamples/epoch: 756/1182/2, Loss: 1114.681884765625\n","Train -> sample/numSamples/epoch: 757/1182/2, Loss: 1139.9034423828125\n","Train -> sample/numSamples/epoch: 758/1182/2, Loss: 1078.916015625\n","Train -> sample/numSamples/epoch: 759/1182/2, Loss: 967.6486206054688\n","Train -> sample/numSamples/epoch: 760/1182/2, Loss: 911.750732421875\n","Train -> sample/numSamples/epoch: 761/1182/2, Loss: 1150.0352783203125\n","Train -> sample/numSamples/epoch: 762/1182/2, Loss: 679.9185180664062\n","Train -> sample/numSamples/epoch: 763/1182/2, Loss: 1260.7779541015625\n","Train -> sample/numSamples/epoch: 764/1182/2, Loss: 934.0936279296875\n","Train -> sample/numSamples/epoch: 765/1182/2, Loss: 652.6484985351562\n","Train -> sample/numSamples/epoch: 766/1182/2, Loss: 1103.784423828125\n","Train -> sample/numSamples/epoch: 767/1182/2, Loss: 1527.4561767578125\n","Train -> sample/numSamples/epoch: 768/1182/2, Loss: 957.1819458007812\n","Train -> sample/numSamples/epoch: 769/1182/2, Loss: 993.3470458984375\n","Train -> sample/numSamples/epoch: 770/1182/2, Loss: 429.8419189453125\n","Train -> sample/numSamples/epoch: 771/1182/2, Loss: 847.0452270507812\n","Train -> sample/numSamples/epoch: 772/1182/2, Loss: 1401.926513671875\n","Train -> sample/numSamples/epoch: 773/1182/2, Loss: 1152.2261962890625\n","Train -> sample/numSamples/epoch: 774/1182/2, Loss: 698.6358032226562\n","Train -> sample/numSamples/epoch: 775/1182/2, Loss: 1087.293701171875\n","Train -> sample/numSamples/epoch: 776/1182/2, Loss: 1426.1903076171875\n","Train -> sample/numSamples/epoch: 777/1182/2, Loss: 1216.8006591796875\n","Train -> sample/numSamples/epoch: 778/1182/2, Loss: 1077.7362060546875\n","Train -> sample/numSamples/epoch: 779/1182/2, Loss: 934.359130859375\n","Train -> sample/numSamples/epoch: 780/1182/2, Loss: 901.7758178710938\n","Train -> sample/numSamples/epoch: 781/1182/2, Loss: 1139.2298583984375\n","Train -> sample/numSamples/epoch: 782/1182/2, Loss: 1181.1917724609375\n","Train -> sample/numSamples/epoch: 783/1182/2, Loss: 804.839599609375\n","Train -> sample/numSamples/epoch: 784/1182/2, Loss: 961.9177856445312\n","Train -> sample/numSamples/epoch: 785/1182/2, Loss: 979.4407348632812\n","Train -> sample/numSamples/epoch: 786/1182/2, Loss: 1161.45263671875\n","Train -> sample/numSamples/epoch: 787/1182/2, Loss: 1011.5872192382812\n","Train -> sample/numSamples/epoch: 788/1182/2, Loss: 1361.6324462890625\n","Train -> sample/numSamples/epoch: 789/1182/2, Loss: 692.3571166992188\n","Train -> sample/numSamples/epoch: 790/1182/2, Loss: 1120.7237548828125\n","Train -> sample/numSamples/epoch: 791/1182/2, Loss: 881.4794311523438\n","Train -> sample/numSamples/epoch: 792/1182/2, Loss: 1087.235595703125\n","Train -> sample/numSamples/epoch: 793/1182/2, Loss: 1402.953125\n","Train -> sample/numSamples/epoch: 794/1182/2, Loss: 995.2992553710938\n","Train -> sample/numSamples/epoch: 795/1182/2, Loss: 952.4843139648438\n","Train -> sample/numSamples/epoch: 796/1182/2, Loss: 1291.8504638671875\n","Train -> sample/numSamples/epoch: 797/1182/2, Loss: 1179.5936279296875\n","Train -> sample/numSamples/epoch: 798/1182/2, Loss: 720.2899169921875\n","Train -> sample/numSamples/epoch: 799/1182/2, Loss: 1208.9503173828125\n","Train -> sample/numSamples/epoch: 800/1182/2, Loss: 907.4400634765625\n","Train -> sample/numSamples/epoch: 801/1182/2, Loss: 757.1127319335938\n","Train -> sample/numSamples/epoch: 802/1182/2, Loss: 1189.5826416015625\n","Train -> sample/numSamples/epoch: 803/1182/2, Loss: 1150.306640625\n","Train -> sample/numSamples/epoch: 804/1182/2, Loss: 977.2225341796875\n","Train -> sample/numSamples/epoch: 805/1182/2, Loss: 1231.1180419921875\n","Train -> sample/numSamples/epoch: 806/1182/2, Loss: 1144.6514892578125\n","Train -> sample/numSamples/epoch: 807/1182/2, Loss: 455.6378479003906\n","Train -> sample/numSamples/epoch: 808/1182/2, Loss: 913.8980102539062\n","Train -> sample/numSamples/epoch: 809/1182/2, Loss: 567.7462158203125\n","Train -> sample/numSamples/epoch: 810/1182/2, Loss: 824.76416015625\n","Train -> sample/numSamples/epoch: 811/1182/2, Loss: 912.7841186523438\n","Train -> sample/numSamples/epoch: 812/1182/2, Loss: 648.3526611328125\n","Train -> sample/numSamples/epoch: 813/1182/2, Loss: 1011.0912475585938\n","Train -> sample/numSamples/epoch: 814/1182/2, Loss: 1093.274169921875\n","Train -> sample/numSamples/epoch: 815/1182/2, Loss: 1158.5726318359375\n","Train -> sample/numSamples/epoch: 816/1182/2, Loss: 1215.390380859375\n","Train -> sample/numSamples/epoch: 817/1182/2, Loss: 1377.2305908203125\n","Train -> sample/numSamples/epoch: 818/1182/2, Loss: 976.7822875976562\n","Train -> sample/numSamples/epoch: 819/1182/2, Loss: 1077.718505859375\n","Train -> sample/numSamples/epoch: 820/1182/2, Loss: 1131.7626953125\n","Train -> sample/numSamples/epoch: 821/1182/2, Loss: 1273.4390869140625\n","Train -> sample/numSamples/epoch: 822/1182/2, Loss: 1292.105712890625\n","Train -> sample/numSamples/epoch: 823/1182/2, Loss: 1006.8508911132812\n","Train -> sample/numSamples/epoch: 824/1182/2, Loss: 799.2256469726562\n","Train -> sample/numSamples/epoch: 825/1182/2, Loss: 1067.316650390625\n","Train -> sample/numSamples/epoch: 826/1182/2, Loss: 987.6124267578125\n","Train -> sample/numSamples/epoch: 827/1182/2, Loss: 1184.6641845703125\n","Train -> sample/numSamples/epoch: 828/1182/2, Loss: 1367.34765625\n","Train -> sample/numSamples/epoch: 829/1182/2, Loss: 1115.4906005859375\n","Train -> sample/numSamples/epoch: 830/1182/2, Loss: 782.3497924804688\n","Train -> sample/numSamples/epoch: 831/1182/2, Loss: 1307.05078125\n","Train -> sample/numSamples/epoch: 832/1182/2, Loss: 715.04248046875\n","Train -> sample/numSamples/epoch: 833/1182/2, Loss: 1008.3154296875\n","Train -> sample/numSamples/epoch: 834/1182/2, Loss: 789.801025390625\n","Train -> sample/numSamples/epoch: 835/1182/2, Loss: 1189.7955322265625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 836/1182/2, Loss: 853.3426513671875\n","Train -> sample/numSamples/epoch: 837/1182/2, Loss: 808.9110107421875\n","Train -> sample/numSamples/epoch: 838/1182/2, Loss: 1353.0501708984375\n","Train -> sample/numSamples/epoch: 839/1182/2, Loss: 766.8350219726562\n","Train -> sample/numSamples/epoch: 840/1182/2, Loss: 883.2610473632812\n","Train -> sample/numSamples/epoch: 841/1182/2, Loss: 888.1798095703125\n","Train -> sample/numSamples/epoch: 842/1182/2, Loss: 1043.7757568359375\n","Train -> sample/numSamples/epoch: 843/1182/2, Loss: 1088.564453125\n","Train -> sample/numSamples/epoch: 844/1182/2, Loss: 745.1920166015625\n","Train -> sample/numSamples/epoch: 845/1182/2, Loss: 918.4429931640625\n","Train -> sample/numSamples/epoch: 846/1182/2, Loss: 954.263427734375\n","Train -> sample/numSamples/epoch: 847/1182/2, Loss: 892.0859375\n","Train -> sample/numSamples/epoch: 848/1182/2, Loss: 890.1640625\n","Train -> sample/numSamples/epoch: 849/1182/2, Loss: 1158.590087890625\n","Train -> sample/numSamples/epoch: 850/1182/2, Loss: 1301.3272705078125\n","Train -> sample/numSamples/epoch: 851/1182/2, Loss: 1055.60302734375\n","Train -> sample/numSamples/epoch: 852/1182/2, Loss: 1055.108154296875\n","Train -> sample/numSamples/epoch: 853/1182/2, Loss: 476.2869567871094\n","Train -> sample/numSamples/epoch: 854/1182/2, Loss: 986.8447875976562\n","Train -> sample/numSamples/epoch: 855/1182/2, Loss: 451.4859313964844\n","Train -> sample/numSamples/epoch: 856/1182/2, Loss: 830.3451538085938\n","Train -> sample/numSamples/epoch: 857/1182/2, Loss: 743.389892578125\n","Train -> sample/numSamples/epoch: 858/1182/2, Loss: 1160.67333984375\n","Train -> sample/numSamples/epoch: 859/1182/2, Loss: 856.6126098632812\n","Train -> sample/numSamples/epoch: 860/1182/2, Loss: 944.765625\n","Train -> sample/numSamples/epoch: 861/1182/2, Loss: 678.935791015625\n","Train -> sample/numSamples/epoch: 862/1182/2, Loss: 983.782958984375\n","Train -> sample/numSamples/epoch: 863/1182/2, Loss: 824.1552124023438\n","Train -> sample/numSamples/epoch: 864/1182/2, Loss: 1131.9378662109375\n","Train -> sample/numSamples/epoch: 865/1182/2, Loss: 1559.9482421875\n","Train -> sample/numSamples/epoch: 866/1182/2, Loss: 1206.2757568359375\n","Train -> sample/numSamples/epoch: 867/1182/2, Loss: 960.4539184570312\n","Train -> sample/numSamples/epoch: 868/1182/2, Loss: 960.2615966796875\n","Train -> sample/numSamples/epoch: 869/1182/2, Loss: 1424.34814453125\n","Train -> sample/numSamples/epoch: 870/1182/2, Loss: 830.0872192382812\n","Train -> sample/numSamples/epoch: 871/1182/2, Loss: 1449.793701171875\n","Train -> sample/numSamples/epoch: 872/1182/2, Loss: 1161.77197265625\n","Train -> sample/numSamples/epoch: 873/1182/2, Loss: 1273.5096435546875\n","Train -> sample/numSamples/epoch: 874/1182/2, Loss: 929.5291137695312\n","Train -> sample/numSamples/epoch: 875/1182/2, Loss: 1051.91064453125\n","Train -> sample/numSamples/epoch: 876/1182/2, Loss: 1405.5599365234375\n","Train -> sample/numSamples/epoch: 877/1182/2, Loss: 1049.973876953125\n","Train -> sample/numSamples/epoch: 878/1182/2, Loss: 834.0633544921875\n","Train -> sample/numSamples/epoch: 879/1182/2, Loss: 893.2578735351562\n","Train -> sample/numSamples/epoch: 880/1182/2, Loss: 1021.5274658203125\n","Train -> sample/numSamples/epoch: 881/1182/2, Loss: 1228.0303955078125\n","Train -> sample/numSamples/epoch: 882/1182/2, Loss: 1150.9005126953125\n","Train -> sample/numSamples/epoch: 883/1182/2, Loss: 796.5927734375\n","Train -> sample/numSamples/epoch: 884/1182/2, Loss: 1078.5528564453125\n","Train -> sample/numSamples/epoch: 885/1182/2, Loss: 1479.28271484375\n","Train -> sample/numSamples/epoch: 886/1182/2, Loss: 929.571533203125\n","Train -> sample/numSamples/epoch: 887/1182/2, Loss: 814.2364501953125\n","Train -> sample/numSamples/epoch: 888/1182/2, Loss: 1233.3802490234375\n","Train -> sample/numSamples/epoch: 889/1182/2, Loss: 580.8779296875\n","Train -> sample/numSamples/epoch: 890/1182/2, Loss: 1387.8975830078125\n","Train -> sample/numSamples/epoch: 891/1182/2, Loss: 887.6339721679688\n","Train -> sample/numSamples/epoch: 892/1182/2, Loss: 981.928955078125\n","Train -> sample/numSamples/epoch: 893/1182/2, Loss: 966.7208251953125\n","Train -> sample/numSamples/epoch: 894/1182/2, Loss: 1311.2431640625\n","Train -> sample/numSamples/epoch: 895/1182/2, Loss: 1300.607666015625\n","Train -> sample/numSamples/epoch: 896/1182/2, Loss: 1015.024169921875\n","Train -> sample/numSamples/epoch: 897/1182/2, Loss: 1232.865234375\n","Train -> sample/numSamples/epoch: 898/1182/2, Loss: 1038.9940185546875\n","Train -> sample/numSamples/epoch: 899/1182/2, Loss: 1278.9268798828125\n","Train -> sample/numSamples/epoch: 900/1182/2, Loss: 767.5709838867188\n","Train -> sample/numSamples/epoch: 901/1182/2, Loss: 767.1851806640625\n","Train -> sample/numSamples/epoch: 902/1182/2, Loss: 783.1785278320312\n","Train -> sample/numSamples/epoch: 903/1182/2, Loss: 853.0006103515625\n","Train -> sample/numSamples/epoch: 904/1182/2, Loss: 835.9403686523438\n","Train -> sample/numSamples/epoch: 905/1182/2, Loss: 996.4927978515625\n","Train -> sample/numSamples/epoch: 906/1182/2, Loss: 1451.875\n","Train -> sample/numSamples/epoch: 907/1182/2, Loss: 958.4083251953125\n","Train -> sample/numSamples/epoch: 908/1182/2, Loss: 1020.263916015625\n","Train -> sample/numSamples/epoch: 909/1182/2, Loss: 1290.2197265625\n","Train -> sample/numSamples/epoch: 910/1182/2, Loss: 1180.247314453125\n","Train -> sample/numSamples/epoch: 911/1182/2, Loss: 554.4838256835938\n","Train -> sample/numSamples/epoch: 912/1182/2, Loss: 527.487060546875\n","Train -> sample/numSamples/epoch: 913/1182/2, Loss: 1035.494873046875\n","Train -> sample/numSamples/epoch: 914/1182/2, Loss: 1250.6075439453125\n","Train -> sample/numSamples/epoch: 915/1182/2, Loss: 1398.4246826171875\n","Train -> sample/numSamples/epoch: 916/1182/2, Loss: 978.10546875\n","Train -> sample/numSamples/epoch: 917/1182/2, Loss: 951.1118774414062\n","Train -> sample/numSamples/epoch: 918/1182/2, Loss: 794.6524658203125\n","Train -> sample/numSamples/epoch: 919/1182/2, Loss: 1781.70166015625\n","Train -> sample/numSamples/epoch: 920/1182/2, Loss: 783.0859375\n","Train -> sample/numSamples/epoch: 921/1182/2, Loss: 740.911376953125\n","Train -> sample/numSamples/epoch: 922/1182/2, Loss: 916.6210327148438\n","Train -> sample/numSamples/epoch: 923/1182/2, Loss: 1005.9053955078125\n","Train -> sample/numSamples/epoch: 924/1182/2, Loss: 728.9689331054688\n","Train -> sample/numSamples/epoch: 925/1182/2, Loss: 1161.7283935546875\n","Train -> sample/numSamples/epoch: 926/1182/2, Loss: 1215.5548095703125\n","Train -> sample/numSamples/epoch: 927/1182/2, Loss: 785.7837524414062\n","Train -> sample/numSamples/epoch: 928/1182/2, Loss: 922.0404052734375\n","Train -> sample/numSamples/epoch: 929/1182/2, Loss: 759.353515625\n","Train -> sample/numSamples/epoch: 930/1182/2, Loss: 548.1836547851562\n","Train -> sample/numSamples/epoch: 931/1182/2, Loss: 976.5087890625\n","Train -> sample/numSamples/epoch: 932/1182/2, Loss: 1152.5152587890625\n","Train -> sample/numSamples/epoch: 933/1182/2, Loss: 1036.2086181640625\n","Train -> sample/numSamples/epoch: 934/1182/2, Loss: 631.9939575195312\n","Train -> sample/numSamples/epoch: 935/1182/2, Loss: 1279.0406494140625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 936/1182/2, Loss: 987.5736083984375\n","Train -> sample/numSamples/epoch: 937/1182/2, Loss: 1554.59619140625\n","Train -> sample/numSamples/epoch: 938/1182/2, Loss: 1075.1900634765625\n","Train -> sample/numSamples/epoch: 939/1182/2, Loss: 784.1758422851562\n","Train -> sample/numSamples/epoch: 940/1182/2, Loss: 970.630126953125\n","Train -> sample/numSamples/epoch: 941/1182/2, Loss: 998.5531005859375\n","Train -> sample/numSamples/epoch: 942/1182/2, Loss: 1155.935302734375\n","Train -> sample/numSamples/epoch: 943/1182/2, Loss: 1690.912109375\n","Train -> sample/numSamples/epoch: 944/1182/2, Loss: 838.3543701171875\n","Train -> sample/numSamples/epoch: 945/1182/2, Loss: 1186.109619140625\n","Train -> sample/numSamples/epoch: 946/1182/2, Loss: 706.272705078125\n","Train -> sample/numSamples/epoch: 947/1182/2, Loss: 957.6143798828125\n","Train -> sample/numSamples/epoch: 948/1182/2, Loss: 1215.3629150390625\n","Train -> sample/numSamples/epoch: 949/1182/2, Loss: 963.684814453125\n","Train -> sample/numSamples/epoch: 950/1182/2, Loss: 876.8742065429688\n","Train -> sample/numSamples/epoch: 951/1182/2, Loss: 1194.7279052734375\n","Train -> sample/numSamples/epoch: 952/1182/2, Loss: 1141.58837890625\n","Train -> sample/numSamples/epoch: 953/1182/2, Loss: 702.503662109375\n","Train -> sample/numSamples/epoch: 954/1182/2, Loss: 1325.1661376953125\n","Train -> sample/numSamples/epoch: 955/1182/2, Loss: 1207.3017578125\n","Train -> sample/numSamples/epoch: 956/1182/2, Loss: 1103.62451171875\n","Train -> sample/numSamples/epoch: 957/1182/2, Loss: 1033.8612060546875\n","Train -> sample/numSamples/epoch: 958/1182/2, Loss: 1671.99609375\n","Train -> sample/numSamples/epoch: 959/1182/2, Loss: 1098.86767578125\n","Train -> sample/numSamples/epoch: 960/1182/2, Loss: 1184.1519775390625\n","Train -> sample/numSamples/epoch: 961/1182/2, Loss: 1193.1558837890625\n","Train -> sample/numSamples/epoch: 962/1182/2, Loss: 1167.597412109375\n","Train -> sample/numSamples/epoch: 963/1182/2, Loss: 723.4419555664062\n","Train -> sample/numSamples/epoch: 964/1182/2, Loss: 1208.2225341796875\n","Train -> sample/numSamples/epoch: 965/1182/2, Loss: 1045.6815185546875\n","Train -> sample/numSamples/epoch: 966/1182/2, Loss: 772.4447631835938\n","Train -> sample/numSamples/epoch: 967/1182/2, Loss: 813.6744995117188\n","Train -> sample/numSamples/epoch: 968/1182/2, Loss: 797.6985473632812\n","Train -> sample/numSamples/epoch: 969/1182/2, Loss: 977.7656860351562\n","Train -> sample/numSamples/epoch: 970/1182/2, Loss: 824.396240234375\n","Train -> sample/numSamples/epoch: 971/1182/2, Loss: 1323.62548828125\n","Train -> sample/numSamples/epoch: 972/1182/2, Loss: 1376.93408203125\n","Train -> sample/numSamples/epoch: 973/1182/2, Loss: 1302.19384765625\n","Train -> sample/numSamples/epoch: 974/1182/2, Loss: 1005.230712890625\n","Train -> sample/numSamples/epoch: 975/1182/2, Loss: 1278.7244873046875\n","Train -> sample/numSamples/epoch: 976/1182/2, Loss: 1092.9090576171875\n","Train -> sample/numSamples/epoch: 977/1182/2, Loss: 993.8595581054688\n","Train -> sample/numSamples/epoch: 978/1182/2, Loss: 670.3245239257812\n","Train -> sample/numSamples/epoch: 979/1182/2, Loss: 1013.4700317382812\n","Train -> sample/numSamples/epoch: 980/1182/2, Loss: 1095.1187744140625\n","Train -> sample/numSamples/epoch: 981/1182/2, Loss: 1173.2099609375\n","Train -> sample/numSamples/epoch: 982/1182/2, Loss: 1026.3077392578125\n","Train -> sample/numSamples/epoch: 983/1182/2, Loss: 1241.0938720703125\n","Train -> sample/numSamples/epoch: 984/1182/2, Loss: 1515.9737548828125\n","Train -> sample/numSamples/epoch: 985/1182/2, Loss: 1176.3277587890625\n","Train -> sample/numSamples/epoch: 986/1182/2, Loss: 1106.7435302734375\n","Train -> sample/numSamples/epoch: 987/1182/2, Loss: 1201.01806640625\n","Train -> sample/numSamples/epoch: 988/1182/2, Loss: 940.948974609375\n","Train -> sample/numSamples/epoch: 989/1182/2, Loss: 1096.112060546875\n","Train -> sample/numSamples/epoch: 990/1182/2, Loss: 765.2899780273438\n","Train -> sample/numSamples/epoch: 991/1182/2, Loss: 783.0717163085938\n","Train -> sample/numSamples/epoch: 992/1182/2, Loss: 975.02197265625\n","Train -> sample/numSamples/epoch: 993/1182/2, Loss: 1216.966064453125\n","Train -> sample/numSamples/epoch: 994/1182/2, Loss: 849.497802734375\n","Train -> sample/numSamples/epoch: 995/1182/2, Loss: 1520.4163818359375\n","Train -> sample/numSamples/epoch: 996/1182/2, Loss: 1103.9619140625\n","Train -> sample/numSamples/epoch: 997/1182/2, Loss: 1071.7283935546875\n","Train -> sample/numSamples/epoch: 998/1182/2, Loss: 858.55615234375\n","Train -> sample/numSamples/epoch: 999/1182/2, Loss: 1209.974609375\n","Train -> sample/numSamples/epoch: 1000/1182/2, Loss: 1457.63916015625\n","Train -> sample/numSamples/epoch: 1001/1182/2, Loss: 939.1498413085938\n","Train -> sample/numSamples/epoch: 1002/1182/2, Loss: 1010.7514038085938\n","Train -> sample/numSamples/epoch: 1003/1182/2, Loss: 774.5355224609375\n","Train -> sample/numSamples/epoch: 1004/1182/2, Loss: 1032.718994140625\n","Train -> sample/numSamples/epoch: 1005/1182/2, Loss: 1153.111083984375\n","Train -> sample/numSamples/epoch: 1006/1182/2, Loss: 1142.8153076171875\n","Train -> sample/numSamples/epoch: 1007/1182/2, Loss: 1583.4754638671875\n","Train -> sample/numSamples/epoch: 1008/1182/2, Loss: 704.3159790039062\n","Train -> sample/numSamples/epoch: 1009/1182/2, Loss: 974.3007202148438\n","Train -> sample/numSamples/epoch: 1010/1182/2, Loss: 941.760498046875\n","Train -> sample/numSamples/epoch: 1011/1182/2, Loss: 1230.3736572265625\n","Train -> sample/numSamples/epoch: 1012/1182/2, Loss: 1238.06591796875\n","Train -> sample/numSamples/epoch: 1013/1182/2, Loss: 871.4813842773438\n","Train -> sample/numSamples/epoch: 1014/1182/2, Loss: 1494.0662841796875\n","Train -> sample/numSamples/epoch: 1015/1182/2, Loss: 1149.3909912109375\n","Train -> sample/numSamples/epoch: 1016/1182/2, Loss: 1011.4951171875\n","Train -> sample/numSamples/epoch: 1017/1182/2, Loss: 957.9644165039062\n","Train -> sample/numSamples/epoch: 1018/1182/2, Loss: 831.1254272460938\n","Train -> sample/numSamples/epoch: 1019/1182/2, Loss: 1224.826171875\n","Train -> sample/numSamples/epoch: 1020/1182/2, Loss: 1206.6339111328125\n","Train -> sample/numSamples/epoch: 1021/1182/2, Loss: 1222.0745849609375\n","Train -> sample/numSamples/epoch: 1022/1182/2, Loss: 895.1513061523438\n","Train -> sample/numSamples/epoch: 1023/1182/2, Loss: 1232.7952880859375\n","Train -> sample/numSamples/epoch: 1024/1182/2, Loss: 1174.0040283203125\n","Train -> sample/numSamples/epoch: 1025/1182/2, Loss: 500.3963623046875\n","Train -> sample/numSamples/epoch: 1026/1182/2, Loss: 682.3446655273438\n","Train -> sample/numSamples/epoch: 1027/1182/2, Loss: 1070.6075439453125\n","Train -> sample/numSamples/epoch: 1028/1182/2, Loss: 1170.5439453125\n","Train -> sample/numSamples/epoch: 1029/1182/2, Loss: 1309.8092041015625\n","Train -> sample/numSamples/epoch: 1030/1182/2, Loss: 788.3301391601562\n","Train -> sample/numSamples/epoch: 1031/1182/2, Loss: 1090.9500732421875\n","Train -> sample/numSamples/epoch: 1032/1182/2, Loss: 1119.32861328125\n","Train -> sample/numSamples/epoch: 1033/1182/2, Loss: 1215.968994140625\n","Train -> sample/numSamples/epoch: 1034/1182/2, Loss: 992.3299560546875\n","Train -> sample/numSamples/epoch: 1035/1182/2, Loss: 1277.293212890625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1036/1182/2, Loss: 1365.1824951171875\n","Train -> sample/numSamples/epoch: 1037/1182/2, Loss: 1140.205322265625\n","Train -> sample/numSamples/epoch: 1038/1182/2, Loss: 844.3026123046875\n","Train -> sample/numSamples/epoch: 1039/1182/2, Loss: 1204.5172119140625\n","Train -> sample/numSamples/epoch: 1040/1182/2, Loss: 1085.848388671875\n","Train -> sample/numSamples/epoch: 1041/1182/2, Loss: 666.7108154296875\n","Train -> sample/numSamples/epoch: 1042/1182/2, Loss: 839.8649291992188\n","Train -> sample/numSamples/epoch: 1043/1182/2, Loss: 1208.28173828125\n","Train -> sample/numSamples/epoch: 1044/1182/2, Loss: 973.5144653320312\n","Train -> sample/numSamples/epoch: 1045/1182/2, Loss: 1499.9180908203125\n","Train -> sample/numSamples/epoch: 1046/1182/2, Loss: 987.4463500976562\n","Train -> sample/numSamples/epoch: 1047/1182/2, Loss: 894.5335083007812\n","Train -> sample/numSamples/epoch: 1048/1182/2, Loss: 1306.5311279296875\n","Train -> sample/numSamples/epoch: 1049/1182/2, Loss: 1185.889404296875\n","Train -> sample/numSamples/epoch: 1050/1182/2, Loss: 744.3760986328125\n","Train -> sample/numSamples/epoch: 1051/1182/2, Loss: 1102.4368896484375\n","Train -> sample/numSamples/epoch: 1052/1182/2, Loss: 1047.1881103515625\n","Train -> sample/numSamples/epoch: 1053/1182/2, Loss: 1313.8145751953125\n","Train -> sample/numSamples/epoch: 1054/1182/2, Loss: 987.630126953125\n","Train -> sample/numSamples/epoch: 1055/1182/2, Loss: 1284.197509765625\n","Train -> sample/numSamples/epoch: 1056/1182/2, Loss: 665.8101806640625\n","Train -> sample/numSamples/epoch: 1057/1182/2, Loss: 956.7938842773438\n","Train -> sample/numSamples/epoch: 1058/1182/2, Loss: 1030.3853759765625\n","Train -> sample/numSamples/epoch: 1059/1182/2, Loss: 1028.8502197265625\n","Train -> sample/numSamples/epoch: 1060/1182/2, Loss: 735.9766235351562\n","Train -> sample/numSamples/epoch: 1061/1182/2, Loss: 655.0596923828125\n","Train -> sample/numSamples/epoch: 1062/1182/2, Loss: 1090.2960205078125\n","Train -> sample/numSamples/epoch: 1063/1182/2, Loss: 913.1229858398438\n","Train -> sample/numSamples/epoch: 1064/1182/2, Loss: 1051.393310546875\n","Train -> sample/numSamples/epoch: 1065/1182/2, Loss: 1208.595703125\n","Train -> sample/numSamples/epoch: 1066/1182/2, Loss: 920.7706298828125\n","Train -> sample/numSamples/epoch: 1067/1182/2, Loss: 1567.4217529296875\n","Train -> sample/numSamples/epoch: 1068/1182/2, Loss: 1039.69091796875\n","Train -> sample/numSamples/epoch: 1069/1182/2, Loss: 967.3768310546875\n","Train -> sample/numSamples/epoch: 1070/1182/2, Loss: 698.3724365234375\n","Train -> sample/numSamples/epoch: 1071/1182/2, Loss: 1150.6240234375\n","Train -> sample/numSamples/epoch: 1072/1182/2, Loss: 1059.19677734375\n","Train -> sample/numSamples/epoch: 1073/1182/2, Loss: 678.51708984375\n","Train -> sample/numSamples/epoch: 1074/1182/2, Loss: 1199.3111572265625\n","Train -> sample/numSamples/epoch: 1075/1182/2, Loss: 861.8778076171875\n","Train -> sample/numSamples/epoch: 1076/1182/2, Loss: 1047.2940673828125\n","Train -> sample/numSamples/epoch: 1077/1182/2, Loss: 845.3453979492188\n","Train -> sample/numSamples/epoch: 1078/1182/2, Loss: 905.6455078125\n","Train -> sample/numSamples/epoch: 1079/1182/2, Loss: 855.2801513671875\n","Train -> sample/numSamples/epoch: 1080/1182/2, Loss: 896.5048217773438\n","Train -> sample/numSamples/epoch: 1081/1182/2, Loss: 1054.5086669921875\n","Train -> sample/numSamples/epoch: 1082/1182/2, Loss: 925.0460205078125\n","Train -> sample/numSamples/epoch: 1083/1182/2, Loss: 1595.1348876953125\n","Train -> sample/numSamples/epoch: 1084/1182/2, Loss: 1165.682373046875\n","Train -> sample/numSamples/epoch: 1085/1182/2, Loss: 754.1759033203125\n","Train -> sample/numSamples/epoch: 1086/1182/2, Loss: 1021.7139282226562\n","Train -> sample/numSamples/epoch: 1087/1182/2, Loss: 687.3298950195312\n","Train -> sample/numSamples/epoch: 1088/1182/2, Loss: 1332.0181884765625\n","Train -> sample/numSamples/epoch: 1089/1182/2, Loss: 968.8883666992188\n","Train -> sample/numSamples/epoch: 1090/1182/2, Loss: 729.5720825195312\n","Train -> sample/numSamples/epoch: 1091/1182/2, Loss: 937.4403686523438\n","Train -> sample/numSamples/epoch: 1092/1182/2, Loss: 921.7320556640625\n","Train -> sample/numSamples/epoch: 1093/1182/2, Loss: 1294.2841796875\n","Train -> sample/numSamples/epoch: 1094/1182/2, Loss: 950.1528930664062\n","Train -> sample/numSamples/epoch: 1095/1182/2, Loss: 1400.32275390625\n","Train -> sample/numSamples/epoch: 1096/1182/2, Loss: 873.9617309570312\n","Train -> sample/numSamples/epoch: 1097/1182/2, Loss: 1126.88623046875\n","Train -> sample/numSamples/epoch: 1098/1182/2, Loss: 496.54302978515625\n","Train -> sample/numSamples/epoch: 1099/1182/2, Loss: 808.0365600585938\n","Train -> sample/numSamples/epoch: 1100/1182/2, Loss: 983.8416137695312\n","Train -> sample/numSamples/epoch: 1101/1182/2, Loss: 1336.80908203125\n","Train -> sample/numSamples/epoch: 1102/1182/2, Loss: 993.1647338867188\n","Train -> sample/numSamples/epoch: 1103/1182/2, Loss: 1088.502685546875\n","Train -> sample/numSamples/epoch: 1104/1182/2, Loss: 877.646728515625\n","Train -> sample/numSamples/epoch: 1105/1182/2, Loss: 782.6124267578125\n","Train -> sample/numSamples/epoch: 1106/1182/2, Loss: 1048.1207275390625\n","Train -> sample/numSamples/epoch: 1107/1182/2, Loss: 808.0474243164062\n","Train -> sample/numSamples/epoch: 1108/1182/2, Loss: 1011.6107177734375\n","Train -> sample/numSamples/epoch: 1109/1182/2, Loss: 787.4016723632812\n","Train -> sample/numSamples/epoch: 1110/1182/2, Loss: 1374.2432861328125\n","Train -> sample/numSamples/epoch: 1111/1182/2, Loss: 1389.6748046875\n","Train -> sample/numSamples/epoch: 1112/1182/2, Loss: 926.2203979492188\n","Train -> sample/numSamples/epoch: 1113/1182/2, Loss: 1215.8123779296875\n","Train -> sample/numSamples/epoch: 1114/1182/2, Loss: 669.8108520507812\n","Train -> sample/numSamples/epoch: 1115/1182/2, Loss: 1109.025390625\n","Train -> sample/numSamples/epoch: 1116/1182/2, Loss: 482.4252014160156\n","Train -> sample/numSamples/epoch: 1117/1182/2, Loss: 981.0360717773438\n","Train -> sample/numSamples/epoch: 1118/1182/2, Loss: 741.5097045898438\n","Train -> sample/numSamples/epoch: 1119/1182/2, Loss: 1490.5538330078125\n","Train -> sample/numSamples/epoch: 1120/1182/2, Loss: 1102.1329345703125\n","Train -> sample/numSamples/epoch: 1121/1182/2, Loss: 810.8939208984375\n","Train -> sample/numSamples/epoch: 1122/1182/2, Loss: 730.5786743164062\n","Train -> sample/numSamples/epoch: 1123/1182/2, Loss: 1564.0076904296875\n","Train -> sample/numSamples/epoch: 1124/1182/2, Loss: 921.5467529296875\n","Train -> sample/numSamples/epoch: 1125/1182/2, Loss: 1302.8795166015625\n","Train -> sample/numSamples/epoch: 1126/1182/2, Loss: 1033.1903076171875\n","Train -> sample/numSamples/epoch: 1127/1182/2, Loss: 1028.9393310546875\n","Train -> sample/numSamples/epoch: 1128/1182/2, Loss: 1199.7159423828125\n","Train -> sample/numSamples/epoch: 1129/1182/2, Loss: 1316.7696533203125\n","Train -> sample/numSamples/epoch: 1130/1182/2, Loss: 1318.0223388671875\n","Train -> sample/numSamples/epoch: 1131/1182/2, Loss: 752.7296142578125\n","Train -> sample/numSamples/epoch: 1132/1182/2, Loss: 1102.2938232421875\n","Train -> sample/numSamples/epoch: 1133/1182/2, Loss: 1278.1331787109375\n","Train -> sample/numSamples/epoch: 1134/1182/2, Loss: 1056.1512451171875\n","Train -> sample/numSamples/epoch: 1135/1182/2, Loss: 766.9121704101562\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1136/1182/2, Loss: 1367.6864013671875\n","Train -> sample/numSamples/epoch: 1137/1182/2, Loss: 1015.4075317382812\n","Train -> sample/numSamples/epoch: 1138/1182/2, Loss: 1091.8548583984375\n","Train -> sample/numSamples/epoch: 1139/1182/2, Loss: 834.082275390625\n","Train -> sample/numSamples/epoch: 1140/1182/2, Loss: 847.4718627929688\n","Train -> sample/numSamples/epoch: 1141/1182/2, Loss: 950.9366455078125\n","Train -> sample/numSamples/epoch: 1142/1182/2, Loss: 1078.6834716796875\n","Train -> sample/numSamples/epoch: 1143/1182/2, Loss: 1145.306396484375\n","Train -> sample/numSamples/epoch: 1144/1182/2, Loss: 1259.509765625\n","Train -> sample/numSamples/epoch: 1145/1182/2, Loss: 1054.47802734375\n","Train -> sample/numSamples/epoch: 1146/1182/2, Loss: 892.2175903320312\n","Train -> sample/numSamples/epoch: 1147/1182/2, Loss: 730.996337890625\n","Train -> sample/numSamples/epoch: 1148/1182/2, Loss: 964.9029541015625\n","Train -> sample/numSamples/epoch: 1149/1182/2, Loss: 1081.732177734375\n","Train -> sample/numSamples/epoch: 1150/1182/2, Loss: 810.0994873046875\n","Train -> sample/numSamples/epoch: 1151/1182/2, Loss: 943.36962890625\n","Train -> sample/numSamples/epoch: 1152/1182/2, Loss: 1048.0167236328125\n","Train -> sample/numSamples/epoch: 1153/1182/2, Loss: 982.1281127929688\n","Train -> sample/numSamples/epoch: 1154/1182/2, Loss: 1240.0968017578125\n","Train -> sample/numSamples/epoch: 1155/1182/2, Loss: 1152.3973388671875\n","Train -> sample/numSamples/epoch: 1156/1182/2, Loss: 814.7987670898438\n","Train -> sample/numSamples/epoch: 1157/1182/2, Loss: 837.306640625\n","Train -> sample/numSamples/epoch: 1158/1182/2, Loss: 968.0742797851562\n","Train -> sample/numSamples/epoch: 1159/1182/2, Loss: 826.096435546875\n","Train -> sample/numSamples/epoch: 1160/1182/2, Loss: 944.1784057617188\n","Train -> sample/numSamples/epoch: 1161/1182/2, Loss: 816.3687744140625\n","Train -> sample/numSamples/epoch: 1162/1182/2, Loss: 1293.713623046875\n","Train -> sample/numSamples/epoch: 1163/1182/2, Loss: 1222.9271240234375\n","Train -> sample/numSamples/epoch: 1164/1182/2, Loss: 776.9381713867188\n","Train -> sample/numSamples/epoch: 1165/1182/2, Loss: 1201.1419677734375\n","Train -> sample/numSamples/epoch: 1166/1182/2, Loss: 1041.630859375\n","Train -> sample/numSamples/epoch: 1167/1182/2, Loss: 933.6669311523438\n","Train -> sample/numSamples/epoch: 1168/1182/2, Loss: 920.8199462890625\n","Train -> sample/numSamples/epoch: 1169/1182/2, Loss: 1580.5694580078125\n","Train -> sample/numSamples/epoch: 1170/1182/2, Loss: 835.9653930664062\n","Train -> sample/numSamples/epoch: 1171/1182/2, Loss: 675.0391235351562\n","Train -> sample/numSamples/epoch: 1172/1182/2, Loss: 1208.93505859375\n","Train -> sample/numSamples/epoch: 1173/1182/2, Loss: 603.7783203125\n","Train -> sample/numSamples/epoch: 1174/1182/2, Loss: 1043.04443359375\n","Train -> sample/numSamples/epoch: 1175/1182/2, Loss: 1291.4000244140625\n","Train -> sample/numSamples/epoch: 1176/1182/2, Loss: 1120.5416259765625\n","Train -> sample/numSamples/epoch: 1177/1182/2, Loss: 793.0482788085938\n","Train -> sample/numSamples/epoch: 1178/1182/2, Loss: 1213.2449951171875\n","Train -> sample/numSamples/epoch: 1179/1182/2, Loss: 865.123779296875\n","Train -> sample/numSamples/epoch: 1180/1182/2, Loss: 933.6097412109375\n","Train -> sample/numSamples/epoch: 1181/1182/2, Loss: 1267.269775390625\n","Train -> sample/numSamples/epoch: 0/1182/3, Loss: 1327.0802001953125\n","Train -> sample/numSamples/epoch: 1/1182/3, Loss: 1752.1474609375\n","Train -> sample/numSamples/epoch: 2/1182/3, Loss: 1416.013671875\n","Train -> sample/numSamples/epoch: 3/1182/3, Loss: 679.8729248046875\n","Train -> sample/numSamples/epoch: 4/1182/3, Loss: 1047.8946533203125\n","Train -> sample/numSamples/epoch: 5/1182/3, Loss: 1059.7623291015625\n","Train -> sample/numSamples/epoch: 6/1182/3, Loss: 976.0853271484375\n","Train -> sample/numSamples/epoch: 7/1182/3, Loss: 813.0374755859375\n","Train -> sample/numSamples/epoch: 8/1182/3, Loss: 1389.8095703125\n","Train -> sample/numSamples/epoch: 9/1182/3, Loss: 804.1471557617188\n","Train -> sample/numSamples/epoch: 10/1182/3, Loss: 1070.463623046875\n","Train -> sample/numSamples/epoch: 11/1182/3, Loss: 437.0650329589844\n","Train -> sample/numSamples/epoch: 12/1182/3, Loss: 921.46875\n","Train -> sample/numSamples/epoch: 13/1182/3, Loss: 975.3953247070312\n","Train -> sample/numSamples/epoch: 14/1182/3, Loss: 1225.063232421875\n","Train -> sample/numSamples/epoch: 15/1182/3, Loss: 1216.7100830078125\n","Train -> sample/numSamples/epoch: 16/1182/3, Loss: 1191.3350830078125\n","Train -> sample/numSamples/epoch: 17/1182/3, Loss: 1346.7996826171875\n","Train -> sample/numSamples/epoch: 18/1182/3, Loss: 1164.16650390625\n","Train -> sample/numSamples/epoch: 19/1182/3, Loss: 1128.9500732421875\n","Train -> sample/numSamples/epoch: 20/1182/3, Loss: 1110.6888427734375\n","Train -> sample/numSamples/epoch: 21/1182/3, Loss: 770.0127563476562\n","Train -> sample/numSamples/epoch: 22/1182/3, Loss: 1119.086669921875\n","Train -> sample/numSamples/epoch: 23/1182/3, Loss: 1427.9384765625\n","Train -> sample/numSamples/epoch: 24/1182/3, Loss: 661.59716796875\n","Train -> sample/numSamples/epoch: 25/1182/3, Loss: 1238.9033203125\n","Train -> sample/numSamples/epoch: 26/1182/3, Loss: 815.3991088867188\n","Train -> sample/numSamples/epoch: 27/1182/3, Loss: 1037.78125\n","Train -> sample/numSamples/epoch: 28/1182/3, Loss: 733.8001098632812\n","Train -> sample/numSamples/epoch: 29/1182/3, Loss: 1279.591796875\n","Train -> sample/numSamples/epoch: 30/1182/3, Loss: 807.0516967773438\n","Train -> sample/numSamples/epoch: 31/1182/3, Loss: 1488.6610107421875\n","Train -> sample/numSamples/epoch: 32/1182/3, Loss: 936.4220581054688\n","Train -> sample/numSamples/epoch: 33/1182/3, Loss: 712.6764526367188\n","Train -> sample/numSamples/epoch: 34/1182/3, Loss: 975.6776733398438\n","Train -> sample/numSamples/epoch: 35/1182/3, Loss: 1030.175537109375\n","Train -> sample/numSamples/epoch: 36/1182/3, Loss: 820.3001098632812\n","Train -> sample/numSamples/epoch: 37/1182/3, Loss: 1260.162109375\n","Train -> sample/numSamples/epoch: 38/1182/3, Loss: 266.6393737792969\n","Train -> sample/numSamples/epoch: 39/1182/3, Loss: 740.4442749023438\n","Train -> sample/numSamples/epoch: 40/1182/3, Loss: 900.1695556640625\n","Train -> sample/numSamples/epoch: 41/1182/3, Loss: 1039.4498291015625\n","Train -> sample/numSamples/epoch: 42/1182/3, Loss: 1207.802001953125\n","Train -> sample/numSamples/epoch: 43/1182/3, Loss: 840.65625\n","Train -> sample/numSamples/epoch: 44/1182/3, Loss: 1041.6885986328125\n","Train -> sample/numSamples/epoch: 45/1182/3, Loss: 679.3707885742188\n","Train -> sample/numSamples/epoch: 46/1182/3, Loss: 1116.11279296875\n","Train -> sample/numSamples/epoch: 47/1182/3, Loss: 1420.8773193359375\n","Train -> sample/numSamples/epoch: 48/1182/3, Loss: 1086.5281982421875\n","Train -> sample/numSamples/epoch: 49/1182/3, Loss: 1122.6712646484375\n","Train -> sample/numSamples/epoch: 50/1182/3, Loss: 1240.272705078125\n","Train -> sample/numSamples/epoch: 51/1182/3, Loss: 649.4812622070312\n","Train -> sample/numSamples/epoch: 52/1182/3, Loss: 998.1173095703125\n","Train -> sample/numSamples/epoch: 53/1182/3, Loss: 1073.22216796875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 54/1182/3, Loss: 906.0636596679688\n","Train -> sample/numSamples/epoch: 55/1182/3, Loss: 1043.7308349609375\n","Train -> sample/numSamples/epoch: 56/1182/3, Loss: 1300.3966064453125\n","Train -> sample/numSamples/epoch: 57/1182/3, Loss: 891.3495483398438\n","Train -> sample/numSamples/epoch: 58/1182/3, Loss: 1254.61474609375\n","Train -> sample/numSamples/epoch: 59/1182/3, Loss: 671.8444213867188\n","Train -> sample/numSamples/epoch: 60/1182/3, Loss: 1132.36865234375\n","Train -> sample/numSamples/epoch: 61/1182/3, Loss: 899.4689331054688\n","Train -> sample/numSamples/epoch: 62/1182/3, Loss: 1083.0870361328125\n","Train -> sample/numSamples/epoch: 63/1182/3, Loss: 858.2449951171875\n","Train -> sample/numSamples/epoch: 64/1182/3, Loss: 1358.109619140625\n","Train -> sample/numSamples/epoch: 65/1182/3, Loss: 1031.771728515625\n","Train -> sample/numSamples/epoch: 66/1182/3, Loss: 1269.603759765625\n","Train -> sample/numSamples/epoch: 67/1182/3, Loss: 1065.39306640625\n","Train -> sample/numSamples/epoch: 68/1182/3, Loss: 1161.3104248046875\n","Train -> sample/numSamples/epoch: 69/1182/3, Loss: 1356.9930419921875\n","Train -> sample/numSamples/epoch: 70/1182/3, Loss: 763.5504760742188\n","Train -> sample/numSamples/epoch: 71/1182/3, Loss: 1445.36669921875\n","Train -> sample/numSamples/epoch: 72/1182/3, Loss: 876.2324829101562\n","Train -> sample/numSamples/epoch: 73/1182/3, Loss: 983.7271728515625\n","Train -> sample/numSamples/epoch: 74/1182/3, Loss: 686.0079956054688\n","Train -> sample/numSamples/epoch: 75/1182/3, Loss: 685.3590087890625\n","Train -> sample/numSamples/epoch: 76/1182/3, Loss: 851.2586669921875\n","Train -> sample/numSamples/epoch: 77/1182/3, Loss: 1466.9576416015625\n","Train -> sample/numSamples/epoch: 78/1182/3, Loss: 1655.674072265625\n","Train -> sample/numSamples/epoch: 79/1182/3, Loss: 521.0979614257812\n","Train -> sample/numSamples/epoch: 80/1182/3, Loss: 627.2567749023438\n","Train -> sample/numSamples/epoch: 81/1182/3, Loss: 853.8101196289062\n","Train -> sample/numSamples/epoch: 82/1182/3, Loss: 1092.2113037109375\n","Train -> sample/numSamples/epoch: 83/1182/3, Loss: 662.3080444335938\n","Train -> sample/numSamples/epoch: 84/1182/3, Loss: 929.28759765625\n","Train -> sample/numSamples/epoch: 85/1182/3, Loss: 1050.415771484375\n","Train -> sample/numSamples/epoch: 86/1182/3, Loss: 855.0792846679688\n","Train -> sample/numSamples/epoch: 87/1182/3, Loss: 1378.6881103515625\n","Train -> sample/numSamples/epoch: 88/1182/3, Loss: 1405.302490234375\n","Train -> sample/numSamples/epoch: 89/1182/3, Loss: 1189.126220703125\n","Train -> sample/numSamples/epoch: 90/1182/3, Loss: 802.408935546875\n","Train -> sample/numSamples/epoch: 91/1182/3, Loss: 1063.5489501953125\n","Train -> sample/numSamples/epoch: 92/1182/3, Loss: 1439.168212890625\n","Train -> sample/numSamples/epoch: 93/1182/3, Loss: 1294.5338134765625\n","Train -> sample/numSamples/epoch: 94/1182/3, Loss: 1721.6131591796875\n","Train -> sample/numSamples/epoch: 95/1182/3, Loss: 1019.93310546875\n","Train -> sample/numSamples/epoch: 96/1182/3, Loss: 1045.81982421875\n","Train -> sample/numSamples/epoch: 97/1182/3, Loss: 1030.5115966796875\n","Train -> sample/numSamples/epoch: 98/1182/3, Loss: 787.46484375\n","Train -> sample/numSamples/epoch: 99/1182/3, Loss: 1263.2462158203125\n","Train -> sample/numSamples/epoch: 100/1182/3, Loss: 614.1207275390625\n","Train -> sample/numSamples/epoch: 101/1182/3, Loss: 1061.105224609375\n","Train -> sample/numSamples/epoch: 102/1182/3, Loss: 1003.700439453125\n","Train -> sample/numSamples/epoch: 103/1182/3, Loss: 728.8341674804688\n","Train -> sample/numSamples/epoch: 104/1182/3, Loss: 1094.12451171875\n","Train -> sample/numSamples/epoch: 105/1182/3, Loss: 1123.640869140625\n","Train -> sample/numSamples/epoch: 106/1182/3, Loss: 1024.0391845703125\n","Train -> sample/numSamples/epoch: 107/1182/3, Loss: 917.3092651367188\n","Train -> sample/numSamples/epoch: 108/1182/3, Loss: 1092.01708984375\n","Train -> sample/numSamples/epoch: 109/1182/3, Loss: 1093.0096435546875\n","Train -> sample/numSamples/epoch: 110/1182/3, Loss: 1435.421630859375\n","Train -> sample/numSamples/epoch: 111/1182/3, Loss: 1333.542724609375\n","Train -> sample/numSamples/epoch: 112/1182/3, Loss: 927.5382690429688\n","Train -> sample/numSamples/epoch: 113/1182/3, Loss: 966.3587646484375\n","Train -> sample/numSamples/epoch: 114/1182/3, Loss: 1251.6943359375\n","Train -> sample/numSamples/epoch: 115/1182/3, Loss: 758.2362060546875\n","Train -> sample/numSamples/epoch: 116/1182/3, Loss: 754.080078125\n","Train -> sample/numSamples/epoch: 117/1182/3, Loss: 1118.5296630859375\n","Train -> sample/numSamples/epoch: 118/1182/3, Loss: 1449.014404296875\n","Train -> sample/numSamples/epoch: 119/1182/3, Loss: 1456.8353271484375\n","Train -> sample/numSamples/epoch: 120/1182/3, Loss: 1584.30419921875\n","Train -> sample/numSamples/epoch: 121/1182/3, Loss: 1101.021728515625\n","Train -> sample/numSamples/epoch: 122/1182/3, Loss: 1216.1253662109375\n","Train -> sample/numSamples/epoch: 123/1182/3, Loss: 891.0075073242188\n","Train -> sample/numSamples/epoch: 124/1182/3, Loss: 1127.0703125\n","Train -> sample/numSamples/epoch: 125/1182/3, Loss: 503.47686767578125\n","Train -> sample/numSamples/epoch: 126/1182/3, Loss: 1027.4901123046875\n","Train -> sample/numSamples/epoch: 127/1182/3, Loss: 1216.5467529296875\n","Train -> sample/numSamples/epoch: 128/1182/3, Loss: 1354.3486328125\n","Train -> sample/numSamples/epoch: 129/1182/3, Loss: 994.1649169921875\n","Train -> sample/numSamples/epoch: 130/1182/3, Loss: 1296.7373046875\n","Train -> sample/numSamples/epoch: 131/1182/3, Loss: 1124.4097900390625\n","Train -> sample/numSamples/epoch: 132/1182/3, Loss: 1007.21630859375\n","Train -> sample/numSamples/epoch: 133/1182/3, Loss: 714.43310546875\n","Train -> sample/numSamples/epoch: 134/1182/3, Loss: 872.645751953125\n","Train -> sample/numSamples/epoch: 135/1182/3, Loss: 778.8196411132812\n","Train -> sample/numSamples/epoch: 136/1182/3, Loss: 994.991455078125\n","Train -> sample/numSamples/epoch: 137/1182/3, Loss: 1257.0438232421875\n","Train -> sample/numSamples/epoch: 138/1182/3, Loss: 1171.521240234375\n","Train -> sample/numSamples/epoch: 139/1182/3, Loss: 661.3406982421875\n","Train -> sample/numSamples/epoch: 140/1182/3, Loss: 1185.3851318359375\n","Train -> sample/numSamples/epoch: 141/1182/3, Loss: 1509.4241943359375\n","Train -> sample/numSamples/epoch: 142/1182/3, Loss: 898.900146484375\n","Train -> sample/numSamples/epoch: 143/1182/3, Loss: 1435.7171630859375\n","Train -> sample/numSamples/epoch: 144/1182/3, Loss: 813.1725463867188\n","Train -> sample/numSamples/epoch: 145/1182/3, Loss: 948.76123046875\n","Train -> sample/numSamples/epoch: 146/1182/3, Loss: 1270.3826904296875\n","Train -> sample/numSamples/epoch: 147/1182/3, Loss: 836.0223388671875\n","Train -> sample/numSamples/epoch: 148/1182/3, Loss: 720.6559448242188\n","Train -> sample/numSamples/epoch: 149/1182/3, Loss: 1276.673583984375\n","Train -> sample/numSamples/epoch: 150/1182/3, Loss: 1779.0047607421875\n","Train -> sample/numSamples/epoch: 151/1182/3, Loss: 1441.4464111328125\n","Train -> sample/numSamples/epoch: 152/1182/3, Loss: 819.2236328125\n","Train -> sample/numSamples/epoch: 153/1182/3, Loss: 1273.1163330078125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 154/1182/3, Loss: 1120.939208984375\n","Train -> sample/numSamples/epoch: 155/1182/3, Loss: 900.1576538085938\n","Train -> sample/numSamples/epoch: 156/1182/3, Loss: 938.7280883789062\n","Train -> sample/numSamples/epoch: 157/1182/3, Loss: 1529.228515625\n","Train -> sample/numSamples/epoch: 158/1182/3, Loss: 1355.158203125\n","Train -> sample/numSamples/epoch: 159/1182/3, Loss: 1239.02392578125\n","Train -> sample/numSamples/epoch: 160/1182/3, Loss: 699.979248046875\n","Train -> sample/numSamples/epoch: 161/1182/3, Loss: 1309.983642578125\n","Train -> sample/numSamples/epoch: 162/1182/3, Loss: 841.74853515625\n","Train -> sample/numSamples/epoch: 163/1182/3, Loss: 1253.761474609375\n","Train -> sample/numSamples/epoch: 164/1182/3, Loss: 946.9120483398438\n","Train -> sample/numSamples/epoch: 165/1182/3, Loss: 1318.784423828125\n","Train -> sample/numSamples/epoch: 166/1182/3, Loss: 558.3754272460938\n","Train -> sample/numSamples/epoch: 167/1182/3, Loss: 917.1710205078125\n","Train -> sample/numSamples/epoch: 168/1182/3, Loss: 1311.6436767578125\n","Train -> sample/numSamples/epoch: 169/1182/3, Loss: 1249.52685546875\n","Train -> sample/numSamples/epoch: 170/1182/3, Loss: 1010.6390991210938\n","Train -> sample/numSamples/epoch: 171/1182/3, Loss: 1152.2977294921875\n","Train -> sample/numSamples/epoch: 172/1182/3, Loss: 1150.013671875\n","Train -> sample/numSamples/epoch: 173/1182/3, Loss: 754.2803955078125\n","Train -> sample/numSamples/epoch: 174/1182/3, Loss: 584.9165649414062\n","Train -> sample/numSamples/epoch: 175/1182/3, Loss: 893.6243286132812\n","Train -> sample/numSamples/epoch: 176/1182/3, Loss: 812.5359497070312\n","Train -> sample/numSamples/epoch: 177/1182/3, Loss: 910.948486328125\n","Train -> sample/numSamples/epoch: 178/1182/3, Loss: 1861.23193359375\n","Train -> sample/numSamples/epoch: 179/1182/3, Loss: 958.149658203125\n","Train -> sample/numSamples/epoch: 180/1182/3, Loss: 1269.953369140625\n","Train -> sample/numSamples/epoch: 181/1182/3, Loss: 654.94677734375\n","Train -> sample/numSamples/epoch: 182/1182/3, Loss: 885.6951293945312\n","Train -> sample/numSamples/epoch: 183/1182/3, Loss: 1240.171875\n","Train -> sample/numSamples/epoch: 184/1182/3, Loss: 1376.047607421875\n","Train -> sample/numSamples/epoch: 185/1182/3, Loss: 1246.9696044921875\n","Train -> sample/numSamples/epoch: 186/1182/3, Loss: 1096.7593994140625\n","Train -> sample/numSamples/epoch: 187/1182/3, Loss: 629.2706909179688\n","Train -> sample/numSamples/epoch: 188/1182/3, Loss: 830.8443603515625\n","Train -> sample/numSamples/epoch: 189/1182/3, Loss: 912.995361328125\n","Train -> sample/numSamples/epoch: 190/1182/3, Loss: 1241.3634033203125\n","Train -> sample/numSamples/epoch: 191/1182/3, Loss: 1113.0950927734375\n","Train -> sample/numSamples/epoch: 192/1182/3, Loss: 979.9321899414062\n","Train -> sample/numSamples/epoch: 193/1182/3, Loss: 1368.940185546875\n","Train -> sample/numSamples/epoch: 194/1182/3, Loss: 772.5064086914062\n","Train -> sample/numSamples/epoch: 195/1182/3, Loss: 1381.21142578125\n","Train -> sample/numSamples/epoch: 196/1182/3, Loss: 1054.50439453125\n","Train -> sample/numSamples/epoch: 197/1182/3, Loss: 833.2271728515625\n","Train -> sample/numSamples/epoch: 198/1182/3, Loss: 787.3418579101562\n","Train -> sample/numSamples/epoch: 199/1182/3, Loss: 1077.5545654296875\n","Train -> sample/numSamples/epoch: 200/1182/3, Loss: 865.306396484375\n","Train -> sample/numSamples/epoch: 201/1182/3, Loss: 770.1122436523438\n","Train -> sample/numSamples/epoch: 202/1182/3, Loss: 820.1707763671875\n","Train -> sample/numSamples/epoch: 203/1182/3, Loss: 952.4674072265625\n","Train -> sample/numSamples/epoch: 204/1182/3, Loss: 692.93798828125\n","Train -> sample/numSamples/epoch: 205/1182/3, Loss: 1111.2725830078125\n","Train -> sample/numSamples/epoch: 206/1182/3, Loss: 1270.9093017578125\n","Train -> sample/numSamples/epoch: 207/1182/3, Loss: 1256.995849609375\n","Train -> sample/numSamples/epoch: 208/1182/3, Loss: 1393.73193359375\n","Train -> sample/numSamples/epoch: 209/1182/3, Loss: 1232.0809326171875\n","Train -> sample/numSamples/epoch: 210/1182/3, Loss: 1051.626953125\n","Train -> sample/numSamples/epoch: 211/1182/3, Loss: 818.4874267578125\n","Train -> sample/numSamples/epoch: 212/1182/3, Loss: 1138.4749755859375\n","Train -> sample/numSamples/epoch: 213/1182/3, Loss: 1009.3924560546875\n","Train -> sample/numSamples/epoch: 214/1182/3, Loss: 692.2116088867188\n","Train -> sample/numSamples/epoch: 215/1182/3, Loss: 896.80029296875\n","Train -> sample/numSamples/epoch: 216/1182/3, Loss: 869.7923583984375\n","Train -> sample/numSamples/epoch: 217/1182/3, Loss: 710.0966796875\n","Train -> sample/numSamples/epoch: 218/1182/3, Loss: 1187.34716796875\n","Train -> sample/numSamples/epoch: 219/1182/3, Loss: 1302.4075927734375\n","Train -> sample/numSamples/epoch: 220/1182/3, Loss: 1361.22216796875\n","Train -> sample/numSamples/epoch: 221/1182/3, Loss: 1149.055908203125\n","Train -> sample/numSamples/epoch: 222/1182/3, Loss: 1319.2022705078125\n","Train -> sample/numSamples/epoch: 223/1182/3, Loss: 1019.706298828125\n","Train -> sample/numSamples/epoch: 224/1182/3, Loss: 908.6634521484375\n","Train -> sample/numSamples/epoch: 225/1182/3, Loss: 988.7721557617188\n","Train -> sample/numSamples/epoch: 226/1182/3, Loss: 938.9379272460938\n","Train -> sample/numSamples/epoch: 227/1182/3, Loss: 979.3076782226562\n","Train -> sample/numSamples/epoch: 228/1182/3, Loss: 959.351806640625\n","Train -> sample/numSamples/epoch: 229/1182/3, Loss: 1218.662353515625\n","Train -> sample/numSamples/epoch: 230/1182/3, Loss: 844.5298461914062\n","Train -> sample/numSamples/epoch: 231/1182/3, Loss: 784.079345703125\n","Train -> sample/numSamples/epoch: 232/1182/3, Loss: 1011.1105346679688\n","Train -> sample/numSamples/epoch: 233/1182/3, Loss: 977.0121459960938\n","Train -> sample/numSamples/epoch: 234/1182/3, Loss: 686.1449584960938\n","Train -> sample/numSamples/epoch: 235/1182/3, Loss: 981.4376220703125\n","Train -> sample/numSamples/epoch: 236/1182/3, Loss: 737.80615234375\n","Train -> sample/numSamples/epoch: 237/1182/3, Loss: 624.6588745117188\n","Train -> sample/numSamples/epoch: 238/1182/3, Loss: 680.2498168945312\n","Train -> sample/numSamples/epoch: 239/1182/3, Loss: 839.5775756835938\n","Train -> sample/numSamples/epoch: 240/1182/3, Loss: 825.4248046875\n","Train -> sample/numSamples/epoch: 241/1182/3, Loss: 967.3831787109375\n","Train -> sample/numSamples/epoch: 242/1182/3, Loss: 1285.0494384765625\n","Train -> sample/numSamples/epoch: 243/1182/3, Loss: 879.334716796875\n","Train -> sample/numSamples/epoch: 244/1182/3, Loss: 1103.68408203125\n","Train -> sample/numSamples/epoch: 245/1182/3, Loss: 1316.948486328125\n","Train -> sample/numSamples/epoch: 246/1182/3, Loss: 628.9996337890625\n","Train -> sample/numSamples/epoch: 247/1182/3, Loss: 936.1571655273438\n","Train -> sample/numSamples/epoch: 248/1182/3, Loss: 1027.0567626953125\n","Train -> sample/numSamples/epoch: 249/1182/3, Loss: 806.2283935546875\n","Train -> sample/numSamples/epoch: 250/1182/3, Loss: 578.3143310546875\n","Train -> sample/numSamples/epoch: 251/1182/3, Loss: 1196.67626953125\n","Train -> sample/numSamples/epoch: 252/1182/3, Loss: 744.7105712890625\n","Train -> sample/numSamples/epoch: 253/1182/3, Loss: 603.0180053710938\n","imatge guardada\n","Train -> sample/numSamples/epoch: 254/1182/3, Loss: 872.3582763671875\n","Train -> sample/numSamples/epoch: 255/1182/3, Loss: 788.5101318359375\n","Train -> sample/numSamples/epoch: 256/1182/3, Loss: 1065.207275390625\n","Train -> sample/numSamples/epoch: 257/1182/3, Loss: 868.2744140625\n","Train -> sample/numSamples/epoch: 258/1182/3, Loss: 518.2841796875\n","Train -> sample/numSamples/epoch: 259/1182/3, Loss: 1581.427978515625\n","Train -> sample/numSamples/epoch: 260/1182/3, Loss: 846.8759765625\n","Train -> sample/numSamples/epoch: 261/1182/3, Loss: 896.35302734375\n","Train -> sample/numSamples/epoch: 262/1182/3, Loss: 894.108154296875\n","Train -> sample/numSamples/epoch: 263/1182/3, Loss: 972.5811767578125\n","Train -> sample/numSamples/epoch: 264/1182/3, Loss: 629.4622802734375\n","Train -> sample/numSamples/epoch: 265/1182/3, Loss: 1186.257080078125\n","Train -> sample/numSamples/epoch: 266/1182/3, Loss: 620.126953125\n","Train -> sample/numSamples/epoch: 267/1182/3, Loss: 717.3126831054688\n","Train -> sample/numSamples/epoch: 268/1182/3, Loss: 1012.8809204101562\n","Train -> sample/numSamples/epoch: 269/1182/3, Loss: 838.0670166015625\n","Train -> sample/numSamples/epoch: 270/1182/3, Loss: 948.8009033203125\n","Train -> sample/numSamples/epoch: 271/1182/3, Loss: 1027.7718505859375\n","Train -> sample/numSamples/epoch: 272/1182/3, Loss: 1836.5400390625\n","Train -> sample/numSamples/epoch: 273/1182/3, Loss: 1076.81884765625\n","Train -> sample/numSamples/epoch: 274/1182/3, Loss: 1258.4449462890625\n","Train -> sample/numSamples/epoch: 275/1182/3, Loss: 1105.528564453125\n","Train -> sample/numSamples/epoch: 276/1182/3, Loss: 1030.926513671875\n","Train -> sample/numSamples/epoch: 277/1182/3, Loss: 1316.2354736328125\n","Train -> sample/numSamples/epoch: 278/1182/3, Loss: 1334.3726806640625\n","Train -> sample/numSamples/epoch: 279/1182/3, Loss: 578.7579345703125\n","Train -> sample/numSamples/epoch: 280/1182/3, Loss: 894.481201171875\n","Train -> sample/numSamples/epoch: 281/1182/3, Loss: 974.377197265625\n","Train -> sample/numSamples/epoch: 282/1182/3, Loss: 816.980224609375\n","Train -> sample/numSamples/epoch: 283/1182/3, Loss: 1353.741455078125\n","Train -> sample/numSamples/epoch: 284/1182/3, Loss: 1620.7894287109375\n","Train -> sample/numSamples/epoch: 285/1182/3, Loss: 611.57958984375\n","Train -> sample/numSamples/epoch: 286/1182/3, Loss: 658.2711181640625\n","Train -> sample/numSamples/epoch: 287/1182/3, Loss: 818.47265625\n","Train -> sample/numSamples/epoch: 288/1182/3, Loss: 1077.8472900390625\n","Train -> sample/numSamples/epoch: 289/1182/3, Loss: 1186.4871826171875\n","Train -> sample/numSamples/epoch: 290/1182/3, Loss: 785.136962890625\n","Train -> sample/numSamples/epoch: 291/1182/3, Loss: 1005.5450439453125\n","Train -> sample/numSamples/epoch: 292/1182/3, Loss: 819.7333374023438\n","Train -> sample/numSamples/epoch: 293/1182/3, Loss: 970.2928466796875\n","Train -> sample/numSamples/epoch: 294/1182/3, Loss: 1026.980712890625\n","Train -> sample/numSamples/epoch: 295/1182/3, Loss: 682.1343383789062\n","Train -> sample/numSamples/epoch: 296/1182/3, Loss: 931.6083984375\n","Train -> sample/numSamples/epoch: 297/1182/3, Loss: 992.3099975585938\n","Train -> sample/numSamples/epoch: 298/1182/3, Loss: 718.4244995117188\n","Train -> sample/numSamples/epoch: 299/1182/3, Loss: 744.665771484375\n","Train -> sample/numSamples/epoch: 300/1182/3, Loss: 1179.220703125\n","Train -> sample/numSamples/epoch: 301/1182/3, Loss: 1216.46142578125\n","Train -> sample/numSamples/epoch: 302/1182/3, Loss: 655.7239379882812\n","Train -> sample/numSamples/epoch: 303/1182/3, Loss: 691.4636840820312\n","Train -> sample/numSamples/epoch: 304/1182/3, Loss: 925.8307495117188\n","Train -> sample/numSamples/epoch: 305/1182/3, Loss: 1235.35107421875\n","Train -> sample/numSamples/epoch: 306/1182/3, Loss: 911.17333984375\n","Train -> sample/numSamples/epoch: 307/1182/3, Loss: 947.3972778320312\n","Train -> sample/numSamples/epoch: 308/1182/3, Loss: 656.8936767578125\n","Train -> sample/numSamples/epoch: 309/1182/3, Loss: 1002.8740234375\n","Train -> sample/numSamples/epoch: 310/1182/3, Loss: 1557.020751953125\n","Train -> sample/numSamples/epoch: 311/1182/3, Loss: 1077.8006591796875\n","Train -> sample/numSamples/epoch: 312/1182/3, Loss: 708.1593017578125\n","Train -> sample/numSamples/epoch: 313/1182/3, Loss: 802.9865112304688\n","Train -> sample/numSamples/epoch: 314/1182/3, Loss: 820.22900390625\n","Train -> sample/numSamples/epoch: 315/1182/3, Loss: 720.6895141601562\n","Train -> sample/numSamples/epoch: 316/1182/3, Loss: 1050.6844482421875\n","Train -> sample/numSamples/epoch: 317/1182/3, Loss: 1424.6778564453125\n","Train -> sample/numSamples/epoch: 318/1182/3, Loss: 499.8174743652344\n","Train -> sample/numSamples/epoch: 319/1182/3, Loss: 1253.8284912109375\n","Train -> sample/numSamples/epoch: 320/1182/3, Loss: 1071.802734375\n","Train -> sample/numSamples/epoch: 321/1182/3, Loss: 1844.7117919921875\n","Train -> sample/numSamples/epoch: 322/1182/3, Loss: 916.79931640625\n","Train -> sample/numSamples/epoch: 323/1182/3, Loss: 1008.3456420898438\n","Train -> sample/numSamples/epoch: 324/1182/3, Loss: 665.5648193359375\n","Train -> sample/numSamples/epoch: 325/1182/3, Loss: 736.9701538085938\n","Train -> sample/numSamples/epoch: 326/1182/3, Loss: 1157.67919921875\n","Train -> sample/numSamples/epoch: 327/1182/3, Loss: 1154.9957275390625\n","Train -> sample/numSamples/epoch: 328/1182/3, Loss: 1401.5726318359375\n","Train -> sample/numSamples/epoch: 329/1182/3, Loss: 979.6372680664062\n","Train -> sample/numSamples/epoch: 330/1182/3, Loss: 1129.8360595703125\n","Train -> sample/numSamples/epoch: 331/1182/3, Loss: 866.5577392578125\n","Train -> sample/numSamples/epoch: 332/1182/3, Loss: 757.6163940429688\n","Train -> sample/numSamples/epoch: 333/1182/3, Loss: 1204.5521240234375\n","Train -> sample/numSamples/epoch: 334/1182/3, Loss: 485.25531005859375\n","Train -> sample/numSamples/epoch: 335/1182/3, Loss: 1149.90234375\n","Train -> sample/numSamples/epoch: 336/1182/3, Loss: 949.848876953125\n","Train -> sample/numSamples/epoch: 337/1182/3, Loss: 1175.04150390625\n","Train -> sample/numSamples/epoch: 338/1182/3, Loss: 765.100341796875\n","Train -> sample/numSamples/epoch: 339/1182/3, Loss: 1328.9356689453125\n","Train -> sample/numSamples/epoch: 340/1182/3, Loss: 1247.5758056640625\n","Train -> sample/numSamples/epoch: 341/1182/3, Loss: 545.8190307617188\n","Train -> sample/numSamples/epoch: 342/1182/3, Loss: 875.6668090820312\n","Train -> sample/numSamples/epoch: 343/1182/3, Loss: 770.1455688476562\n","Train -> sample/numSamples/epoch: 344/1182/3, Loss: 1479.0025634765625\n","Train -> sample/numSamples/epoch: 345/1182/3, Loss: 705.2830810546875\n","Train -> sample/numSamples/epoch: 346/1182/3, Loss: 805.1889038085938\n","Train -> sample/numSamples/epoch: 347/1182/3, Loss: 604.8215942382812\n","Train -> sample/numSamples/epoch: 348/1182/3, Loss: 991.0787353515625\n","Train -> sample/numSamples/epoch: 349/1182/3, Loss: 822.530517578125\n","Train -> sample/numSamples/epoch: 350/1182/3, Loss: 1344.129638671875\n","Train -> sample/numSamples/epoch: 351/1182/3, Loss: 866.640380859375\n","Train -> sample/numSamples/epoch: 352/1182/3, Loss: 966.5432739257812\n","Train -> sample/numSamples/epoch: 353/1182/3, Loss: 1227.6649169921875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 354/1182/3, Loss: 786.885498046875\n","Train -> sample/numSamples/epoch: 355/1182/3, Loss: 1021.00634765625\n","Train -> sample/numSamples/epoch: 356/1182/3, Loss: 763.6356811523438\n","Train -> sample/numSamples/epoch: 357/1182/3, Loss: 946.019287109375\n","Train -> sample/numSamples/epoch: 358/1182/3, Loss: 1083.4176025390625\n","Train -> sample/numSamples/epoch: 359/1182/3, Loss: 918.1708374023438\n","Train -> sample/numSamples/epoch: 360/1182/3, Loss: 655.0618286132812\n","Train -> sample/numSamples/epoch: 361/1182/3, Loss: 838.6302490234375\n","Train -> sample/numSamples/epoch: 362/1182/3, Loss: 822.2899169921875\n","Train -> sample/numSamples/epoch: 363/1182/3, Loss: 914.7514038085938\n","Train -> sample/numSamples/epoch: 364/1182/3, Loss: 1229.654541015625\n","Train -> sample/numSamples/epoch: 365/1182/3, Loss: 700.6310424804688\n","Train -> sample/numSamples/epoch: 366/1182/3, Loss: 831.8052978515625\n","Train -> sample/numSamples/epoch: 367/1182/3, Loss: 1129.147216796875\n","Train -> sample/numSamples/epoch: 368/1182/3, Loss: 732.841552734375\n","Train -> sample/numSamples/epoch: 369/1182/3, Loss: 878.5064086914062\n","Train -> sample/numSamples/epoch: 370/1182/3, Loss: 743.9586791992188\n","Train -> sample/numSamples/epoch: 371/1182/3, Loss: 729.4214477539062\n","Train -> sample/numSamples/epoch: 372/1182/3, Loss: 1173.1883544921875\n","Train -> sample/numSamples/epoch: 373/1182/3, Loss: 1063.8028564453125\n","Train -> sample/numSamples/epoch: 374/1182/3, Loss: 751.4402465820312\n","Train -> sample/numSamples/epoch: 375/1182/3, Loss: 818.7422485351562\n","Train -> sample/numSamples/epoch: 376/1182/3, Loss: 937.2520141601562\n","Train -> sample/numSamples/epoch: 377/1182/3, Loss: 1059.667724609375\n","Train -> sample/numSamples/epoch: 378/1182/3, Loss: 624.9320068359375\n","Train -> sample/numSamples/epoch: 379/1182/3, Loss: 1070.69384765625\n","Train -> sample/numSamples/epoch: 380/1182/3, Loss: 927.7472534179688\n","Train -> sample/numSamples/epoch: 381/1182/3, Loss: 933.7606811523438\n","Train -> sample/numSamples/epoch: 382/1182/3, Loss: 1379.2935791015625\n","Train -> sample/numSamples/epoch: 383/1182/3, Loss: 1126.985107421875\n","Train -> sample/numSamples/epoch: 384/1182/3, Loss: 1050.49072265625\n","Train -> sample/numSamples/epoch: 385/1182/3, Loss: 1069.17333984375\n","Train -> sample/numSamples/epoch: 386/1182/3, Loss: 664.011474609375\n","Train -> sample/numSamples/epoch: 387/1182/3, Loss: 859.7067260742188\n","Train -> sample/numSamples/epoch: 388/1182/3, Loss: 932.9844360351562\n","Train -> sample/numSamples/epoch: 389/1182/3, Loss: 602.1180419921875\n","Train -> sample/numSamples/epoch: 390/1182/3, Loss: 1198.18603515625\n","Train -> sample/numSamples/epoch: 391/1182/3, Loss: 1000.982177734375\n","Train -> sample/numSamples/epoch: 392/1182/3, Loss: 1082.1007080078125\n","Train -> sample/numSamples/epoch: 393/1182/3, Loss: 752.774169921875\n","Train -> sample/numSamples/epoch: 394/1182/3, Loss: 1041.13330078125\n","Train -> sample/numSamples/epoch: 395/1182/3, Loss: 1344.279541015625\n","Train -> sample/numSamples/epoch: 396/1182/3, Loss: 1058.8399658203125\n","Train -> sample/numSamples/epoch: 397/1182/3, Loss: 899.1687622070312\n","Train -> sample/numSamples/epoch: 398/1182/3, Loss: 1258.13134765625\n","Train -> sample/numSamples/epoch: 399/1182/3, Loss: 894.5396728515625\n","Train -> sample/numSamples/epoch: 400/1182/3, Loss: 660.2623291015625\n","Train -> sample/numSamples/epoch: 401/1182/3, Loss: 918.7503662109375\n","Train -> sample/numSamples/epoch: 402/1182/3, Loss: 1019.5669555664062\n","Train -> sample/numSamples/epoch: 403/1182/3, Loss: 908.2362670898438\n","Train -> sample/numSamples/epoch: 404/1182/3, Loss: 1072.2940673828125\n","Train -> sample/numSamples/epoch: 405/1182/3, Loss: 1412.4866943359375\n","Train -> sample/numSamples/epoch: 406/1182/3, Loss: 1019.0813598632812\n","Train -> sample/numSamples/epoch: 407/1182/3, Loss: 828.9109497070312\n","Train -> sample/numSamples/epoch: 408/1182/3, Loss: 983.2286987304688\n","Train -> sample/numSamples/epoch: 409/1182/3, Loss: 976.8382568359375\n","Train -> sample/numSamples/epoch: 410/1182/3, Loss: 845.6070556640625\n","Train -> sample/numSamples/epoch: 411/1182/3, Loss: 767.0182495117188\n","Train -> sample/numSamples/epoch: 412/1182/3, Loss: 1189.754638671875\n","Train -> sample/numSamples/epoch: 413/1182/3, Loss: 1207.37890625\n","Train -> sample/numSamples/epoch: 414/1182/3, Loss: 1131.423095703125\n","Train -> sample/numSamples/epoch: 415/1182/3, Loss: 1332.464111328125\n","Train -> sample/numSamples/epoch: 416/1182/3, Loss: 948.630126953125\n","Train -> sample/numSamples/epoch: 417/1182/3, Loss: 1378.5611572265625\n","Train -> sample/numSamples/epoch: 418/1182/3, Loss: 1429.40966796875\n","Train -> sample/numSamples/epoch: 419/1182/3, Loss: 1208.9227294921875\n","Train -> sample/numSamples/epoch: 420/1182/3, Loss: 1018.2689819335938\n","Train -> sample/numSamples/epoch: 421/1182/3, Loss: 925.8978881835938\n","Train -> sample/numSamples/epoch: 422/1182/3, Loss: 963.3467407226562\n","Train -> sample/numSamples/epoch: 423/1182/3, Loss: 1054.169677734375\n","Train -> sample/numSamples/epoch: 424/1182/3, Loss: 1107.322509765625\n","Train -> sample/numSamples/epoch: 425/1182/3, Loss: 1369.31591796875\n","Train -> sample/numSamples/epoch: 426/1182/3, Loss: 1115.7633056640625\n","Train -> sample/numSamples/epoch: 427/1182/3, Loss: 559.2996826171875\n","Train -> sample/numSamples/epoch: 428/1182/3, Loss: 1126.95361328125\n","Train -> sample/numSamples/epoch: 429/1182/3, Loss: 1136.255859375\n","Train -> sample/numSamples/epoch: 430/1182/3, Loss: 619.3848266601562\n","Train -> sample/numSamples/epoch: 431/1182/3, Loss: 544.8692626953125\n","Train -> sample/numSamples/epoch: 432/1182/3, Loss: 708.4061889648438\n","Train -> sample/numSamples/epoch: 433/1182/3, Loss: 876.779541015625\n","Train -> sample/numSamples/epoch: 434/1182/3, Loss: 961.363037109375\n","Train -> sample/numSamples/epoch: 435/1182/3, Loss: 745.4424438476562\n","Train -> sample/numSamples/epoch: 436/1182/3, Loss: 1265.9769287109375\n","Train -> sample/numSamples/epoch: 437/1182/3, Loss: 822.3528442382812\n","Train -> sample/numSamples/epoch: 438/1182/3, Loss: 957.4888916015625\n","Train -> sample/numSamples/epoch: 439/1182/3, Loss: 747.0404052734375\n","Train -> sample/numSamples/epoch: 440/1182/3, Loss: 783.592041015625\n","Train -> sample/numSamples/epoch: 441/1182/3, Loss: 899.1471557617188\n","Train -> sample/numSamples/epoch: 442/1182/3, Loss: 892.0436401367188\n","Train -> sample/numSamples/epoch: 443/1182/3, Loss: 595.67041015625\n","Train -> sample/numSamples/epoch: 444/1182/3, Loss: 1206.370361328125\n","Train -> sample/numSamples/epoch: 445/1182/3, Loss: 762.0053100585938\n","Train -> sample/numSamples/epoch: 446/1182/3, Loss: 869.53759765625\n","Train -> sample/numSamples/epoch: 447/1182/3, Loss: 1201.9500732421875\n","Train -> sample/numSamples/epoch: 448/1182/3, Loss: 664.16259765625\n","Train -> sample/numSamples/epoch: 449/1182/3, Loss: 1178.197021484375\n","Train -> sample/numSamples/epoch: 450/1182/3, Loss: 916.1463012695312\n","Train -> sample/numSamples/epoch: 451/1182/3, Loss: 921.5043334960938\n","Train -> sample/numSamples/epoch: 452/1182/3, Loss: 944.8336181640625\n","Train -> sample/numSamples/epoch: 453/1182/3, Loss: 1005.3607177734375\n","imatge guardada\n","Train -> sample/numSamples/epoch: 454/1182/3, Loss: 1199.32568359375\n","Train -> sample/numSamples/epoch: 455/1182/3, Loss: 1549.393798828125\n","Train -> sample/numSamples/epoch: 456/1182/3, Loss: 709.9154663085938\n","Train -> sample/numSamples/epoch: 457/1182/3, Loss: 1568.8701171875\n","Train -> sample/numSamples/epoch: 458/1182/3, Loss: 1213.748046875\n","Train -> sample/numSamples/epoch: 459/1182/3, Loss: 1033.6036376953125\n","Train -> sample/numSamples/epoch: 460/1182/3, Loss: 1228.6124267578125\n","Train -> sample/numSamples/epoch: 461/1182/3, Loss: 1169.526123046875\n","Train -> sample/numSamples/epoch: 462/1182/3, Loss: 795.5487060546875\n","Train -> sample/numSamples/epoch: 463/1182/3, Loss: 1271.4051513671875\n","Train -> sample/numSamples/epoch: 464/1182/3, Loss: 668.5488891601562\n","Train -> sample/numSamples/epoch: 465/1182/3, Loss: 857.6619262695312\n","Train -> sample/numSamples/epoch: 466/1182/3, Loss: 644.5323486328125\n","Train -> sample/numSamples/epoch: 467/1182/3, Loss: 985.2479858398438\n","Train -> sample/numSamples/epoch: 468/1182/3, Loss: 1048.0419921875\n","Train -> sample/numSamples/epoch: 469/1182/3, Loss: 983.8480834960938\n","Train -> sample/numSamples/epoch: 470/1182/3, Loss: 1131.535400390625\n","Train -> sample/numSamples/epoch: 471/1182/3, Loss: 1218.88818359375\n","Train -> sample/numSamples/epoch: 472/1182/3, Loss: 628.7449951171875\n","Train -> sample/numSamples/epoch: 473/1182/3, Loss: 996.3064575195312\n","Train -> sample/numSamples/epoch: 474/1182/3, Loss: 1011.183349609375\n","Train -> sample/numSamples/epoch: 475/1182/3, Loss: 1276.4967041015625\n","Train -> sample/numSamples/epoch: 476/1182/3, Loss: 1137.875\n","Train -> sample/numSamples/epoch: 477/1182/3, Loss: 891.983154296875\n","Train -> sample/numSamples/epoch: 478/1182/3, Loss: 1134.69580078125\n","Train -> sample/numSamples/epoch: 479/1182/3, Loss: 887.3096313476562\n","Train -> sample/numSamples/epoch: 480/1182/3, Loss: 947.5316162109375\n","Train -> sample/numSamples/epoch: 481/1182/3, Loss: 1362.9830322265625\n","Train -> sample/numSamples/epoch: 482/1182/3, Loss: 1307.1365966796875\n","Train -> sample/numSamples/epoch: 483/1182/3, Loss: 897.1395874023438\n","Train -> sample/numSamples/epoch: 484/1182/3, Loss: 1325.15869140625\n","Train -> sample/numSamples/epoch: 485/1182/3, Loss: 1431.6929931640625\n","Train -> sample/numSamples/epoch: 486/1182/3, Loss: 985.9086303710938\n","Train -> sample/numSamples/epoch: 487/1182/3, Loss: 844.9666137695312\n","Train -> sample/numSamples/epoch: 488/1182/3, Loss: 997.3659057617188\n","Train -> sample/numSamples/epoch: 489/1182/3, Loss: 816.0006103515625\n","Train -> sample/numSamples/epoch: 490/1182/3, Loss: 934.2218017578125\n","Train -> sample/numSamples/epoch: 491/1182/3, Loss: 909.571044921875\n","Train -> sample/numSamples/epoch: 492/1182/3, Loss: 1266.9677734375\n","Train -> sample/numSamples/epoch: 493/1182/3, Loss: 831.1826171875\n","Train -> sample/numSamples/epoch: 494/1182/3, Loss: 1183.18017578125\n","Train -> sample/numSamples/epoch: 495/1182/3, Loss: 671.3833618164062\n","Train -> sample/numSamples/epoch: 496/1182/3, Loss: 1139.7203369140625\n","Train -> sample/numSamples/epoch: 497/1182/3, Loss: 1305.717529296875\n","Train -> sample/numSamples/epoch: 498/1182/3, Loss: 1296.2681884765625\n","Train -> sample/numSamples/epoch: 499/1182/3, Loss: 1344.7705078125\n","Train -> sample/numSamples/epoch: 500/1182/3, Loss: 1250.6229248046875\n","Train -> sample/numSamples/epoch: 501/1182/3, Loss: 941.3767700195312\n","Train -> sample/numSamples/epoch: 502/1182/3, Loss: 1176.0908203125\n","Train -> sample/numSamples/epoch: 503/1182/3, Loss: 1606.7474365234375\n","Train -> sample/numSamples/epoch: 504/1182/3, Loss: 690.3091430664062\n","Train -> sample/numSamples/epoch: 505/1182/3, Loss: 1051.7001953125\n","Train -> sample/numSamples/epoch: 506/1182/3, Loss: 729.4686279296875\n","Train -> sample/numSamples/epoch: 507/1182/3, Loss: 1838.0577392578125\n","Train -> sample/numSamples/epoch: 508/1182/3, Loss: 1506.56103515625\n","Train -> sample/numSamples/epoch: 509/1182/3, Loss: 856.6962280273438\n","Train -> sample/numSamples/epoch: 510/1182/3, Loss: 797.94921875\n","Train -> sample/numSamples/epoch: 511/1182/3, Loss: 955.925048828125\n","Train -> sample/numSamples/epoch: 512/1182/3, Loss: 1060.6005859375\n","Train -> sample/numSamples/epoch: 513/1182/3, Loss: 1516.1669921875\n","Train -> sample/numSamples/epoch: 514/1182/3, Loss: 1269.288818359375\n","Train -> sample/numSamples/epoch: 515/1182/3, Loss: 1102.382568359375\n","Train -> sample/numSamples/epoch: 516/1182/3, Loss: 1070.0162353515625\n","Train -> sample/numSamples/epoch: 517/1182/3, Loss: 1016.1635131835938\n","Train -> sample/numSamples/epoch: 518/1182/3, Loss: 944.7753295898438\n","Train -> sample/numSamples/epoch: 519/1182/3, Loss: 1106.0133056640625\n","Train -> sample/numSamples/epoch: 520/1182/3, Loss: 668.8643188476562\n","Train -> sample/numSamples/epoch: 521/1182/3, Loss: 842.7997436523438\n","Train -> sample/numSamples/epoch: 522/1182/3, Loss: 743.91259765625\n","Train -> sample/numSamples/epoch: 523/1182/3, Loss: 738.6659545898438\n","Train -> sample/numSamples/epoch: 524/1182/3, Loss: 1171.627197265625\n","Train -> sample/numSamples/epoch: 525/1182/3, Loss: 548.18359375\n","Train -> sample/numSamples/epoch: 526/1182/3, Loss: 962.8173828125\n","Train -> sample/numSamples/epoch: 527/1182/3, Loss: 881.9442138671875\n","Train -> sample/numSamples/epoch: 528/1182/3, Loss: 603.9302368164062\n","Train -> sample/numSamples/epoch: 529/1182/3, Loss: 976.7109985351562\n","Train -> sample/numSamples/epoch: 530/1182/3, Loss: 1225.306396484375\n","Train -> sample/numSamples/epoch: 531/1182/3, Loss: 787.108154296875\n","Train -> sample/numSamples/epoch: 532/1182/3, Loss: 1224.087890625\n","Train -> sample/numSamples/epoch: 533/1182/3, Loss: 675.6377563476562\n","Train -> sample/numSamples/epoch: 534/1182/3, Loss: 1400.22998046875\n","Train -> sample/numSamples/epoch: 535/1182/3, Loss: 1414.048828125\n","Train -> sample/numSamples/epoch: 536/1182/3, Loss: 1393.5047607421875\n","Train -> sample/numSamples/epoch: 537/1182/3, Loss: 895.8271484375\n","Train -> sample/numSamples/epoch: 538/1182/3, Loss: 1092.074951171875\n","Train -> sample/numSamples/epoch: 539/1182/3, Loss: 625.4368286132812\n","Train -> sample/numSamples/epoch: 540/1182/3, Loss: 1134.548828125\n","Train -> sample/numSamples/epoch: 541/1182/3, Loss: 1166.6190185546875\n","Train -> sample/numSamples/epoch: 542/1182/3, Loss: 995.1162109375\n","Train -> sample/numSamples/epoch: 543/1182/3, Loss: 1307.0406494140625\n","Train -> sample/numSamples/epoch: 544/1182/3, Loss: 1285.7120361328125\n","Train -> sample/numSamples/epoch: 545/1182/3, Loss: 1295.25732421875\n","Train -> sample/numSamples/epoch: 546/1182/3, Loss: 961.2431640625\n","Train -> sample/numSamples/epoch: 547/1182/3, Loss: 1447.53125\n","Train -> sample/numSamples/epoch: 548/1182/3, Loss: 1197.91845703125\n","Train -> sample/numSamples/epoch: 549/1182/3, Loss: 936.1021118164062\n","Train -> sample/numSamples/epoch: 550/1182/3, Loss: 1293.680908203125\n","Train -> sample/numSamples/epoch: 551/1182/3, Loss: 1173.119384765625\n","Train -> sample/numSamples/epoch: 552/1182/3, Loss: 1485.714599609375\n","Train -> sample/numSamples/epoch: 553/1182/3, Loss: 1623.6163330078125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 554/1182/3, Loss: 684.2112426757812\n","Train -> sample/numSamples/epoch: 555/1182/3, Loss: 766.2396850585938\n","Train -> sample/numSamples/epoch: 556/1182/3, Loss: 1232.503173828125\n","Train -> sample/numSamples/epoch: 557/1182/3, Loss: 924.9608154296875\n","Train -> sample/numSamples/epoch: 558/1182/3, Loss: 978.6195068359375\n","Train -> sample/numSamples/epoch: 559/1182/3, Loss: 1382.7593994140625\n","Train -> sample/numSamples/epoch: 560/1182/3, Loss: 941.8243408203125\n","Train -> sample/numSamples/epoch: 561/1182/3, Loss: 906.6536865234375\n","Train -> sample/numSamples/epoch: 562/1182/3, Loss: 1197.946044921875\n","Train -> sample/numSamples/epoch: 563/1182/3, Loss: 899.52099609375\n","Train -> sample/numSamples/epoch: 564/1182/3, Loss: 1440.197509765625\n","Train -> sample/numSamples/epoch: 565/1182/3, Loss: 1372.5802001953125\n","Train -> sample/numSamples/epoch: 566/1182/3, Loss: 755.0457763671875\n","Train -> sample/numSamples/epoch: 567/1182/3, Loss: 978.2890625\n","Train -> sample/numSamples/epoch: 568/1182/3, Loss: 488.8055725097656\n","Train -> sample/numSamples/epoch: 569/1182/3, Loss: 1289.424560546875\n","Train -> sample/numSamples/epoch: 570/1182/3, Loss: 911.5971069335938\n","Train -> sample/numSamples/epoch: 571/1182/3, Loss: 765.4288940429688\n","Train -> sample/numSamples/epoch: 572/1182/3, Loss: 1411.284912109375\n","Train -> sample/numSamples/epoch: 573/1182/3, Loss: 1267.10888671875\n","Train -> sample/numSamples/epoch: 574/1182/3, Loss: 983.5993041992188\n","Train -> sample/numSamples/epoch: 575/1182/3, Loss: 1027.4644775390625\n","Train -> sample/numSamples/epoch: 576/1182/3, Loss: 889.1971435546875\n","Train -> sample/numSamples/epoch: 577/1182/3, Loss: 779.0059204101562\n","Train -> sample/numSamples/epoch: 578/1182/3, Loss: 1096.366455078125\n","Train -> sample/numSamples/epoch: 579/1182/3, Loss: 1227.7972412109375\n","Train -> sample/numSamples/epoch: 580/1182/3, Loss: 1000.55224609375\n","Train -> sample/numSamples/epoch: 581/1182/3, Loss: 1239.758056640625\n","Train -> sample/numSamples/epoch: 582/1182/3, Loss: 1252.4114990234375\n","Train -> sample/numSamples/epoch: 583/1182/3, Loss: 934.5469360351562\n","Train -> sample/numSamples/epoch: 584/1182/3, Loss: 776.114501953125\n","Train -> sample/numSamples/epoch: 585/1182/3, Loss: 819.1902465820312\n","Train -> sample/numSamples/epoch: 586/1182/3, Loss: 430.16058349609375\n","Train -> sample/numSamples/epoch: 587/1182/3, Loss: 1071.621337890625\n","Train -> sample/numSamples/epoch: 588/1182/3, Loss: 592.9448852539062\n","Train -> sample/numSamples/epoch: 589/1182/3, Loss: 1597.046630859375\n","Train -> sample/numSamples/epoch: 590/1182/3, Loss: 732.4049072265625\n","Train -> sample/numSamples/epoch: 591/1182/3, Loss: 941.7831420898438\n","Train -> sample/numSamples/epoch: 592/1182/3, Loss: 1051.80615234375\n","Train -> sample/numSamples/epoch: 593/1182/3, Loss: 1202.5050048828125\n","Train -> sample/numSamples/epoch: 594/1182/3, Loss: 1291.8165283203125\n","Train -> sample/numSamples/epoch: 595/1182/3, Loss: 1436.49609375\n","Train -> sample/numSamples/epoch: 596/1182/3, Loss: 1221.306884765625\n","Train -> sample/numSamples/epoch: 597/1182/3, Loss: 920.8321533203125\n","Train -> sample/numSamples/epoch: 598/1182/3, Loss: 959.7720947265625\n","Train -> sample/numSamples/epoch: 599/1182/3, Loss: 1171.935302734375\n","Train -> sample/numSamples/epoch: 600/1182/3, Loss: 873.1686401367188\n","Train -> sample/numSamples/epoch: 601/1182/3, Loss: 999.8335571289062\n","Train -> sample/numSamples/epoch: 602/1182/3, Loss: 1159.819580078125\n","Train -> sample/numSamples/epoch: 603/1182/3, Loss: 1029.839599609375\n","Train -> sample/numSamples/epoch: 604/1182/3, Loss: 1154.210693359375\n","Train -> sample/numSamples/epoch: 605/1182/3, Loss: 825.4755249023438\n","Train -> sample/numSamples/epoch: 606/1182/3, Loss: 1249.1082763671875\n","Train -> sample/numSamples/epoch: 607/1182/3, Loss: 1387.4119873046875\n","Train -> sample/numSamples/epoch: 608/1182/3, Loss: 647.8887939453125\n","Train -> sample/numSamples/epoch: 609/1182/3, Loss: 596.3056640625\n","Train -> sample/numSamples/epoch: 610/1182/3, Loss: 1097.072021484375\n","Train -> sample/numSamples/epoch: 611/1182/3, Loss: 1280.843505859375\n","Train -> sample/numSamples/epoch: 612/1182/3, Loss: 1058.801025390625\n","Train -> sample/numSamples/epoch: 613/1182/3, Loss: 947.8499755859375\n","Train -> sample/numSamples/epoch: 614/1182/3, Loss: 994.0087890625\n","Train -> sample/numSamples/epoch: 615/1182/3, Loss: 1104.51904296875\n","Train -> sample/numSamples/epoch: 616/1182/3, Loss: 1039.68310546875\n","Train -> sample/numSamples/epoch: 617/1182/3, Loss: 728.5430297851562\n","Train -> sample/numSamples/epoch: 618/1182/3, Loss: 1137.683349609375\n","Train -> sample/numSamples/epoch: 619/1182/3, Loss: 1097.451416015625\n","Train -> sample/numSamples/epoch: 620/1182/3, Loss: 1309.258056640625\n","Train -> sample/numSamples/epoch: 621/1182/3, Loss: 939.8502197265625\n","Train -> sample/numSamples/epoch: 622/1182/3, Loss: 818.7722778320312\n","Train -> sample/numSamples/epoch: 623/1182/3, Loss: 1363.689453125\n","Train -> sample/numSamples/epoch: 624/1182/3, Loss: 856.1319580078125\n","Train -> sample/numSamples/epoch: 625/1182/3, Loss: 1283.4608154296875\n","Train -> sample/numSamples/epoch: 626/1182/3, Loss: 1486.396484375\n","Train -> sample/numSamples/epoch: 627/1182/3, Loss: 1127.32177734375\n","Train -> sample/numSamples/epoch: 628/1182/3, Loss: 1201.0775146484375\n","Train -> sample/numSamples/epoch: 629/1182/3, Loss: 1206.837890625\n","Train -> sample/numSamples/epoch: 630/1182/3, Loss: 1319.52197265625\n","Train -> sample/numSamples/epoch: 631/1182/3, Loss: 1355.9466552734375\n","Train -> sample/numSamples/epoch: 632/1182/3, Loss: 807.4624633789062\n","Train -> sample/numSamples/epoch: 633/1182/3, Loss: 772.0437622070312\n","Train -> sample/numSamples/epoch: 634/1182/3, Loss: 1013.7496337890625\n","Train -> sample/numSamples/epoch: 635/1182/3, Loss: 668.824462890625\n","Train -> sample/numSamples/epoch: 636/1182/3, Loss: 883.65478515625\n","Train -> sample/numSamples/epoch: 637/1182/3, Loss: 925.154052734375\n","Train -> sample/numSamples/epoch: 638/1182/3, Loss: 1048.33837890625\n","Train -> sample/numSamples/epoch: 639/1182/3, Loss: 631.7832641601562\n","Train -> sample/numSamples/epoch: 640/1182/3, Loss: 953.0439453125\n","Train -> sample/numSamples/epoch: 641/1182/3, Loss: 1017.3504028320312\n","Train -> sample/numSamples/epoch: 642/1182/3, Loss: 1203.5869140625\n","Train -> sample/numSamples/epoch: 643/1182/3, Loss: 1113.671875\n","Train -> sample/numSamples/epoch: 644/1182/3, Loss: 1000.5247192382812\n","Train -> sample/numSamples/epoch: 645/1182/3, Loss: 697.552490234375\n","Train -> sample/numSamples/epoch: 646/1182/3, Loss: 1049.5487060546875\n","Train -> sample/numSamples/epoch: 647/1182/3, Loss: 793.263671875\n","Train -> sample/numSamples/epoch: 648/1182/3, Loss: 915.593505859375\n","Train -> sample/numSamples/epoch: 649/1182/3, Loss: 1256.980712890625\n","Train -> sample/numSamples/epoch: 650/1182/3, Loss: 1146.805419921875\n","Train -> sample/numSamples/epoch: 651/1182/3, Loss: 1207.390869140625\n","Train -> sample/numSamples/epoch: 652/1182/3, Loss: 1011.8302612304688\n","Train -> sample/numSamples/epoch: 653/1182/3, Loss: 717.2205200195312\n","imatge guardada\n","Train -> sample/numSamples/epoch: 654/1182/3, Loss: 1299.5958251953125\n","Train -> sample/numSamples/epoch: 655/1182/3, Loss: 870.9387817382812\n","Train -> sample/numSamples/epoch: 656/1182/3, Loss: 877.7993774414062\n","Train -> sample/numSamples/epoch: 657/1182/3, Loss: 1022.9155883789062\n","Train -> sample/numSamples/epoch: 658/1182/3, Loss: 716.3712768554688\n","Train -> sample/numSamples/epoch: 659/1182/3, Loss: 1205.2227783203125\n","Train -> sample/numSamples/epoch: 660/1182/3, Loss: 1178.01416015625\n","Train -> sample/numSamples/epoch: 661/1182/3, Loss: 840.2438354492188\n","Train -> sample/numSamples/epoch: 662/1182/3, Loss: 1314.0106201171875\n","Train -> sample/numSamples/epoch: 663/1182/3, Loss: 933.8751220703125\n","Train -> sample/numSamples/epoch: 664/1182/3, Loss: 821.918701171875\n","Train -> sample/numSamples/epoch: 665/1182/3, Loss: 845.1626586914062\n","Train -> sample/numSamples/epoch: 666/1182/3, Loss: 969.8809814453125\n","Train -> sample/numSamples/epoch: 667/1182/3, Loss: 1081.7620849609375\n","Train -> sample/numSamples/epoch: 668/1182/3, Loss: 1024.10205078125\n","Train -> sample/numSamples/epoch: 669/1182/3, Loss: 988.4957275390625\n","Train -> sample/numSamples/epoch: 670/1182/3, Loss: 1368.6898193359375\n","Train -> sample/numSamples/epoch: 671/1182/3, Loss: 1263.1883544921875\n","Train -> sample/numSamples/epoch: 672/1182/3, Loss: 1022.6572265625\n","Train -> sample/numSamples/epoch: 673/1182/3, Loss: 1004.0800170898438\n","Train -> sample/numSamples/epoch: 674/1182/3, Loss: 1357.74462890625\n","Train -> sample/numSamples/epoch: 675/1182/3, Loss: 1099.546875\n","Train -> sample/numSamples/epoch: 676/1182/3, Loss: 1213.4576416015625\n","Train -> sample/numSamples/epoch: 677/1182/3, Loss: 1488.375\n","Train -> sample/numSamples/epoch: 678/1182/3, Loss: 1140.1072998046875\n","Train -> sample/numSamples/epoch: 679/1182/3, Loss: 960.8446044921875\n","Train -> sample/numSamples/epoch: 680/1182/3, Loss: 674.6121215820312\n","Train -> sample/numSamples/epoch: 681/1182/3, Loss: 998.6790771484375\n","Train -> sample/numSamples/epoch: 682/1182/3, Loss: 1099.7108154296875\n","Train -> sample/numSamples/epoch: 683/1182/3, Loss: 1327.643798828125\n","Train -> sample/numSamples/epoch: 684/1182/3, Loss: 1125.2711181640625\n","Train -> sample/numSamples/epoch: 685/1182/3, Loss: 1203.215087890625\n","Train -> sample/numSamples/epoch: 686/1182/3, Loss: 1556.8035888671875\n","Train -> sample/numSamples/epoch: 687/1182/3, Loss: 1158.652587890625\n","Train -> sample/numSamples/epoch: 688/1182/3, Loss: 1053.514404296875\n","Train -> sample/numSamples/epoch: 689/1182/3, Loss: 1194.0732421875\n","Train -> sample/numSamples/epoch: 690/1182/3, Loss: 1605.7255859375\n","Train -> sample/numSamples/epoch: 691/1182/3, Loss: 644.8533325195312\n","Train -> sample/numSamples/epoch: 692/1182/3, Loss: 694.4473876953125\n","Train -> sample/numSamples/epoch: 693/1182/3, Loss: 774.9182739257812\n","Train -> sample/numSamples/epoch: 694/1182/3, Loss: 568.9730224609375\n","Train -> sample/numSamples/epoch: 695/1182/3, Loss: 954.4717407226562\n","Train -> sample/numSamples/epoch: 696/1182/3, Loss: 1034.8792724609375\n","Train -> sample/numSamples/epoch: 697/1182/3, Loss: 925.3545532226562\n","Train -> sample/numSamples/epoch: 698/1182/3, Loss: 1503.87109375\n","Train -> sample/numSamples/epoch: 699/1182/3, Loss: 1137.4532470703125\n","Train -> sample/numSamples/epoch: 700/1182/3, Loss: 1005.373046875\n","Train -> sample/numSamples/epoch: 701/1182/3, Loss: 1182.3831787109375\n","Train -> sample/numSamples/epoch: 702/1182/3, Loss: 1133.4471435546875\n","Train -> sample/numSamples/epoch: 703/1182/3, Loss: 1338.064208984375\n","Train -> sample/numSamples/epoch: 704/1182/3, Loss: 1052.5692138671875\n","Train -> sample/numSamples/epoch: 705/1182/3, Loss: 1015.2105712890625\n","Train -> sample/numSamples/epoch: 706/1182/3, Loss: 935.5960693359375\n","Train -> sample/numSamples/epoch: 707/1182/3, Loss: 1037.1451416015625\n","Train -> sample/numSamples/epoch: 708/1182/3, Loss: 1148.450439453125\n","Train -> sample/numSamples/epoch: 709/1182/3, Loss: 1577.330078125\n","Train -> sample/numSamples/epoch: 710/1182/3, Loss: 1464.7113037109375\n","Train -> sample/numSamples/epoch: 711/1182/3, Loss: 1154.4932861328125\n","Train -> sample/numSamples/epoch: 712/1182/3, Loss: 1487.429443359375\n","Train -> sample/numSamples/epoch: 713/1182/3, Loss: 962.1170654296875\n","Train -> sample/numSamples/epoch: 714/1182/3, Loss: 1473.5748291015625\n","Train -> sample/numSamples/epoch: 715/1182/3, Loss: 1112.0194091796875\n","Train -> sample/numSamples/epoch: 716/1182/3, Loss: 1190.080810546875\n","Train -> sample/numSamples/epoch: 717/1182/3, Loss: 1176.4959716796875\n","Train -> sample/numSamples/epoch: 718/1182/3, Loss: 962.150390625\n","Train -> sample/numSamples/epoch: 719/1182/3, Loss: 1026.253662109375\n","Train -> sample/numSamples/epoch: 720/1182/3, Loss: 1272.4595947265625\n","Train -> sample/numSamples/epoch: 721/1182/3, Loss: 636.3872680664062\n","Train -> sample/numSamples/epoch: 722/1182/3, Loss: 1506.1822509765625\n","Train -> sample/numSamples/epoch: 723/1182/3, Loss: 1038.806884765625\n","Train -> sample/numSamples/epoch: 724/1182/3, Loss: 1252.4239501953125\n","Train -> sample/numSamples/epoch: 725/1182/3, Loss: 763.7234497070312\n","Train -> sample/numSamples/epoch: 726/1182/3, Loss: 626.1160888671875\n","Train -> sample/numSamples/epoch: 727/1182/3, Loss: 931.5249633789062\n","Train -> sample/numSamples/epoch: 728/1182/3, Loss: 821.6304931640625\n","Train -> sample/numSamples/epoch: 729/1182/3, Loss: 809.8414916992188\n","Train -> sample/numSamples/epoch: 730/1182/3, Loss: 844.6625366210938\n","Train -> sample/numSamples/epoch: 731/1182/3, Loss: 713.497802734375\n","Train -> sample/numSamples/epoch: 732/1182/3, Loss: 924.6200561523438\n","Train -> sample/numSamples/epoch: 733/1182/3, Loss: 1080.4541015625\n","Train -> sample/numSamples/epoch: 734/1182/3, Loss: 1135.238525390625\n","Train -> sample/numSamples/epoch: 735/1182/3, Loss: 1202.7252197265625\n","Train -> sample/numSamples/epoch: 736/1182/3, Loss: 962.435302734375\n","Train -> sample/numSamples/epoch: 737/1182/3, Loss: 938.33642578125\n","Train -> sample/numSamples/epoch: 738/1182/3, Loss: 1228.34326171875\n","Train -> sample/numSamples/epoch: 739/1182/3, Loss: 1521.1207275390625\n","Train -> sample/numSamples/epoch: 740/1182/3, Loss: 649.8663940429688\n","Train -> sample/numSamples/epoch: 741/1182/3, Loss: 1167.65576171875\n","Train -> sample/numSamples/epoch: 742/1182/3, Loss: 583.8665161132812\n","Train -> sample/numSamples/epoch: 743/1182/3, Loss: 778.8353271484375\n","Train -> sample/numSamples/epoch: 744/1182/3, Loss: 947.6896362304688\n","Train -> sample/numSamples/epoch: 745/1182/3, Loss: 885.5557861328125\n","Train -> sample/numSamples/epoch: 746/1182/3, Loss: 1289.5224609375\n","Train -> sample/numSamples/epoch: 747/1182/3, Loss: 744.9359130859375\n","Train -> sample/numSamples/epoch: 748/1182/3, Loss: 856.4810791015625\n","Train -> sample/numSamples/epoch: 749/1182/3, Loss: 896.6345825195312\n","Train -> sample/numSamples/epoch: 750/1182/3, Loss: 1377.71337890625\n","Train -> sample/numSamples/epoch: 751/1182/3, Loss: 933.2728271484375\n","Train -> sample/numSamples/epoch: 752/1182/3, Loss: 1033.794677734375\n","Train -> sample/numSamples/epoch: 753/1182/3, Loss: 1029.0565185546875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 754/1182/3, Loss: 1338.796630859375\n","Train -> sample/numSamples/epoch: 755/1182/3, Loss: 1155.47802734375\n","Train -> sample/numSamples/epoch: 756/1182/3, Loss: 387.3638000488281\n","Train -> sample/numSamples/epoch: 757/1182/3, Loss: 735.053466796875\n","Train -> sample/numSamples/epoch: 758/1182/3, Loss: 760.4033203125\n","Train -> sample/numSamples/epoch: 759/1182/3, Loss: 1133.3876953125\n","Train -> sample/numSamples/epoch: 760/1182/3, Loss: 694.7152099609375\n","Train -> sample/numSamples/epoch: 761/1182/3, Loss: 851.2174072265625\n","Train -> sample/numSamples/epoch: 762/1182/3, Loss: 1160.694091796875\n","Train -> sample/numSamples/epoch: 763/1182/3, Loss: 684.8170776367188\n","Train -> sample/numSamples/epoch: 764/1182/3, Loss: 702.1043090820312\n","Train -> sample/numSamples/epoch: 765/1182/3, Loss: 855.5927124023438\n","Train -> sample/numSamples/epoch: 766/1182/3, Loss: 1000.91064453125\n","Train -> sample/numSamples/epoch: 767/1182/3, Loss: 1056.480712890625\n","Train -> sample/numSamples/epoch: 768/1182/3, Loss: 1160.2293701171875\n","Train -> sample/numSamples/epoch: 769/1182/3, Loss: 1025.7225341796875\n","Train -> sample/numSamples/epoch: 770/1182/3, Loss: 792.6265258789062\n","Train -> sample/numSamples/epoch: 771/1182/3, Loss: 756.1088256835938\n","Train -> sample/numSamples/epoch: 772/1182/3, Loss: 1123.604248046875\n","Train -> sample/numSamples/epoch: 773/1182/3, Loss: 1229.373046875\n","Train -> sample/numSamples/epoch: 774/1182/3, Loss: 1040.4864501953125\n","Train -> sample/numSamples/epoch: 775/1182/3, Loss: 1109.157470703125\n","Train -> sample/numSamples/epoch: 776/1182/3, Loss: 1195.91748046875\n","Train -> sample/numSamples/epoch: 777/1182/3, Loss: 946.8150024414062\n","Train -> sample/numSamples/epoch: 778/1182/3, Loss: 908.5291748046875\n","Train -> sample/numSamples/epoch: 779/1182/3, Loss: 1050.9095458984375\n","Train -> sample/numSamples/epoch: 780/1182/3, Loss: 1234.1158447265625\n","Train -> sample/numSamples/epoch: 781/1182/3, Loss: 1119.7413330078125\n","Train -> sample/numSamples/epoch: 782/1182/3, Loss: 903.7632446289062\n","Train -> sample/numSamples/epoch: 783/1182/3, Loss: 1092.5321044921875\n","Train -> sample/numSamples/epoch: 784/1182/3, Loss: 1101.549560546875\n","Train -> sample/numSamples/epoch: 785/1182/3, Loss: 835.7192993164062\n","Train -> sample/numSamples/epoch: 786/1182/3, Loss: 1249.5950927734375\n","Train -> sample/numSamples/epoch: 787/1182/3, Loss: 585.833984375\n","Train -> sample/numSamples/epoch: 788/1182/3, Loss: 764.19775390625\n","Train -> sample/numSamples/epoch: 789/1182/3, Loss: 762.9346923828125\n","Train -> sample/numSamples/epoch: 790/1182/3, Loss: 1067.380615234375\n","Train -> sample/numSamples/epoch: 791/1182/3, Loss: 898.65087890625\n","Train -> sample/numSamples/epoch: 792/1182/3, Loss: 1117.0833740234375\n","Train -> sample/numSamples/epoch: 793/1182/3, Loss: 813.042724609375\n","Train -> sample/numSamples/epoch: 794/1182/3, Loss: 1089.0975341796875\n","Train -> sample/numSamples/epoch: 795/1182/3, Loss: 1274.4649658203125\n","Train -> sample/numSamples/epoch: 796/1182/3, Loss: 1115.531005859375\n","Train -> sample/numSamples/epoch: 797/1182/3, Loss: 927.3744506835938\n","Train -> sample/numSamples/epoch: 798/1182/3, Loss: 1228.2017822265625\n","Train -> sample/numSamples/epoch: 799/1182/3, Loss: 1373.653076171875\n","Train -> sample/numSamples/epoch: 800/1182/3, Loss: 943.9348754882812\n","Train -> sample/numSamples/epoch: 801/1182/3, Loss: 1056.0726318359375\n","Train -> sample/numSamples/epoch: 802/1182/3, Loss: 813.7141723632812\n","Train -> sample/numSamples/epoch: 803/1182/3, Loss: 835.6793823242188\n","Train -> sample/numSamples/epoch: 804/1182/3, Loss: 1508.06396484375\n","Train -> sample/numSamples/epoch: 805/1182/3, Loss: 1197.0758056640625\n","Train -> sample/numSamples/epoch: 806/1182/3, Loss: 755.940673828125\n","Train -> sample/numSamples/epoch: 807/1182/3, Loss: 909.189208984375\n","Train -> sample/numSamples/epoch: 808/1182/3, Loss: 763.098388671875\n","Train -> sample/numSamples/epoch: 809/1182/3, Loss: 1524.3238525390625\n","Train -> sample/numSamples/epoch: 810/1182/3, Loss: 866.146484375\n","Train -> sample/numSamples/epoch: 811/1182/3, Loss: 918.2904663085938\n","Train -> sample/numSamples/epoch: 812/1182/3, Loss: 800.37646484375\n","Train -> sample/numSamples/epoch: 813/1182/3, Loss: 614.60986328125\n","Train -> sample/numSamples/epoch: 814/1182/3, Loss: 1228.2635498046875\n","Train -> sample/numSamples/epoch: 815/1182/3, Loss: 484.0131530761719\n","Train -> sample/numSamples/epoch: 816/1182/3, Loss: 1313.67626953125\n","Train -> sample/numSamples/epoch: 817/1182/3, Loss: 971.959228515625\n","Train -> sample/numSamples/epoch: 818/1182/3, Loss: 1148.674072265625\n","Train -> sample/numSamples/epoch: 819/1182/3, Loss: 878.0824584960938\n","Train -> sample/numSamples/epoch: 820/1182/3, Loss: 1101.1064453125\n","Train -> sample/numSamples/epoch: 821/1182/3, Loss: 1178.2239990234375\n","Train -> sample/numSamples/epoch: 822/1182/3, Loss: 1185.6483154296875\n","Train -> sample/numSamples/epoch: 823/1182/3, Loss: 1046.9256591796875\n","Train -> sample/numSamples/epoch: 824/1182/3, Loss: 1094.64111328125\n","Train -> sample/numSamples/epoch: 825/1182/3, Loss: 914.1373901367188\n","Train -> sample/numSamples/epoch: 826/1182/3, Loss: 878.5599365234375\n","Train -> sample/numSamples/epoch: 827/1182/3, Loss: 857.8853149414062\n","Train -> sample/numSamples/epoch: 828/1182/3, Loss: 920.848388671875\n","Train -> sample/numSamples/epoch: 829/1182/3, Loss: 1091.5159912109375\n","Train -> sample/numSamples/epoch: 830/1182/3, Loss: 872.4054565429688\n","Train -> sample/numSamples/epoch: 831/1182/3, Loss: 1028.6954345703125\n","Train -> sample/numSamples/epoch: 832/1182/3, Loss: 484.0710144042969\n","Train -> sample/numSamples/epoch: 833/1182/3, Loss: 715.5645141601562\n","Train -> sample/numSamples/epoch: 834/1182/3, Loss: 1357.0224609375\n","Train -> sample/numSamples/epoch: 835/1182/3, Loss: 1014.4977416992188\n","Train -> sample/numSamples/epoch: 836/1182/3, Loss: 1047.0535888671875\n","Train -> sample/numSamples/epoch: 837/1182/3, Loss: 1230.19677734375\n","Train -> sample/numSamples/epoch: 838/1182/3, Loss: 1037.4720458984375\n","Train -> sample/numSamples/epoch: 839/1182/3, Loss: 672.7272338867188\n","Train -> sample/numSamples/epoch: 840/1182/3, Loss: 1642.4169921875\n","Train -> sample/numSamples/epoch: 841/1182/3, Loss: 1219.9854736328125\n","Train -> sample/numSamples/epoch: 842/1182/3, Loss: 1049.0963134765625\n","Train -> sample/numSamples/epoch: 843/1182/3, Loss: 673.930419921875\n","Train -> sample/numSamples/epoch: 844/1182/3, Loss: 772.3854370117188\n","Train -> sample/numSamples/epoch: 845/1182/3, Loss: 919.2456665039062\n","Train -> sample/numSamples/epoch: 846/1182/3, Loss: 905.0277709960938\n","Train -> sample/numSamples/epoch: 847/1182/3, Loss: 940.7828979492188\n","Train -> sample/numSamples/epoch: 848/1182/3, Loss: 968.970703125\n","Train -> sample/numSamples/epoch: 849/1182/3, Loss: 446.2812805175781\n","Train -> sample/numSamples/epoch: 850/1182/3, Loss: 1235.7330322265625\n","Train -> sample/numSamples/epoch: 851/1182/3, Loss: 644.9005126953125\n","Train -> sample/numSamples/epoch: 852/1182/3, Loss: 898.4317016601562\n","Train -> sample/numSamples/epoch: 853/1182/3, Loss: 838.645751953125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 854/1182/3, Loss: 1401.674560546875\n","Train -> sample/numSamples/epoch: 855/1182/3, Loss: 952.1367797851562\n","Train -> sample/numSamples/epoch: 856/1182/3, Loss: 716.0936279296875\n","Train -> sample/numSamples/epoch: 857/1182/3, Loss: 1102.910888671875\n","Train -> sample/numSamples/epoch: 858/1182/3, Loss: 1003.1333618164062\n","Train -> sample/numSamples/epoch: 859/1182/3, Loss: 993.0399780273438\n","Train -> sample/numSamples/epoch: 860/1182/3, Loss: 1151.765869140625\n","Train -> sample/numSamples/epoch: 861/1182/3, Loss: 1387.796875\n","Train -> sample/numSamples/epoch: 862/1182/3, Loss: 1037.885498046875\n","Train -> sample/numSamples/epoch: 863/1182/3, Loss: 732.7344970703125\n","Train -> sample/numSamples/epoch: 864/1182/3, Loss: 1198.2738037109375\n","Train -> sample/numSamples/epoch: 865/1182/3, Loss: 1144.7115478515625\n","Train -> sample/numSamples/epoch: 866/1182/3, Loss: 1157.329345703125\n","Train -> sample/numSamples/epoch: 867/1182/3, Loss: 703.8941040039062\n","Train -> sample/numSamples/epoch: 868/1182/3, Loss: 592.2013549804688\n","Train -> sample/numSamples/epoch: 869/1182/3, Loss: 1037.91162109375\n","Train -> sample/numSamples/epoch: 870/1182/3, Loss: 974.3091430664062\n","Train -> sample/numSamples/epoch: 871/1182/3, Loss: 752.5733642578125\n","Train -> sample/numSamples/epoch: 872/1182/3, Loss: 1726.063720703125\n","Train -> sample/numSamples/epoch: 873/1182/3, Loss: 949.1380004882812\n","Train -> sample/numSamples/epoch: 874/1182/3, Loss: 1087.430908203125\n","Train -> sample/numSamples/epoch: 875/1182/3, Loss: 1051.28466796875\n","Train -> sample/numSamples/epoch: 876/1182/3, Loss: 1129.1458740234375\n","Train -> sample/numSamples/epoch: 877/1182/3, Loss: 720.1171264648438\n","Train -> sample/numSamples/epoch: 878/1182/3, Loss: 1282.11328125\n","Train -> sample/numSamples/epoch: 879/1182/3, Loss: 850.1242065429688\n","Train -> sample/numSamples/epoch: 880/1182/3, Loss: 879.3416137695312\n","Train -> sample/numSamples/epoch: 881/1182/3, Loss: 1450.5731201171875\n","Train -> sample/numSamples/epoch: 882/1182/3, Loss: 1337.57958984375\n","Train -> sample/numSamples/epoch: 883/1182/3, Loss: 1011.1087036132812\n","Train -> sample/numSamples/epoch: 884/1182/3, Loss: 1232.876953125\n","Train -> sample/numSamples/epoch: 885/1182/3, Loss: 631.16748046875\n","Train -> sample/numSamples/epoch: 886/1182/3, Loss: 1197.4261474609375\n","Train -> sample/numSamples/epoch: 887/1182/3, Loss: 1235.8770751953125\n","Train -> sample/numSamples/epoch: 888/1182/3, Loss: 1113.6583251953125\n","Train -> sample/numSamples/epoch: 889/1182/3, Loss: 1192.3809814453125\n","Train -> sample/numSamples/epoch: 890/1182/3, Loss: 1083.0343017578125\n","Train -> sample/numSamples/epoch: 891/1182/3, Loss: 1092.35107421875\n","Train -> sample/numSamples/epoch: 892/1182/3, Loss: 949.61865234375\n","Train -> sample/numSamples/epoch: 893/1182/3, Loss: 975.4078979492188\n","Train -> sample/numSamples/epoch: 894/1182/3, Loss: 951.4646606445312\n","Train -> sample/numSamples/epoch: 895/1182/3, Loss: 936.822021484375\n","Train -> sample/numSamples/epoch: 896/1182/3, Loss: 876.543212890625\n","Train -> sample/numSamples/epoch: 897/1182/3, Loss: 1093.4869384765625\n","Train -> sample/numSamples/epoch: 898/1182/3, Loss: 854.3380126953125\n","Train -> sample/numSamples/epoch: 899/1182/3, Loss: 1013.521728515625\n","Train -> sample/numSamples/epoch: 900/1182/3, Loss: 739.7089233398438\n","Train -> sample/numSamples/epoch: 901/1182/3, Loss: 744.3327026367188\n","Train -> sample/numSamples/epoch: 902/1182/3, Loss: 1153.1241455078125\n","Train -> sample/numSamples/epoch: 903/1182/3, Loss: 864.9074096679688\n","Train -> sample/numSamples/epoch: 904/1182/3, Loss: 511.4668884277344\n","Train -> sample/numSamples/epoch: 905/1182/3, Loss: 911.7471923828125\n","Train -> sample/numSamples/epoch: 906/1182/3, Loss: 882.8108520507812\n","Train -> sample/numSamples/epoch: 907/1182/3, Loss: 627.0078735351562\n","Train -> sample/numSamples/epoch: 908/1182/3, Loss: 1409.8409423828125\n","Train -> sample/numSamples/epoch: 909/1182/3, Loss: 1175.6180419921875\n","Train -> sample/numSamples/epoch: 910/1182/3, Loss: 1201.656982421875\n","Train -> sample/numSamples/epoch: 911/1182/3, Loss: 936.544921875\n","Train -> sample/numSamples/epoch: 912/1182/3, Loss: 830.6268310546875\n","Train -> sample/numSamples/epoch: 913/1182/3, Loss: 811.2296142578125\n","Train -> sample/numSamples/epoch: 914/1182/3, Loss: 831.0780639648438\n","Train -> sample/numSamples/epoch: 915/1182/3, Loss: 951.0888671875\n","Train -> sample/numSamples/epoch: 916/1182/3, Loss: 1022.2379150390625\n","Train -> sample/numSamples/epoch: 917/1182/3, Loss: 1281.8802490234375\n","Train -> sample/numSamples/epoch: 918/1182/3, Loss: 850.9501342773438\n","Train -> sample/numSamples/epoch: 919/1182/3, Loss: 1325.4881591796875\n","Train -> sample/numSamples/epoch: 920/1182/3, Loss: 1178.087890625\n","Train -> sample/numSamples/epoch: 921/1182/3, Loss: 1057.7086181640625\n","Train -> sample/numSamples/epoch: 922/1182/3, Loss: 1189.778564453125\n","Train -> sample/numSamples/epoch: 923/1182/3, Loss: 619.3929443359375\n","Train -> sample/numSamples/epoch: 924/1182/3, Loss: 1231.6724853515625\n","Train -> sample/numSamples/epoch: 925/1182/3, Loss: 1071.659423828125\n","Train -> sample/numSamples/epoch: 926/1182/3, Loss: 1024.8702392578125\n","Train -> sample/numSamples/epoch: 927/1182/3, Loss: 1112.403076171875\n","Train -> sample/numSamples/epoch: 928/1182/3, Loss: 938.98095703125\n","Train -> sample/numSamples/epoch: 929/1182/3, Loss: 1074.5360107421875\n","Train -> sample/numSamples/epoch: 930/1182/3, Loss: 568.4412231445312\n","Train -> sample/numSamples/epoch: 931/1182/3, Loss: 1169.199951171875\n","Train -> sample/numSamples/epoch: 932/1182/3, Loss: 732.6910400390625\n","Train -> sample/numSamples/epoch: 933/1182/3, Loss: 836.4400634765625\n","Train -> sample/numSamples/epoch: 934/1182/3, Loss: 901.62060546875\n","Train -> sample/numSamples/epoch: 935/1182/3, Loss: 1271.2120361328125\n","Train -> sample/numSamples/epoch: 936/1182/3, Loss: 1506.174560546875\n","Train -> sample/numSamples/epoch: 937/1182/3, Loss: 667.8282470703125\n","Train -> sample/numSamples/epoch: 938/1182/3, Loss: 816.8716430664062\n","Train -> sample/numSamples/epoch: 939/1182/3, Loss: 1176.416748046875\n","Train -> sample/numSamples/epoch: 940/1182/3, Loss: 949.3169555664062\n","Train -> sample/numSamples/epoch: 941/1182/3, Loss: 1243.43017578125\n","Train -> sample/numSamples/epoch: 942/1182/3, Loss: 869.5504760742188\n","Train -> sample/numSamples/epoch: 943/1182/3, Loss: 1214.526123046875\n","Train -> sample/numSamples/epoch: 944/1182/3, Loss: 1405.211181640625\n","Train -> sample/numSamples/epoch: 945/1182/3, Loss: 1098.8411865234375\n","Train -> sample/numSamples/epoch: 946/1182/3, Loss: 860.5437622070312\n","Train -> sample/numSamples/epoch: 947/1182/3, Loss: 953.370849609375\n","Train -> sample/numSamples/epoch: 948/1182/3, Loss: 1223.2850341796875\n","Train -> sample/numSamples/epoch: 949/1182/3, Loss: 1147.213623046875\n","Train -> sample/numSamples/epoch: 950/1182/3, Loss: 787.787841796875\n","Train -> sample/numSamples/epoch: 951/1182/3, Loss: 1164.54052734375\n","Train -> sample/numSamples/epoch: 952/1182/3, Loss: 920.6028442382812\n","Train -> sample/numSamples/epoch: 953/1182/3, Loss: 615.7144775390625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 954/1182/3, Loss: 761.8036499023438\n","Train -> sample/numSamples/epoch: 955/1182/3, Loss: 1206.4881591796875\n","Train -> sample/numSamples/epoch: 956/1182/3, Loss: 1074.930908203125\n","Train -> sample/numSamples/epoch: 957/1182/3, Loss: 1168.6385498046875\n","Train -> sample/numSamples/epoch: 958/1182/3, Loss: 904.860595703125\n","Train -> sample/numSamples/epoch: 959/1182/3, Loss: 1110.8001708984375\n","Train -> sample/numSamples/epoch: 960/1182/3, Loss: 1089.955322265625\n","Train -> sample/numSamples/epoch: 961/1182/3, Loss: 975.4302978515625\n","Train -> sample/numSamples/epoch: 962/1182/3, Loss: 1296.4681396484375\n","Train -> sample/numSamples/epoch: 963/1182/3, Loss: 1472.1539306640625\n","Train -> sample/numSamples/epoch: 964/1182/3, Loss: 1236.0943603515625\n","Train -> sample/numSamples/epoch: 965/1182/3, Loss: 931.8270874023438\n","Train -> sample/numSamples/epoch: 966/1182/3, Loss: 1215.2933349609375\n","Train -> sample/numSamples/epoch: 967/1182/3, Loss: 1174.4822998046875\n","Train -> sample/numSamples/epoch: 968/1182/3, Loss: 1214.236572265625\n","Train -> sample/numSamples/epoch: 969/1182/3, Loss: 961.0638427734375\n","Train -> sample/numSamples/epoch: 970/1182/3, Loss: 871.0498046875\n","Train -> sample/numSamples/epoch: 971/1182/3, Loss: 1007.0477905273438\n","Train -> sample/numSamples/epoch: 972/1182/3, Loss: 692.4324340820312\n","Train -> sample/numSamples/epoch: 973/1182/3, Loss: 1187.30517578125\n","Train -> sample/numSamples/epoch: 974/1182/3, Loss: 1153.835205078125\n","Train -> sample/numSamples/epoch: 975/1182/3, Loss: 1013.3870849609375\n","Train -> sample/numSamples/epoch: 976/1182/3, Loss: 1056.329833984375\n","Train -> sample/numSamples/epoch: 977/1182/3, Loss: 547.7900390625\n","Train -> sample/numSamples/epoch: 978/1182/3, Loss: 1403.021728515625\n","Train -> sample/numSamples/epoch: 979/1182/3, Loss: 1186.43505859375\n","Train -> sample/numSamples/epoch: 980/1182/3, Loss: 1302.941162109375\n","Train -> sample/numSamples/epoch: 981/1182/3, Loss: 1177.8626708984375\n","Train -> sample/numSamples/epoch: 982/1182/3, Loss: 1170.1458740234375\n","Train -> sample/numSamples/epoch: 983/1182/3, Loss: 1223.91259765625\n","Train -> sample/numSamples/epoch: 984/1182/3, Loss: 1223.1927490234375\n","Train -> sample/numSamples/epoch: 985/1182/3, Loss: 1231.73876953125\n","Train -> sample/numSamples/epoch: 986/1182/3, Loss: 1005.5895385742188\n","Train -> sample/numSamples/epoch: 987/1182/3, Loss: 924.9002685546875\n","Train -> sample/numSamples/epoch: 988/1182/3, Loss: 731.8253784179688\n","Train -> sample/numSamples/epoch: 989/1182/3, Loss: 889.4615478515625\n","Train -> sample/numSamples/epoch: 990/1182/3, Loss: 983.7301025390625\n","Train -> sample/numSamples/epoch: 991/1182/3, Loss: 884.1465454101562\n","Train -> sample/numSamples/epoch: 992/1182/3, Loss: 988.6860961914062\n","Train -> sample/numSamples/epoch: 993/1182/3, Loss: 641.577880859375\n","Train -> sample/numSamples/epoch: 994/1182/3, Loss: 528.577392578125\n","Train -> sample/numSamples/epoch: 995/1182/3, Loss: 1308.327880859375\n","Train -> sample/numSamples/epoch: 996/1182/3, Loss: 876.3038330078125\n","Train -> sample/numSamples/epoch: 997/1182/3, Loss: 1135.3612060546875\n","Train -> sample/numSamples/epoch: 998/1182/3, Loss: 848.2211303710938\n","Train -> sample/numSamples/epoch: 999/1182/3, Loss: 878.171875\n","Train -> sample/numSamples/epoch: 1000/1182/3, Loss: 1131.14501953125\n","Train -> sample/numSamples/epoch: 1001/1182/3, Loss: 1107.595458984375\n","Train -> sample/numSamples/epoch: 1002/1182/3, Loss: 687.497314453125\n","Train -> sample/numSamples/epoch: 1003/1182/3, Loss: 889.1378784179688\n","Train -> sample/numSamples/epoch: 1004/1182/3, Loss: 1122.260986328125\n","Train -> sample/numSamples/epoch: 1005/1182/3, Loss: 1116.895751953125\n","Train -> sample/numSamples/epoch: 1006/1182/3, Loss: 657.4221801757812\n","Train -> sample/numSamples/epoch: 1007/1182/3, Loss: 796.5363159179688\n","Train -> sample/numSamples/epoch: 1008/1182/3, Loss: 1287.54541015625\n","Train -> sample/numSamples/epoch: 1009/1182/3, Loss: 951.5658569335938\n","Train -> sample/numSamples/epoch: 1010/1182/3, Loss: 1227.7545166015625\n","Train -> sample/numSamples/epoch: 1011/1182/3, Loss: 893.6715087890625\n","Train -> sample/numSamples/epoch: 1012/1182/3, Loss: 1297.346923828125\n","Train -> sample/numSamples/epoch: 1013/1182/3, Loss: 743.4537963867188\n","Train -> sample/numSamples/epoch: 1014/1182/3, Loss: 1098.4844970703125\n","Train -> sample/numSamples/epoch: 1015/1182/3, Loss: 879.0960693359375\n","Train -> sample/numSamples/epoch: 1016/1182/3, Loss: 973.6627197265625\n","Train -> sample/numSamples/epoch: 1017/1182/3, Loss: 878.2334594726562\n","Train -> sample/numSamples/epoch: 1018/1182/3, Loss: 1119.5509033203125\n","Train -> sample/numSamples/epoch: 1019/1182/3, Loss: 1130.8631591796875\n","Train -> sample/numSamples/epoch: 1020/1182/3, Loss: 1216.408447265625\n","Train -> sample/numSamples/epoch: 1021/1182/3, Loss: 547.3978271484375\n","Train -> sample/numSamples/epoch: 1022/1182/3, Loss: 1050.117919921875\n","Train -> sample/numSamples/epoch: 1023/1182/3, Loss: 1128.90283203125\n","Train -> sample/numSamples/epoch: 1024/1182/3, Loss: 999.9530639648438\n","Train -> sample/numSamples/epoch: 1025/1182/3, Loss: 986.2910766601562\n","Train -> sample/numSamples/epoch: 1026/1182/3, Loss: 543.675537109375\n","Train -> sample/numSamples/epoch: 1027/1182/3, Loss: 870.1243286132812\n","Train -> sample/numSamples/epoch: 1028/1182/3, Loss: 1148.5462646484375\n","Train -> sample/numSamples/epoch: 1029/1182/3, Loss: 864.8516235351562\n","Train -> sample/numSamples/epoch: 1030/1182/3, Loss: 542.4511108398438\n","Train -> sample/numSamples/epoch: 1031/1182/3, Loss: 1424.6861572265625\n","Train -> sample/numSamples/epoch: 1032/1182/3, Loss: 919.4121704101562\n","Train -> sample/numSamples/epoch: 1033/1182/3, Loss: 1093.8353271484375\n","Train -> sample/numSamples/epoch: 1034/1182/3, Loss: 926.631591796875\n","Train -> sample/numSamples/epoch: 1035/1182/3, Loss: 1450.2384033203125\n","Train -> sample/numSamples/epoch: 1036/1182/3, Loss: 1369.8031005859375\n","Train -> sample/numSamples/epoch: 1037/1182/3, Loss: 1062.5806884765625\n","Train -> sample/numSamples/epoch: 1038/1182/3, Loss: 1267.0185546875\n","Train -> sample/numSamples/epoch: 1039/1182/3, Loss: 1294.75732421875\n","Train -> sample/numSamples/epoch: 1040/1182/3, Loss: 818.642333984375\n","Train -> sample/numSamples/epoch: 1041/1182/3, Loss: 933.911376953125\n","Train -> sample/numSamples/epoch: 1042/1182/3, Loss: 1113.8626708984375\n","Train -> sample/numSamples/epoch: 1043/1182/3, Loss: 1139.563232421875\n","Train -> sample/numSamples/epoch: 1044/1182/3, Loss: 1200.761474609375\n","Train -> sample/numSamples/epoch: 1045/1182/3, Loss: 1097.236328125\n","Train -> sample/numSamples/epoch: 1046/1182/3, Loss: 1251.796875\n","Train -> sample/numSamples/epoch: 1047/1182/3, Loss: 940.7178955078125\n","Train -> sample/numSamples/epoch: 1048/1182/3, Loss: 943.6365356445312\n","Train -> sample/numSamples/epoch: 1049/1182/3, Loss: 956.0072631835938\n","Train -> sample/numSamples/epoch: 1050/1182/3, Loss: 809.78955078125\n","Train -> sample/numSamples/epoch: 1051/1182/3, Loss: 804.697998046875\n","Train -> sample/numSamples/epoch: 1052/1182/3, Loss: 1116.8253173828125\n","Train -> sample/numSamples/epoch: 1053/1182/3, Loss: 1088.0972900390625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1054/1182/3, Loss: 1070.374755859375\n","Train -> sample/numSamples/epoch: 1055/1182/3, Loss: 1429.77294921875\n","Train -> sample/numSamples/epoch: 1056/1182/3, Loss: 586.6202392578125\n","Train -> sample/numSamples/epoch: 1057/1182/3, Loss: 1407.2542724609375\n","Train -> sample/numSamples/epoch: 1058/1182/3, Loss: 1213.868408203125\n","Train -> sample/numSamples/epoch: 1059/1182/3, Loss: 943.7448120117188\n","Train -> sample/numSamples/epoch: 1060/1182/3, Loss: 1680.6258544921875\n","Train -> sample/numSamples/epoch: 1061/1182/3, Loss: 853.1104736328125\n","Train -> sample/numSamples/epoch: 1062/1182/3, Loss: 1046.3662109375\n","Train -> sample/numSamples/epoch: 1063/1182/3, Loss: 1107.498779296875\n","Train -> sample/numSamples/epoch: 1064/1182/3, Loss: 936.0499877929688\n","Train -> sample/numSamples/epoch: 1065/1182/3, Loss: 1237.15283203125\n","Train -> sample/numSamples/epoch: 1066/1182/3, Loss: 734.55810546875\n","Train -> sample/numSamples/epoch: 1067/1182/3, Loss: 1217.62939453125\n","Train -> sample/numSamples/epoch: 1068/1182/3, Loss: 967.0545654296875\n","Train -> sample/numSamples/epoch: 1069/1182/3, Loss: 790.4185791015625\n","Train -> sample/numSamples/epoch: 1070/1182/3, Loss: 1113.868408203125\n","Train -> sample/numSamples/epoch: 1071/1182/3, Loss: 1218.1947021484375\n","Train -> sample/numSamples/epoch: 1072/1182/3, Loss: 928.2939453125\n","Train -> sample/numSamples/epoch: 1073/1182/3, Loss: 1091.1278076171875\n","Train -> sample/numSamples/epoch: 1074/1182/3, Loss: 962.6806030273438\n","Train -> sample/numSamples/epoch: 1075/1182/3, Loss: 1155.2947998046875\n","Train -> sample/numSamples/epoch: 1076/1182/3, Loss: 1542.7266845703125\n","Train -> sample/numSamples/epoch: 1077/1182/3, Loss: 1439.5103759765625\n","Train -> sample/numSamples/epoch: 1078/1182/3, Loss: 972.8624267578125\n","Train -> sample/numSamples/epoch: 1079/1182/3, Loss: 507.1441345214844\n","Train -> sample/numSamples/epoch: 1080/1182/3, Loss: 1303.5120849609375\n","Train -> sample/numSamples/epoch: 1081/1182/3, Loss: 830.2855224609375\n","Train -> sample/numSamples/epoch: 1082/1182/3, Loss: 1088.2091064453125\n","Train -> sample/numSamples/epoch: 1083/1182/3, Loss: 637.82080078125\n","Train -> sample/numSamples/epoch: 1084/1182/3, Loss: 1039.14599609375\n","Train -> sample/numSamples/epoch: 1085/1182/3, Loss: 994.8536987304688\n","Train -> sample/numSamples/epoch: 1086/1182/3, Loss: 861.6399536132812\n","Train -> sample/numSamples/epoch: 1087/1182/3, Loss: 885.0995483398438\n","Train -> sample/numSamples/epoch: 1088/1182/3, Loss: 1730.8126220703125\n","Train -> sample/numSamples/epoch: 1089/1182/3, Loss: 785.0281372070312\n","Train -> sample/numSamples/epoch: 1090/1182/3, Loss: 1085.7235107421875\n","Train -> sample/numSamples/epoch: 1091/1182/3, Loss: 931.6846923828125\n","Train -> sample/numSamples/epoch: 1092/1182/3, Loss: 563.6378173828125\n","Train -> sample/numSamples/epoch: 1093/1182/3, Loss: 630.7153930664062\n","Train -> sample/numSamples/epoch: 1094/1182/3, Loss: 1470.5423583984375\n","Train -> sample/numSamples/epoch: 1095/1182/3, Loss: 997.7352905273438\n","Train -> sample/numSamples/epoch: 1096/1182/3, Loss: 1031.9539794921875\n","Train -> sample/numSamples/epoch: 1097/1182/3, Loss: 967.7450561523438\n","Train -> sample/numSamples/epoch: 1098/1182/3, Loss: 1716.6055908203125\n","Train -> sample/numSamples/epoch: 1099/1182/3, Loss: 902.6771240234375\n","Train -> sample/numSamples/epoch: 1100/1182/3, Loss: 1131.560791015625\n","Train -> sample/numSamples/epoch: 1101/1182/3, Loss: 809.5635986328125\n","Train -> sample/numSamples/epoch: 1102/1182/3, Loss: 1403.69091796875\n","Train -> sample/numSamples/epoch: 1103/1182/3, Loss: 781.1046752929688\n","Train -> sample/numSamples/epoch: 1104/1182/3, Loss: 1088.523681640625\n","Train -> sample/numSamples/epoch: 1105/1182/3, Loss: 1020.6591796875\n","Train -> sample/numSamples/epoch: 1106/1182/3, Loss: 892.1008911132812\n","Train -> sample/numSamples/epoch: 1107/1182/3, Loss: 880.2080078125\n","Train -> sample/numSamples/epoch: 1108/1182/3, Loss: 1197.678955078125\n","Train -> sample/numSamples/epoch: 1109/1182/3, Loss: 573.4164428710938\n","Train -> sample/numSamples/epoch: 1110/1182/3, Loss: 1251.5657958984375\n","Train -> sample/numSamples/epoch: 1111/1182/3, Loss: 1215.9342041015625\n","Train -> sample/numSamples/epoch: 1112/1182/3, Loss: 779.766357421875\n","Train -> sample/numSamples/epoch: 1113/1182/3, Loss: 701.1798095703125\n","Train -> sample/numSamples/epoch: 1114/1182/3, Loss: 1106.822509765625\n","Train -> sample/numSamples/epoch: 1115/1182/3, Loss: 990.2254638671875\n","Train -> sample/numSamples/epoch: 1116/1182/3, Loss: 881.3173828125\n","Train -> sample/numSamples/epoch: 1117/1182/3, Loss: 582.541015625\n","Train -> sample/numSamples/epoch: 1118/1182/3, Loss: 1143.7755126953125\n","Train -> sample/numSamples/epoch: 1119/1182/3, Loss: 828.5410766601562\n","Train -> sample/numSamples/epoch: 1120/1182/3, Loss: 761.2664184570312\n","Train -> sample/numSamples/epoch: 1121/1182/3, Loss: 1089.6873779296875\n","Train -> sample/numSamples/epoch: 1122/1182/3, Loss: 1053.654296875\n","Train -> sample/numSamples/epoch: 1123/1182/3, Loss: 964.7689208984375\n","Train -> sample/numSamples/epoch: 1124/1182/3, Loss: 1152.4390869140625\n","Train -> sample/numSamples/epoch: 1125/1182/3, Loss: 1184.48193359375\n","Train -> sample/numSamples/epoch: 1126/1182/3, Loss: 860.9256591796875\n","Train -> sample/numSamples/epoch: 1127/1182/3, Loss: 577.8455810546875\n","Train -> sample/numSamples/epoch: 1128/1182/3, Loss: 405.8196105957031\n","Train -> sample/numSamples/epoch: 1129/1182/3, Loss: 1155.4483642578125\n","Train -> sample/numSamples/epoch: 1130/1182/3, Loss: 578.1768798828125\n","Train -> sample/numSamples/epoch: 1131/1182/3, Loss: 639.1578369140625\n","Train -> sample/numSamples/epoch: 1132/1182/3, Loss: 1206.9149169921875\n","Train -> sample/numSamples/epoch: 1133/1182/3, Loss: 1300.8135986328125\n","Train -> sample/numSamples/epoch: 1134/1182/3, Loss: 823.9065551757812\n","Train -> sample/numSamples/epoch: 1135/1182/3, Loss: 810.59130859375\n","Train -> sample/numSamples/epoch: 1136/1182/3, Loss: 845.0697631835938\n","Train -> sample/numSamples/epoch: 1137/1182/3, Loss: 821.9693603515625\n","Train -> sample/numSamples/epoch: 1138/1182/3, Loss: 994.8410034179688\n","Train -> sample/numSamples/epoch: 1139/1182/3, Loss: 999.3993530273438\n","Train -> sample/numSamples/epoch: 1140/1182/3, Loss: 719.5013427734375\n","Train -> sample/numSamples/epoch: 1141/1182/3, Loss: 949.1918334960938\n","Train -> sample/numSamples/epoch: 1142/1182/3, Loss: 1299.8736572265625\n","Train -> sample/numSamples/epoch: 1143/1182/3, Loss: 926.0484008789062\n","Train -> sample/numSamples/epoch: 1144/1182/3, Loss: 1190.737548828125\n","Train -> sample/numSamples/epoch: 1145/1182/3, Loss: 1021.9611206054688\n","Train -> sample/numSamples/epoch: 1146/1182/3, Loss: 941.9298706054688\n","Train -> sample/numSamples/epoch: 1147/1182/3, Loss: 1301.370849609375\n","Train -> sample/numSamples/epoch: 1148/1182/3, Loss: 948.6901245117188\n","Train -> sample/numSamples/epoch: 1149/1182/3, Loss: 1249.2373046875\n","Train -> sample/numSamples/epoch: 1150/1182/3, Loss: 802.5217895507812\n","Train -> sample/numSamples/epoch: 1151/1182/3, Loss: 1180.910888671875\n","Train -> sample/numSamples/epoch: 1152/1182/3, Loss: 918.1066284179688\n","Train -> sample/numSamples/epoch: 1153/1182/3, Loss: 776.5643310546875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1154/1182/3, Loss: 1168.74267578125\n","Train -> sample/numSamples/epoch: 1155/1182/3, Loss: 1398.0096435546875\n","Train -> sample/numSamples/epoch: 1156/1182/3, Loss: 1110.00439453125\n","Train -> sample/numSamples/epoch: 1157/1182/3, Loss: 1091.91552734375\n","Train -> sample/numSamples/epoch: 1158/1182/3, Loss: 976.3308715820312\n","Train -> sample/numSamples/epoch: 1159/1182/3, Loss: 1317.69384765625\n","Train -> sample/numSamples/epoch: 1160/1182/3, Loss: 1120.9072265625\n","Train -> sample/numSamples/epoch: 1161/1182/3, Loss: 1125.0521240234375\n","Train -> sample/numSamples/epoch: 1162/1182/3, Loss: 1137.02490234375\n","Train -> sample/numSamples/epoch: 1163/1182/3, Loss: 1048.8251953125\n","Train -> sample/numSamples/epoch: 1164/1182/3, Loss: 1108.16943359375\n","Train -> sample/numSamples/epoch: 1165/1182/3, Loss: 1051.4835205078125\n","Train -> sample/numSamples/epoch: 1166/1182/3, Loss: 926.780517578125\n","Train -> sample/numSamples/epoch: 1167/1182/3, Loss: 1235.1639404296875\n","Train -> sample/numSamples/epoch: 1168/1182/3, Loss: 673.2083740234375\n","Train -> sample/numSamples/epoch: 1169/1182/3, Loss: 1062.608154296875\n","Train -> sample/numSamples/epoch: 1170/1182/3, Loss: 1047.1048583984375\n","Train -> sample/numSamples/epoch: 1171/1182/3, Loss: 887.8147583007812\n","Train -> sample/numSamples/epoch: 1172/1182/3, Loss: 584.5577392578125\n","Train -> sample/numSamples/epoch: 1173/1182/3, Loss: 947.5852661132812\n","Train -> sample/numSamples/epoch: 1174/1182/3, Loss: 754.767333984375\n","Train -> sample/numSamples/epoch: 1175/1182/3, Loss: 846.5880126953125\n","Train -> sample/numSamples/epoch: 1176/1182/3, Loss: 1153.84619140625\n","Train -> sample/numSamples/epoch: 1177/1182/3, Loss: 1010.1701049804688\n","Train -> sample/numSamples/epoch: 1178/1182/3, Loss: 778.4461669921875\n","Train -> sample/numSamples/epoch: 1179/1182/3, Loss: 1129.2210693359375\n","Train -> sample/numSamples/epoch: 1180/1182/3, Loss: 914.2100219726562\n","Train -> sample/numSamples/epoch: 1181/1182/3, Loss: 1466.8033447265625\n","Train -> sample/numSamples/epoch: 0/1182/4, Loss: 974.1451416015625\n","Train -> sample/numSamples/epoch: 1/1182/4, Loss: 857.4334716796875\n","Train -> sample/numSamples/epoch: 2/1182/4, Loss: 1026.1845703125\n","Train -> sample/numSamples/epoch: 3/1182/4, Loss: 899.8760986328125\n","Train -> sample/numSamples/epoch: 4/1182/4, Loss: 1046.764404296875\n","Train -> sample/numSamples/epoch: 5/1182/4, Loss: 1113.04833984375\n","Train -> sample/numSamples/epoch: 6/1182/4, Loss: 935.0689086914062\n","Train -> sample/numSamples/epoch: 7/1182/4, Loss: 1027.908447265625\n","Train -> sample/numSamples/epoch: 8/1182/4, Loss: 829.3836059570312\n","Train -> sample/numSamples/epoch: 9/1182/4, Loss: 1638.9693603515625\n","Train -> sample/numSamples/epoch: 10/1182/4, Loss: 1181.8619384765625\n","Train -> sample/numSamples/epoch: 11/1182/4, Loss: 608.3854370117188\n","Train -> sample/numSamples/epoch: 12/1182/4, Loss: 773.6748046875\n","Train -> sample/numSamples/epoch: 13/1182/4, Loss: 905.3855590820312\n","Train -> sample/numSamples/epoch: 14/1182/4, Loss: 613.1539306640625\n","Train -> sample/numSamples/epoch: 15/1182/4, Loss: 1082.968505859375\n","Train -> sample/numSamples/epoch: 16/1182/4, Loss: 1003.9688110351562\n","Train -> sample/numSamples/epoch: 17/1182/4, Loss: 860.1568603515625\n","Train -> sample/numSamples/epoch: 18/1182/4, Loss: 839.5836791992188\n","Train -> sample/numSamples/epoch: 19/1182/4, Loss: 1053.104736328125\n","Train -> sample/numSamples/epoch: 20/1182/4, Loss: 1172.264892578125\n","Train -> sample/numSamples/epoch: 21/1182/4, Loss: 1206.4315185546875\n","Train -> sample/numSamples/epoch: 22/1182/4, Loss: 1334.74658203125\n","Train -> sample/numSamples/epoch: 23/1182/4, Loss: 608.9608764648438\n","Train -> sample/numSamples/epoch: 24/1182/4, Loss: 1068.8326416015625\n","Train -> sample/numSamples/epoch: 25/1182/4, Loss: 1225.3701171875\n","Train -> sample/numSamples/epoch: 26/1182/4, Loss: 1106.4561767578125\n","Train -> sample/numSamples/epoch: 27/1182/4, Loss: 1169.73291015625\n","Train -> sample/numSamples/epoch: 28/1182/4, Loss: 1005.8501586914062\n","Train -> sample/numSamples/epoch: 29/1182/4, Loss: 525.6795654296875\n","Train -> sample/numSamples/epoch: 30/1182/4, Loss: 967.7362060546875\n","Train -> sample/numSamples/epoch: 31/1182/4, Loss: 1000.4217529296875\n","Train -> sample/numSamples/epoch: 32/1182/4, Loss: 1139.598876953125\n","Train -> sample/numSamples/epoch: 33/1182/4, Loss: 921.3714599609375\n","Train -> sample/numSamples/epoch: 34/1182/4, Loss: 1370.3563232421875\n","Train -> sample/numSamples/epoch: 35/1182/4, Loss: 1035.7955322265625\n","Train -> sample/numSamples/epoch: 36/1182/4, Loss: 1176.393798828125\n","Train -> sample/numSamples/epoch: 37/1182/4, Loss: 637.5580444335938\n","Train -> sample/numSamples/epoch: 38/1182/4, Loss: 675.9715576171875\n","Train -> sample/numSamples/epoch: 39/1182/4, Loss: 1155.89208984375\n","Train -> sample/numSamples/epoch: 40/1182/4, Loss: 1171.937255859375\n","Train -> sample/numSamples/epoch: 41/1182/4, Loss: 627.3417358398438\n","Train -> sample/numSamples/epoch: 42/1182/4, Loss: 1305.833984375\n","Train -> sample/numSamples/epoch: 43/1182/4, Loss: 730.722412109375\n","Train -> sample/numSamples/epoch: 44/1182/4, Loss: 1078.070068359375\n","Train -> sample/numSamples/epoch: 45/1182/4, Loss: 859.7346801757812\n","Train -> sample/numSamples/epoch: 46/1182/4, Loss: 1087.94970703125\n","Train -> sample/numSamples/epoch: 47/1182/4, Loss: 725.799560546875\n","Train -> sample/numSamples/epoch: 48/1182/4, Loss: 988.371337890625\n","Train -> sample/numSamples/epoch: 49/1182/4, Loss: 827.9744262695312\n","Train -> sample/numSamples/epoch: 50/1182/4, Loss: 674.0966796875\n","Train -> sample/numSamples/epoch: 51/1182/4, Loss: 1020.4027099609375\n","Train -> sample/numSamples/epoch: 52/1182/4, Loss: 948.1161499023438\n","Train -> sample/numSamples/epoch: 53/1182/4, Loss: 918.922607421875\n","Train -> sample/numSamples/epoch: 54/1182/4, Loss: 741.4881591796875\n","Train -> sample/numSamples/epoch: 55/1182/4, Loss: 1292.2398681640625\n","Train -> sample/numSamples/epoch: 56/1182/4, Loss: 1441.043701171875\n","Train -> sample/numSamples/epoch: 57/1182/4, Loss: 1429.4940185546875\n","Train -> sample/numSamples/epoch: 58/1182/4, Loss: 952.3363647460938\n","Train -> sample/numSamples/epoch: 59/1182/4, Loss: 1386.9168701171875\n","Train -> sample/numSamples/epoch: 60/1182/4, Loss: 1021.1282348632812\n","Train -> sample/numSamples/epoch: 61/1182/4, Loss: 516.1046752929688\n","Train -> sample/numSamples/epoch: 62/1182/4, Loss: 1299.431640625\n","Train -> sample/numSamples/epoch: 63/1182/4, Loss: 1179.126220703125\n","Train -> sample/numSamples/epoch: 64/1182/4, Loss: 1093.781494140625\n","Train -> sample/numSamples/epoch: 65/1182/4, Loss: 1162.040283203125\n","Train -> sample/numSamples/epoch: 66/1182/4, Loss: 1357.4263916015625\n","Train -> sample/numSamples/epoch: 67/1182/4, Loss: 693.546142578125\n","Train -> sample/numSamples/epoch: 68/1182/4, Loss: 1007.5441284179688\n","Train -> sample/numSamples/epoch: 69/1182/4, Loss: 862.2566528320312\n","Train -> sample/numSamples/epoch: 70/1182/4, Loss: 1323.7296142578125\n","Train -> sample/numSamples/epoch: 71/1182/4, Loss: 462.032470703125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 72/1182/4, Loss: 962.345458984375\n","Train -> sample/numSamples/epoch: 73/1182/4, Loss: 825.5411987304688\n","Train -> sample/numSamples/epoch: 74/1182/4, Loss: 919.82373046875\n","Train -> sample/numSamples/epoch: 75/1182/4, Loss: 986.9749145507812\n","Train -> sample/numSamples/epoch: 76/1182/4, Loss: 878.2052612304688\n","Train -> sample/numSamples/epoch: 77/1182/4, Loss: 1449.3267822265625\n","Train -> sample/numSamples/epoch: 78/1182/4, Loss: 797.9951782226562\n","Train -> sample/numSamples/epoch: 79/1182/4, Loss: 818.2069091796875\n","Train -> sample/numSamples/epoch: 80/1182/4, Loss: 1040.947509765625\n","Train -> sample/numSamples/epoch: 81/1182/4, Loss: 1405.729248046875\n","Train -> sample/numSamples/epoch: 82/1182/4, Loss: 1439.2783203125\n","Train -> sample/numSamples/epoch: 83/1182/4, Loss: 920.3391723632812\n","Train -> sample/numSamples/epoch: 84/1182/4, Loss: 1170.8548583984375\n","Train -> sample/numSamples/epoch: 85/1182/4, Loss: 1494.57958984375\n","Train -> sample/numSamples/epoch: 86/1182/4, Loss: 1114.32861328125\n","Train -> sample/numSamples/epoch: 87/1182/4, Loss: 1237.6378173828125\n","Train -> sample/numSamples/epoch: 88/1182/4, Loss: 1066.720458984375\n","Train -> sample/numSamples/epoch: 89/1182/4, Loss: 981.6012573242188\n","Train -> sample/numSamples/epoch: 90/1182/4, Loss: 1060.32763671875\n","Train -> sample/numSamples/epoch: 91/1182/4, Loss: 912.2738037109375\n","Train -> sample/numSamples/epoch: 92/1182/4, Loss: 985.5294799804688\n","Train -> sample/numSamples/epoch: 93/1182/4, Loss: 1408.9879150390625\n","Train -> sample/numSamples/epoch: 94/1182/4, Loss: 977.4144897460938\n","Train -> sample/numSamples/epoch: 95/1182/4, Loss: 931.2390747070312\n","Train -> sample/numSamples/epoch: 96/1182/4, Loss: 1086.46142578125\n","Train -> sample/numSamples/epoch: 97/1182/4, Loss: 1136.072509765625\n","Train -> sample/numSamples/epoch: 98/1182/4, Loss: 835.966552734375\n","Train -> sample/numSamples/epoch: 99/1182/4, Loss: 752.5888671875\n","Train -> sample/numSamples/epoch: 100/1182/4, Loss: 1308.9906005859375\n","Train -> sample/numSamples/epoch: 101/1182/4, Loss: 1146.1187744140625\n","Train -> sample/numSamples/epoch: 102/1182/4, Loss: 944.0274658203125\n","Train -> sample/numSamples/epoch: 103/1182/4, Loss: 873.86376953125\n","Train -> sample/numSamples/epoch: 104/1182/4, Loss: 853.7107543945312\n","Train -> sample/numSamples/epoch: 105/1182/4, Loss: 1028.657958984375\n","Train -> sample/numSamples/epoch: 106/1182/4, Loss: 1437.4481201171875\n","Train -> sample/numSamples/epoch: 107/1182/4, Loss: 1144.765380859375\n","Train -> sample/numSamples/epoch: 108/1182/4, Loss: 1225.17919921875\n","Train -> sample/numSamples/epoch: 109/1182/4, Loss: 838.9631958007812\n","Train -> sample/numSamples/epoch: 110/1182/4, Loss: 980.626953125\n","Train -> sample/numSamples/epoch: 111/1182/4, Loss: 1104.89208984375\n","Train -> sample/numSamples/epoch: 112/1182/4, Loss: 783.3234252929688\n","Train -> sample/numSamples/epoch: 113/1182/4, Loss: 978.8295288085938\n","Train -> sample/numSamples/epoch: 114/1182/4, Loss: 999.5807495117188\n","Train -> sample/numSamples/epoch: 115/1182/4, Loss: 719.7518310546875\n","Train -> sample/numSamples/epoch: 116/1182/4, Loss: 903.4743041992188\n","Train -> sample/numSamples/epoch: 117/1182/4, Loss: 767.8350830078125\n","Train -> sample/numSamples/epoch: 118/1182/4, Loss: 808.8540649414062\n","Train -> sample/numSamples/epoch: 119/1182/4, Loss: 931.4747924804688\n","Train -> sample/numSamples/epoch: 120/1182/4, Loss: 1103.95166015625\n","Train -> sample/numSamples/epoch: 121/1182/4, Loss: 1178.481201171875\n","Train -> sample/numSamples/epoch: 122/1182/4, Loss: 983.4733276367188\n","Train -> sample/numSamples/epoch: 123/1182/4, Loss: 1052.075439453125\n","Train -> sample/numSamples/epoch: 124/1182/4, Loss: 489.83526611328125\n","Train -> sample/numSamples/epoch: 125/1182/4, Loss: 1111.4647216796875\n","Train -> sample/numSamples/epoch: 126/1182/4, Loss: 874.3375244140625\n","Train -> sample/numSamples/epoch: 127/1182/4, Loss: 912.6016235351562\n","Train -> sample/numSamples/epoch: 128/1182/4, Loss: 636.4105224609375\n","Train -> sample/numSamples/epoch: 129/1182/4, Loss: 965.4806518554688\n","Train -> sample/numSamples/epoch: 130/1182/4, Loss: 1385.5029296875\n","Train -> sample/numSamples/epoch: 131/1182/4, Loss: 1062.20263671875\n","Train -> sample/numSamples/epoch: 132/1182/4, Loss: 1483.23388671875\n","Train -> sample/numSamples/epoch: 133/1182/4, Loss: 949.9976806640625\n","Train -> sample/numSamples/epoch: 134/1182/4, Loss: 769.0016479492188\n","Train -> sample/numSamples/epoch: 135/1182/4, Loss: 1072.6212158203125\n","Train -> sample/numSamples/epoch: 136/1182/4, Loss: 656.9547119140625\n","Train -> sample/numSamples/epoch: 137/1182/4, Loss: 845.5492553710938\n","Train -> sample/numSamples/epoch: 138/1182/4, Loss: 1251.4691162109375\n","Train -> sample/numSamples/epoch: 139/1182/4, Loss: 1110.2083740234375\n","Train -> sample/numSamples/epoch: 140/1182/4, Loss: 1038.61181640625\n","Train -> sample/numSamples/epoch: 141/1182/4, Loss: 1235.4986572265625\n","Train -> sample/numSamples/epoch: 142/1182/4, Loss: 1361.19482421875\n","Train -> sample/numSamples/epoch: 143/1182/4, Loss: 1319.6583251953125\n","Train -> sample/numSamples/epoch: 144/1182/4, Loss: 1069.734619140625\n","Train -> sample/numSamples/epoch: 145/1182/4, Loss: 792.456298828125\n","Train -> sample/numSamples/epoch: 146/1182/4, Loss: 909.5374145507812\n","Train -> sample/numSamples/epoch: 147/1182/4, Loss: 660.9796142578125\n","Train -> sample/numSamples/epoch: 148/1182/4, Loss: 915.0055541992188\n","Train -> sample/numSamples/epoch: 149/1182/4, Loss: 1161.3970947265625\n","Train -> sample/numSamples/epoch: 150/1182/4, Loss: 944.91796875\n","Train -> sample/numSamples/epoch: 151/1182/4, Loss: 822.2181396484375\n","Train -> sample/numSamples/epoch: 152/1182/4, Loss: 1025.9241943359375\n","Train -> sample/numSamples/epoch: 153/1182/4, Loss: 931.9116821289062\n","Train -> sample/numSamples/epoch: 154/1182/4, Loss: 1021.8971557617188\n","Train -> sample/numSamples/epoch: 155/1182/4, Loss: 894.1133422851562\n","Train -> sample/numSamples/epoch: 156/1182/4, Loss: 1355.2958984375\n","Train -> sample/numSamples/epoch: 157/1182/4, Loss: 963.6035766601562\n","Train -> sample/numSamples/epoch: 158/1182/4, Loss: 647.0618896484375\n","Train -> sample/numSamples/epoch: 159/1182/4, Loss: 917.7771606445312\n","Train -> sample/numSamples/epoch: 160/1182/4, Loss: 849.2450561523438\n","Train -> sample/numSamples/epoch: 161/1182/4, Loss: 751.0686645507812\n","Train -> sample/numSamples/epoch: 162/1182/4, Loss: 1161.2474365234375\n","Train -> sample/numSamples/epoch: 163/1182/4, Loss: 1600.32421875\n","Train -> sample/numSamples/epoch: 164/1182/4, Loss: 1135.2431640625\n","Train -> sample/numSamples/epoch: 165/1182/4, Loss: 1689.8712158203125\n","Train -> sample/numSamples/epoch: 166/1182/4, Loss: 643.1486206054688\n","Train -> sample/numSamples/epoch: 167/1182/4, Loss: 1026.8704833984375\n","Train -> sample/numSamples/epoch: 168/1182/4, Loss: 637.9758911132812\n","Train -> sample/numSamples/epoch: 169/1182/4, Loss: 832.1514892578125\n","Train -> sample/numSamples/epoch: 170/1182/4, Loss: 1108.93701171875\n","Train -> sample/numSamples/epoch: 171/1182/4, Loss: 1260.071533203125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 172/1182/4, Loss: 1207.8756103515625\n","Train -> sample/numSamples/epoch: 173/1182/4, Loss: 1156.5870361328125\n","Train -> sample/numSamples/epoch: 174/1182/4, Loss: 1254.051025390625\n","Train -> sample/numSamples/epoch: 175/1182/4, Loss: 1156.716796875\n","Train -> sample/numSamples/epoch: 176/1182/4, Loss: 1016.9446411132812\n","Train -> sample/numSamples/epoch: 177/1182/4, Loss: 772.7457885742188\n","Train -> sample/numSamples/epoch: 178/1182/4, Loss: 946.9473876953125\n","Train -> sample/numSamples/epoch: 179/1182/4, Loss: 1430.9814453125\n","Train -> sample/numSamples/epoch: 180/1182/4, Loss: 644.900390625\n","Train -> sample/numSamples/epoch: 181/1182/4, Loss: 770.1764526367188\n","Train -> sample/numSamples/epoch: 182/1182/4, Loss: 984.1109619140625\n","Train -> sample/numSamples/epoch: 183/1182/4, Loss: 978.2942504882812\n","Train -> sample/numSamples/epoch: 184/1182/4, Loss: 672.6620483398438\n","Train -> sample/numSamples/epoch: 185/1182/4, Loss: 1151.99755859375\n","Train -> sample/numSamples/epoch: 186/1182/4, Loss: 829.4421997070312\n","Train -> sample/numSamples/epoch: 187/1182/4, Loss: 1120.244140625\n","Train -> sample/numSamples/epoch: 188/1182/4, Loss: 863.5165405273438\n","Train -> sample/numSamples/epoch: 189/1182/4, Loss: 1212.91552734375\n","Train -> sample/numSamples/epoch: 190/1182/4, Loss: 985.93896484375\n","Train -> sample/numSamples/epoch: 191/1182/4, Loss: 893.810791015625\n","Train -> sample/numSamples/epoch: 192/1182/4, Loss: 1102.071533203125\n","Train -> sample/numSamples/epoch: 193/1182/4, Loss: 1347.976318359375\n","Train -> sample/numSamples/epoch: 194/1182/4, Loss: 879.16064453125\n","Train -> sample/numSamples/epoch: 195/1182/4, Loss: 920.3753051757812\n","Train -> sample/numSamples/epoch: 196/1182/4, Loss: 1322.427978515625\n","Train -> sample/numSamples/epoch: 197/1182/4, Loss: 1014.1131591796875\n","Train -> sample/numSamples/epoch: 198/1182/4, Loss: 1072.4176025390625\n","Train -> sample/numSamples/epoch: 199/1182/4, Loss: 991.0497436523438\n","Train -> sample/numSamples/epoch: 200/1182/4, Loss: 616.1228637695312\n","Train -> sample/numSamples/epoch: 201/1182/4, Loss: 928.0314331054688\n","Train -> sample/numSamples/epoch: 202/1182/4, Loss: 1302.73779296875\n","Train -> sample/numSamples/epoch: 203/1182/4, Loss: 1203.520751953125\n","Train -> sample/numSamples/epoch: 204/1182/4, Loss: 663.2236938476562\n","Train -> sample/numSamples/epoch: 205/1182/4, Loss: 787.801513671875\n","Train -> sample/numSamples/epoch: 206/1182/4, Loss: 859.3455200195312\n","Train -> sample/numSamples/epoch: 207/1182/4, Loss: 1079.2705078125\n","Train -> sample/numSamples/epoch: 208/1182/4, Loss: 756.6617431640625\n","Train -> sample/numSamples/epoch: 209/1182/4, Loss: 1036.8934326171875\n","Train -> sample/numSamples/epoch: 210/1182/4, Loss: 1022.29150390625\n","Train -> sample/numSamples/epoch: 211/1182/4, Loss: 1142.7589111328125\n","Train -> sample/numSamples/epoch: 212/1182/4, Loss: 971.5704956054688\n","Train -> sample/numSamples/epoch: 213/1182/4, Loss: 964.5176391601562\n","Train -> sample/numSamples/epoch: 214/1182/4, Loss: 1069.6429443359375\n","Train -> sample/numSamples/epoch: 215/1182/4, Loss: 633.402587890625\n","Train -> sample/numSamples/epoch: 216/1182/4, Loss: 1048.396240234375\n","Train -> sample/numSamples/epoch: 217/1182/4, Loss: 774.9754028320312\n","Train -> sample/numSamples/epoch: 218/1182/4, Loss: 954.2019653320312\n","Train -> sample/numSamples/epoch: 219/1182/4, Loss: 971.4783935546875\n","Train -> sample/numSamples/epoch: 220/1182/4, Loss: 1697.3609619140625\n","Train -> sample/numSamples/epoch: 221/1182/4, Loss: 771.5485229492188\n","Train -> sample/numSamples/epoch: 222/1182/4, Loss: 1013.8558959960938\n","Train -> sample/numSamples/epoch: 223/1182/4, Loss: 874.121826171875\n","Train -> sample/numSamples/epoch: 224/1182/4, Loss: 977.656005859375\n","Train -> sample/numSamples/epoch: 225/1182/4, Loss: 1077.6456298828125\n","Train -> sample/numSamples/epoch: 226/1182/4, Loss: 740.6412353515625\n","Train -> sample/numSamples/epoch: 227/1182/4, Loss: 932.3145751953125\n","Train -> sample/numSamples/epoch: 228/1182/4, Loss: 912.9689331054688\n","Train -> sample/numSamples/epoch: 229/1182/4, Loss: 1092.55224609375\n","Train -> sample/numSamples/epoch: 230/1182/4, Loss: 1295.912109375\n","Train -> sample/numSamples/epoch: 231/1182/4, Loss: 1076.119384765625\n","Train -> sample/numSamples/epoch: 232/1182/4, Loss: 1265.5728759765625\n","Train -> sample/numSamples/epoch: 233/1182/4, Loss: 966.5953979492188\n","Train -> sample/numSamples/epoch: 234/1182/4, Loss: 1387.956787109375\n","Train -> sample/numSamples/epoch: 235/1182/4, Loss: 1153.9942626953125\n","Train -> sample/numSamples/epoch: 236/1182/4, Loss: 1078.759765625\n","Train -> sample/numSamples/epoch: 237/1182/4, Loss: 1178.32568359375\n","Train -> sample/numSamples/epoch: 238/1182/4, Loss: 946.7503662109375\n","Train -> sample/numSamples/epoch: 239/1182/4, Loss: 1400.67236328125\n","Train -> sample/numSamples/epoch: 240/1182/4, Loss: 996.148681640625\n","Train -> sample/numSamples/epoch: 241/1182/4, Loss: 513.5748901367188\n","Train -> sample/numSamples/epoch: 242/1182/4, Loss: 531.4661254882812\n","Train -> sample/numSamples/epoch: 243/1182/4, Loss: 750.000244140625\n","Train -> sample/numSamples/epoch: 244/1182/4, Loss: 744.2350463867188\n","Train -> sample/numSamples/epoch: 245/1182/4, Loss: 1366.12548828125\n","Train -> sample/numSamples/epoch: 246/1182/4, Loss: 1211.1409912109375\n","Train -> sample/numSamples/epoch: 247/1182/4, Loss: 624.1318359375\n","Train -> sample/numSamples/epoch: 248/1182/4, Loss: 791.2531127929688\n","Train -> sample/numSamples/epoch: 249/1182/4, Loss: 1036.81982421875\n","Train -> sample/numSamples/epoch: 250/1182/4, Loss: 1081.6212158203125\n","Train -> sample/numSamples/epoch: 251/1182/4, Loss: 711.133056640625\n","Train -> sample/numSamples/epoch: 252/1182/4, Loss: 791.4527587890625\n","Train -> sample/numSamples/epoch: 253/1182/4, Loss: 746.9345703125\n","Train -> sample/numSamples/epoch: 254/1182/4, Loss: 734.7259521484375\n","Train -> sample/numSamples/epoch: 255/1182/4, Loss: 817.9169311523438\n","Train -> sample/numSamples/epoch: 256/1182/4, Loss: 1093.9803466796875\n","Train -> sample/numSamples/epoch: 257/1182/4, Loss: 1152.94580078125\n","Train -> sample/numSamples/epoch: 258/1182/4, Loss: 837.3164672851562\n","Train -> sample/numSamples/epoch: 259/1182/4, Loss: 1079.2032470703125\n","Train -> sample/numSamples/epoch: 260/1182/4, Loss: 843.9163818359375\n","Train -> sample/numSamples/epoch: 261/1182/4, Loss: 634.760498046875\n","Train -> sample/numSamples/epoch: 262/1182/4, Loss: 776.4598999023438\n","Train -> sample/numSamples/epoch: 263/1182/4, Loss: 983.088623046875\n","Train -> sample/numSamples/epoch: 264/1182/4, Loss: 681.2349243164062\n","Train -> sample/numSamples/epoch: 265/1182/4, Loss: 1122.8681640625\n","Train -> sample/numSamples/epoch: 266/1182/4, Loss: 1018.9361572265625\n","Train -> sample/numSamples/epoch: 267/1182/4, Loss: 976.3081665039062\n","Train -> sample/numSamples/epoch: 268/1182/4, Loss: 1100.194091796875\n","Train -> sample/numSamples/epoch: 269/1182/4, Loss: 1146.5313720703125\n","Train -> sample/numSamples/epoch: 270/1182/4, Loss: 975.6771240234375\n","Train -> sample/numSamples/epoch: 271/1182/4, Loss: 1047.73828125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 272/1182/4, Loss: 1270.7540283203125\n","Train -> sample/numSamples/epoch: 273/1182/4, Loss: 839.1879272460938\n","Train -> sample/numSamples/epoch: 274/1182/4, Loss: 1111.2667236328125\n","Train -> sample/numSamples/epoch: 275/1182/4, Loss: 847.6366577148438\n","Train -> sample/numSamples/epoch: 276/1182/4, Loss: 843.7268676757812\n","Train -> sample/numSamples/epoch: 277/1182/4, Loss: 1060.328125\n","Train -> sample/numSamples/epoch: 278/1182/4, Loss: 792.6139526367188\n","Train -> sample/numSamples/epoch: 279/1182/4, Loss: 989.4849853515625\n","Train -> sample/numSamples/epoch: 280/1182/4, Loss: 985.651123046875\n","Train -> sample/numSamples/epoch: 281/1182/4, Loss: 893.6882934570312\n","Train -> sample/numSamples/epoch: 282/1182/4, Loss: 896.6285400390625\n","Train -> sample/numSamples/epoch: 283/1182/4, Loss: 1142.375732421875\n","Train -> sample/numSamples/epoch: 284/1182/4, Loss: 1454.1878662109375\n","Train -> sample/numSamples/epoch: 285/1182/4, Loss: 918.739990234375\n","Train -> sample/numSamples/epoch: 286/1182/4, Loss: 1741.7562255859375\n","Train -> sample/numSamples/epoch: 287/1182/4, Loss: 1477.478271484375\n","Train -> sample/numSamples/epoch: 288/1182/4, Loss: 857.7499389648438\n","Train -> sample/numSamples/epoch: 289/1182/4, Loss: 1010.6607055664062\n","Train -> sample/numSamples/epoch: 290/1182/4, Loss: 910.2251586914062\n","Train -> sample/numSamples/epoch: 291/1182/4, Loss: 897.3074951171875\n","Train -> sample/numSamples/epoch: 292/1182/4, Loss: 534.0885009765625\n","Train -> sample/numSamples/epoch: 293/1182/4, Loss: 1166.364990234375\n","Train -> sample/numSamples/epoch: 294/1182/4, Loss: 899.38134765625\n","Train -> sample/numSamples/epoch: 295/1182/4, Loss: 883.69580078125\n","Train -> sample/numSamples/epoch: 296/1182/4, Loss: 818.291015625\n","Train -> sample/numSamples/epoch: 297/1182/4, Loss: 670.0889282226562\n","Train -> sample/numSamples/epoch: 298/1182/4, Loss: 1389.92822265625\n","Train -> sample/numSamples/epoch: 299/1182/4, Loss: 486.26507568359375\n","Train -> sample/numSamples/epoch: 300/1182/4, Loss: 1044.9425048828125\n","Train -> sample/numSamples/epoch: 301/1182/4, Loss: 739.2752075195312\n","Train -> sample/numSamples/epoch: 302/1182/4, Loss: 861.90087890625\n","Train -> sample/numSamples/epoch: 303/1182/4, Loss: 1034.397705078125\n","Train -> sample/numSamples/epoch: 304/1182/4, Loss: 904.3058471679688\n","Train -> sample/numSamples/epoch: 305/1182/4, Loss: 1018.2672119140625\n","Train -> sample/numSamples/epoch: 306/1182/4, Loss: 1055.21533203125\n","Train -> sample/numSamples/epoch: 307/1182/4, Loss: 1322.1318359375\n","Train -> sample/numSamples/epoch: 308/1182/4, Loss: 1222.5274658203125\n","Train -> sample/numSamples/epoch: 309/1182/4, Loss: 1230.983154296875\n","Train -> sample/numSamples/epoch: 310/1182/4, Loss: 955.0931396484375\n","Train -> sample/numSamples/epoch: 311/1182/4, Loss: 980.149169921875\n","Train -> sample/numSamples/epoch: 312/1182/4, Loss: 746.73876953125\n","Train -> sample/numSamples/epoch: 313/1182/4, Loss: 721.3590087890625\n","Train -> sample/numSamples/epoch: 314/1182/4, Loss: 1090.9246826171875\n","Train -> sample/numSamples/epoch: 315/1182/4, Loss: 1168.0064697265625\n","Train -> sample/numSamples/epoch: 316/1182/4, Loss: 745.6324462890625\n","Train -> sample/numSamples/epoch: 317/1182/4, Loss: 997.03369140625\n","Train -> sample/numSamples/epoch: 318/1182/4, Loss: 1187.9422607421875\n","Train -> sample/numSamples/epoch: 319/1182/4, Loss: 1252.6990966796875\n","Train -> sample/numSamples/epoch: 320/1182/4, Loss: 1446.6744384765625\n","Train -> sample/numSamples/epoch: 321/1182/4, Loss: 904.0772705078125\n","Train -> sample/numSamples/epoch: 322/1182/4, Loss: 724.6656494140625\n","Train -> sample/numSamples/epoch: 323/1182/4, Loss: 729.3304443359375\n","Train -> sample/numSamples/epoch: 324/1182/4, Loss: 1277.851318359375\n","Train -> sample/numSamples/epoch: 325/1182/4, Loss: 1582.102294921875\n","Train -> sample/numSamples/epoch: 326/1182/4, Loss: 671.8054809570312\n","Train -> sample/numSamples/epoch: 327/1182/4, Loss: 819.704833984375\n","Train -> sample/numSamples/epoch: 328/1182/4, Loss: 712.4666748046875\n","Train -> sample/numSamples/epoch: 329/1182/4, Loss: 1215.3392333984375\n","Train -> sample/numSamples/epoch: 330/1182/4, Loss: 829.5443725585938\n","Train -> sample/numSamples/epoch: 331/1182/4, Loss: 1345.2086181640625\n","Train -> sample/numSamples/epoch: 332/1182/4, Loss: 331.09161376953125\n","Train -> sample/numSamples/epoch: 333/1182/4, Loss: 795.4630737304688\n","Train -> sample/numSamples/epoch: 334/1182/4, Loss: 1342.022216796875\n","Train -> sample/numSamples/epoch: 335/1182/4, Loss: 1067.36376953125\n","Train -> sample/numSamples/epoch: 336/1182/4, Loss: 1011.4747924804688\n","Train -> sample/numSamples/epoch: 337/1182/4, Loss: 1021.9462280273438\n","Train -> sample/numSamples/epoch: 338/1182/4, Loss: 863.8623657226562\n","Train -> sample/numSamples/epoch: 339/1182/4, Loss: 1099.82177734375\n","Train -> sample/numSamples/epoch: 340/1182/4, Loss: 1155.6021728515625\n","Train -> sample/numSamples/epoch: 341/1182/4, Loss: 1356.5975341796875\n","Train -> sample/numSamples/epoch: 342/1182/4, Loss: 952.6809692382812\n","Train -> sample/numSamples/epoch: 343/1182/4, Loss: 516.1981201171875\n","Train -> sample/numSamples/epoch: 344/1182/4, Loss: 839.4494018554688\n","Train -> sample/numSamples/epoch: 345/1182/4, Loss: 1111.7659912109375\n","Train -> sample/numSamples/epoch: 346/1182/4, Loss: 1569.5633544921875\n","Train -> sample/numSamples/epoch: 347/1182/4, Loss: 1028.4742431640625\n","Train -> sample/numSamples/epoch: 348/1182/4, Loss: 917.4208374023438\n","Train -> sample/numSamples/epoch: 349/1182/4, Loss: 981.2986450195312\n","Train -> sample/numSamples/epoch: 350/1182/4, Loss: 1104.12744140625\n","Train -> sample/numSamples/epoch: 351/1182/4, Loss: 1294.494873046875\n","Train -> sample/numSamples/epoch: 352/1182/4, Loss: 875.5667724609375\n","Train -> sample/numSamples/epoch: 353/1182/4, Loss: 646.927978515625\n","Train -> sample/numSamples/epoch: 354/1182/4, Loss: 1207.8734130859375\n","Train -> sample/numSamples/epoch: 355/1182/4, Loss: 1641.5699462890625\n","Train -> sample/numSamples/epoch: 356/1182/4, Loss: 1176.7960205078125\n","Train -> sample/numSamples/epoch: 357/1182/4, Loss: 1136.8104248046875\n","Train -> sample/numSamples/epoch: 358/1182/4, Loss: 964.88671875\n","Train -> sample/numSamples/epoch: 359/1182/4, Loss: 1205.45556640625\n","Train -> sample/numSamples/epoch: 360/1182/4, Loss: 777.05078125\n","Train -> sample/numSamples/epoch: 361/1182/4, Loss: 1086.2730712890625\n","Train -> sample/numSamples/epoch: 362/1182/4, Loss: 1364.9461669921875\n","Train -> sample/numSamples/epoch: 363/1182/4, Loss: 546.1920776367188\n","Train -> sample/numSamples/epoch: 364/1182/4, Loss: 1679.8079833984375\n","Train -> sample/numSamples/epoch: 365/1182/4, Loss: 969.2183837890625\n","Train -> sample/numSamples/epoch: 366/1182/4, Loss: 611.3598022460938\n","Train -> sample/numSamples/epoch: 367/1182/4, Loss: 1103.06005859375\n","Train -> sample/numSamples/epoch: 368/1182/4, Loss: 1259.65771484375\n","Train -> sample/numSamples/epoch: 369/1182/4, Loss: 992.3440551757812\n","Train -> sample/numSamples/epoch: 370/1182/4, Loss: 1047.5738525390625\n","Train -> sample/numSamples/epoch: 371/1182/4, Loss: 814.9406127929688\n","imatge guardada\n","Train -> sample/numSamples/epoch: 372/1182/4, Loss: 1010.0006103515625\n","Train -> sample/numSamples/epoch: 373/1182/4, Loss: 910.0746459960938\n","Train -> sample/numSamples/epoch: 374/1182/4, Loss: 1124.6668701171875\n","Train -> sample/numSamples/epoch: 375/1182/4, Loss: 619.9829711914062\n","Train -> sample/numSamples/epoch: 376/1182/4, Loss: 941.771484375\n","Train -> sample/numSamples/epoch: 377/1182/4, Loss: 661.8402709960938\n","Train -> sample/numSamples/epoch: 378/1182/4, Loss: 744.2987670898438\n","Train -> sample/numSamples/epoch: 379/1182/4, Loss: 721.1704711914062\n","Train -> sample/numSamples/epoch: 380/1182/4, Loss: 826.6640625\n","Train -> sample/numSamples/epoch: 381/1182/4, Loss: 633.5344848632812\n","Train -> sample/numSamples/epoch: 382/1182/4, Loss: 1125.3802490234375\n","Train -> sample/numSamples/epoch: 383/1182/4, Loss: 987.8001708984375\n","Train -> sample/numSamples/epoch: 384/1182/4, Loss: 1052.318115234375\n","Train -> sample/numSamples/epoch: 385/1182/4, Loss: 1397.6258544921875\n","Train -> sample/numSamples/epoch: 386/1182/4, Loss: 890.1380615234375\n","Train -> sample/numSamples/epoch: 387/1182/4, Loss: 804.303955078125\n","Train -> sample/numSamples/epoch: 388/1182/4, Loss: 680.3495483398438\n","Train -> sample/numSamples/epoch: 389/1182/4, Loss: 864.1019897460938\n","Train -> sample/numSamples/epoch: 390/1182/4, Loss: 1100.067626953125\n","Train -> sample/numSamples/epoch: 391/1182/4, Loss: 575.978271484375\n","Train -> sample/numSamples/epoch: 392/1182/4, Loss: 825.6266479492188\n","Train -> sample/numSamples/epoch: 393/1182/4, Loss: 891.3262329101562\n","Train -> sample/numSamples/epoch: 394/1182/4, Loss: 1138.65576171875\n","Train -> sample/numSamples/epoch: 395/1182/4, Loss: 872.45556640625\n","Train -> sample/numSamples/epoch: 396/1182/4, Loss: 1145.80908203125\n","Train -> sample/numSamples/epoch: 397/1182/4, Loss: 897.6980590820312\n","Train -> sample/numSamples/epoch: 398/1182/4, Loss: 1147.728271484375\n","Train -> sample/numSamples/epoch: 399/1182/4, Loss: 1196.541015625\n","Train -> sample/numSamples/epoch: 400/1182/4, Loss: 981.817626953125\n","Train -> sample/numSamples/epoch: 401/1182/4, Loss: 1175.47216796875\n","Train -> sample/numSamples/epoch: 402/1182/4, Loss: 880.5867309570312\n","Train -> sample/numSamples/epoch: 403/1182/4, Loss: 609.8422241210938\n","Train -> sample/numSamples/epoch: 404/1182/4, Loss: 866.2752685546875\n","Train -> sample/numSamples/epoch: 405/1182/4, Loss: 1135.56494140625\n","Train -> sample/numSamples/epoch: 406/1182/4, Loss: 959.1141357421875\n","Train -> sample/numSamples/epoch: 407/1182/4, Loss: 854.229248046875\n","Train -> sample/numSamples/epoch: 408/1182/4, Loss: 1216.4696044921875\n","Train -> sample/numSamples/epoch: 409/1182/4, Loss: 1378.684814453125\n","Train -> sample/numSamples/epoch: 410/1182/4, Loss: 1022.8408203125\n","Train -> sample/numSamples/epoch: 411/1182/4, Loss: 1066.435546875\n","Train -> sample/numSamples/epoch: 412/1182/4, Loss: 653.3336791992188\n","Train -> sample/numSamples/epoch: 413/1182/4, Loss: 957.1882934570312\n","Train -> sample/numSamples/epoch: 414/1182/4, Loss: 1315.066162109375\n","Train -> sample/numSamples/epoch: 415/1182/4, Loss: 687.0176391601562\n","Train -> sample/numSamples/epoch: 416/1182/4, Loss: 796.8292236328125\n","Train -> sample/numSamples/epoch: 417/1182/4, Loss: 1011.0759887695312\n","Train -> sample/numSamples/epoch: 418/1182/4, Loss: 1192.9329833984375\n","Train -> sample/numSamples/epoch: 419/1182/4, Loss: 1013.2122192382812\n","Train -> sample/numSamples/epoch: 420/1182/4, Loss: 1109.245849609375\n","Train -> sample/numSamples/epoch: 421/1182/4, Loss: 970.5469360351562\n","Train -> sample/numSamples/epoch: 422/1182/4, Loss: 877.7461547851562\n","Train -> sample/numSamples/epoch: 423/1182/4, Loss: 940.8430786132812\n","Train -> sample/numSamples/epoch: 424/1182/4, Loss: 1335.4290771484375\n","Train -> sample/numSamples/epoch: 425/1182/4, Loss: 1041.1688232421875\n","Train -> sample/numSamples/epoch: 426/1182/4, Loss: 777.1570434570312\n","Train -> sample/numSamples/epoch: 427/1182/4, Loss: 1167.5152587890625\n","Train -> sample/numSamples/epoch: 428/1182/4, Loss: 1010.327880859375\n","Train -> sample/numSamples/epoch: 429/1182/4, Loss: 1156.7474365234375\n","Train -> sample/numSamples/epoch: 430/1182/4, Loss: 1075.319091796875\n","Train -> sample/numSamples/epoch: 431/1182/4, Loss: 932.8764038085938\n","Train -> sample/numSamples/epoch: 432/1182/4, Loss: 1174.224853515625\n","Train -> sample/numSamples/epoch: 433/1182/4, Loss: 1315.0906982421875\n","Train -> sample/numSamples/epoch: 434/1182/4, Loss: 1366.4371337890625\n","Train -> sample/numSamples/epoch: 435/1182/4, Loss: 1263.5350341796875\n","Train -> sample/numSamples/epoch: 436/1182/4, Loss: 944.8302001953125\n","Train -> sample/numSamples/epoch: 437/1182/4, Loss: 1079.124267578125\n","Train -> sample/numSamples/epoch: 438/1182/4, Loss: 729.9395751953125\n","Train -> sample/numSamples/epoch: 439/1182/4, Loss: 1004.1465454101562\n","Train -> sample/numSamples/epoch: 440/1182/4, Loss: 746.7067260742188\n","Train -> sample/numSamples/epoch: 441/1182/4, Loss: 969.3893432617188\n","Train -> sample/numSamples/epoch: 442/1182/4, Loss: 882.0280151367188\n","Train -> sample/numSamples/epoch: 443/1182/4, Loss: 662.5250854492188\n","Train -> sample/numSamples/epoch: 444/1182/4, Loss: 780.0970458984375\n","Train -> sample/numSamples/epoch: 445/1182/4, Loss: 713.8298950195312\n","Train -> sample/numSamples/epoch: 446/1182/4, Loss: 1146.0380859375\n","Train -> sample/numSamples/epoch: 447/1182/4, Loss: 1069.30712890625\n","Train -> sample/numSamples/epoch: 448/1182/4, Loss: 731.6671752929688\n","Train -> sample/numSamples/epoch: 449/1182/4, Loss: 992.8777465820312\n","Train -> sample/numSamples/epoch: 450/1182/4, Loss: 427.89208984375\n","Train -> sample/numSamples/epoch: 451/1182/4, Loss: 987.57861328125\n","Train -> sample/numSamples/epoch: 452/1182/4, Loss: 914.59228515625\n","Train -> sample/numSamples/epoch: 453/1182/4, Loss: 1063.903076171875\n","Train -> sample/numSamples/epoch: 454/1182/4, Loss: 834.2033081054688\n","Train -> sample/numSamples/epoch: 455/1182/4, Loss: 1207.6424560546875\n","Train -> sample/numSamples/epoch: 456/1182/4, Loss: 688.5260620117188\n","Train -> sample/numSamples/epoch: 457/1182/4, Loss: 1387.0987548828125\n","Train -> sample/numSamples/epoch: 458/1182/4, Loss: 1459.6318359375\n","Train -> sample/numSamples/epoch: 459/1182/4, Loss: 1091.108642578125\n","Train -> sample/numSamples/epoch: 460/1182/4, Loss: 882.8538208007812\n","Train -> sample/numSamples/epoch: 461/1182/4, Loss: 1050.9429931640625\n","Train -> sample/numSamples/epoch: 462/1182/4, Loss: 866.7791748046875\n","Train -> sample/numSamples/epoch: 463/1182/4, Loss: 1222.193359375\n","Train -> sample/numSamples/epoch: 464/1182/4, Loss: 1277.3892822265625\n","Train -> sample/numSamples/epoch: 465/1182/4, Loss: 1050.308349609375\n","Train -> sample/numSamples/epoch: 466/1182/4, Loss: 950.1890258789062\n","Train -> sample/numSamples/epoch: 467/1182/4, Loss: 845.001708984375\n","Train -> sample/numSamples/epoch: 468/1182/4, Loss: 1195.9149169921875\n","Train -> sample/numSamples/epoch: 469/1182/4, Loss: 1222.53955078125\n","Train -> sample/numSamples/epoch: 470/1182/4, Loss: 517.13671875\n","Train -> sample/numSamples/epoch: 471/1182/4, Loss: 1655.521484375\n","imatge guardada\n","Train -> sample/numSamples/epoch: 472/1182/4, Loss: 1380.4691162109375\n","Train -> sample/numSamples/epoch: 473/1182/4, Loss: 880.7874145507812\n","Train -> sample/numSamples/epoch: 474/1182/4, Loss: 1131.404541015625\n","Train -> sample/numSamples/epoch: 475/1182/4, Loss: 835.42431640625\n","Train -> sample/numSamples/epoch: 476/1182/4, Loss: 711.1088256835938\n","Train -> sample/numSamples/epoch: 477/1182/4, Loss: 1025.0087890625\n","Train -> sample/numSamples/epoch: 478/1182/4, Loss: 991.54150390625\n","Train -> sample/numSamples/epoch: 479/1182/4, Loss: 1216.4017333984375\n","Train -> sample/numSamples/epoch: 480/1182/4, Loss: 1200.265625\n","Train -> sample/numSamples/epoch: 481/1182/4, Loss: 1373.11962890625\n","Train -> sample/numSamples/epoch: 482/1182/4, Loss: 971.1911010742188\n","Train -> sample/numSamples/epoch: 483/1182/4, Loss: 910.1416015625\n","Train -> sample/numSamples/epoch: 484/1182/4, Loss: 1336.8876953125\n","Train -> sample/numSamples/epoch: 485/1182/4, Loss: 1033.2674560546875\n","Train -> sample/numSamples/epoch: 486/1182/4, Loss: 788.8623046875\n","Train -> sample/numSamples/epoch: 487/1182/4, Loss: 1282.6044921875\n","Train -> sample/numSamples/epoch: 488/1182/4, Loss: 1002.6864624023438\n","Train -> sample/numSamples/epoch: 489/1182/4, Loss: 1124.6370849609375\n","Train -> sample/numSamples/epoch: 490/1182/4, Loss: 1164.4375\n","Train -> sample/numSamples/epoch: 491/1182/4, Loss: 1001.189453125\n","Train -> sample/numSamples/epoch: 492/1182/4, Loss: 1020.1231079101562\n","Train -> sample/numSamples/epoch: 493/1182/4, Loss: 949.3232421875\n","Train -> sample/numSamples/epoch: 494/1182/4, Loss: 1061.0272216796875\n","Train -> sample/numSamples/epoch: 495/1182/4, Loss: 1351.00048828125\n","Train -> sample/numSamples/epoch: 496/1182/4, Loss: 1320.022705078125\n","Train -> sample/numSamples/epoch: 497/1182/4, Loss: 1251.3524169921875\n","Train -> sample/numSamples/epoch: 498/1182/4, Loss: 678.8287353515625\n","Train -> sample/numSamples/epoch: 499/1182/4, Loss: 1339.176513671875\n","Train -> sample/numSamples/epoch: 500/1182/4, Loss: 815.7653198242188\n","Train -> sample/numSamples/epoch: 501/1182/4, Loss: 1005.5615844726562\n","Train -> sample/numSamples/epoch: 502/1182/4, Loss: 1195.87646484375\n","Train -> sample/numSamples/epoch: 503/1182/4, Loss: 953.926025390625\n","Train -> sample/numSamples/epoch: 504/1182/4, Loss: 1012.1866455078125\n","Train -> sample/numSamples/epoch: 505/1182/4, Loss: 530.0276489257812\n","Train -> sample/numSamples/epoch: 506/1182/4, Loss: 1277.0902099609375\n","Train -> sample/numSamples/epoch: 507/1182/4, Loss: 1012.9391479492188\n","Train -> sample/numSamples/epoch: 508/1182/4, Loss: 1427.875\n","Train -> sample/numSamples/epoch: 509/1182/4, Loss: 1048.115478515625\n","Train -> sample/numSamples/epoch: 510/1182/4, Loss: 857.7401123046875\n","Train -> sample/numSamples/epoch: 511/1182/4, Loss: 1345.454345703125\n","Train -> sample/numSamples/epoch: 512/1182/4, Loss: 946.2473754882812\n","Train -> sample/numSamples/epoch: 513/1182/4, Loss: 1100.99853515625\n","Train -> sample/numSamples/epoch: 514/1182/4, Loss: 1052.23291015625\n","Train -> sample/numSamples/epoch: 515/1182/4, Loss: 1105.1788330078125\n","Train -> sample/numSamples/epoch: 516/1182/4, Loss: 1322.111328125\n","Train -> sample/numSamples/epoch: 517/1182/4, Loss: 1279.53955078125\n","Train -> sample/numSamples/epoch: 518/1182/4, Loss: 957.8253784179688\n","Train -> sample/numSamples/epoch: 519/1182/4, Loss: 1055.785888671875\n","Train -> sample/numSamples/epoch: 520/1182/4, Loss: 1225.7989501953125\n","Train -> sample/numSamples/epoch: 521/1182/4, Loss: 851.3761596679688\n","Train -> sample/numSamples/epoch: 522/1182/4, Loss: 770.3107299804688\n","Train -> sample/numSamples/epoch: 523/1182/4, Loss: 1386.938720703125\n","Train -> sample/numSamples/epoch: 524/1182/4, Loss: 1061.865478515625\n","Train -> sample/numSamples/epoch: 525/1182/4, Loss: 946.8798828125\n","Train -> sample/numSamples/epoch: 526/1182/4, Loss: 1048.7274169921875\n","Train -> sample/numSamples/epoch: 527/1182/4, Loss: 947.1447143554688\n","Train -> sample/numSamples/epoch: 528/1182/4, Loss: 1005.8502807617188\n","Train -> sample/numSamples/epoch: 529/1182/4, Loss: 701.9576416015625\n","Train -> sample/numSamples/epoch: 530/1182/4, Loss: 1198.5748291015625\n","Train -> sample/numSamples/epoch: 531/1182/4, Loss: 795.2451782226562\n","Train -> sample/numSamples/epoch: 532/1182/4, Loss: 1181.451416015625\n","Train -> sample/numSamples/epoch: 533/1182/4, Loss: 961.5336303710938\n","Train -> sample/numSamples/epoch: 534/1182/4, Loss: 935.1719970703125\n","Train -> sample/numSamples/epoch: 535/1182/4, Loss: 1336.04638671875\n","Train -> sample/numSamples/epoch: 536/1182/4, Loss: 1224.814697265625\n","Train -> sample/numSamples/epoch: 537/1182/4, Loss: 1095.3094482421875\n","Train -> sample/numSamples/epoch: 538/1182/4, Loss: 877.2335815429688\n","Train -> sample/numSamples/epoch: 539/1182/4, Loss: 816.4154663085938\n","Train -> sample/numSamples/epoch: 540/1182/4, Loss: 920.0951538085938\n","Train -> sample/numSamples/epoch: 541/1182/4, Loss: 913.4179077148438\n","Train -> sample/numSamples/epoch: 542/1182/4, Loss: 1186.022216796875\n","Train -> sample/numSamples/epoch: 543/1182/4, Loss: 595.9658203125\n","Train -> sample/numSamples/epoch: 544/1182/4, Loss: 811.5932006835938\n","Train -> sample/numSamples/epoch: 545/1182/4, Loss: 1607.52587890625\n","Train -> sample/numSamples/epoch: 546/1182/4, Loss: 1320.1627197265625\n","Train -> sample/numSamples/epoch: 547/1182/4, Loss: 777.0235595703125\n","Train -> sample/numSamples/epoch: 548/1182/4, Loss: 854.4583129882812\n","Train -> sample/numSamples/epoch: 549/1182/4, Loss: 1254.7674560546875\n","Train -> sample/numSamples/epoch: 550/1182/4, Loss: 1324.634521484375\n","Train -> sample/numSamples/epoch: 551/1182/4, Loss: 966.6946411132812\n","Train -> sample/numSamples/epoch: 552/1182/4, Loss: 739.71533203125\n","Train -> sample/numSamples/epoch: 553/1182/4, Loss: 789.1860961914062\n","Train -> sample/numSamples/epoch: 554/1182/4, Loss: 1126.5374755859375\n","Train -> sample/numSamples/epoch: 555/1182/4, Loss: 974.8212280273438\n","Train -> sample/numSamples/epoch: 556/1182/4, Loss: 1107.8839111328125\n","Train -> sample/numSamples/epoch: 557/1182/4, Loss: 999.0498657226562\n","Train -> sample/numSamples/epoch: 558/1182/4, Loss: 959.1318359375\n","Train -> sample/numSamples/epoch: 559/1182/4, Loss: 1018.0281372070312\n","Train -> sample/numSamples/epoch: 560/1182/4, Loss: 902.7097778320312\n","Train -> sample/numSamples/epoch: 561/1182/4, Loss: 1085.71484375\n","Train -> sample/numSamples/epoch: 562/1182/4, Loss: 1183.6019287109375\n","Train -> sample/numSamples/epoch: 563/1182/4, Loss: 894.936279296875\n","Train -> sample/numSamples/epoch: 564/1182/4, Loss: 731.0496826171875\n","Train -> sample/numSamples/epoch: 565/1182/4, Loss: 920.7031860351562\n","Train -> sample/numSamples/epoch: 566/1182/4, Loss: 1334.30029296875\n","Train -> sample/numSamples/epoch: 567/1182/4, Loss: 1086.46630859375\n","Train -> sample/numSamples/epoch: 568/1182/4, Loss: 957.725341796875\n","Train -> sample/numSamples/epoch: 569/1182/4, Loss: 1109.810546875\n","Train -> sample/numSamples/epoch: 570/1182/4, Loss: 955.6446533203125\n","Train -> sample/numSamples/epoch: 571/1182/4, Loss: 1073.198486328125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 572/1182/4, Loss: 1043.9609375\n","Train -> sample/numSamples/epoch: 573/1182/4, Loss: 1415.2366943359375\n","Train -> sample/numSamples/epoch: 574/1182/4, Loss: 1122.67236328125\n","Train -> sample/numSamples/epoch: 575/1182/4, Loss: 1524.3536376953125\n","Train -> sample/numSamples/epoch: 576/1182/4, Loss: 1084.7119140625\n","Train -> sample/numSamples/epoch: 577/1182/4, Loss: 906.6827392578125\n","Train -> sample/numSamples/epoch: 578/1182/4, Loss: 1397.19140625\n","Train -> sample/numSamples/epoch: 579/1182/4, Loss: 1042.656494140625\n","Train -> sample/numSamples/epoch: 580/1182/4, Loss: 1196.451416015625\n","Train -> sample/numSamples/epoch: 581/1182/4, Loss: 1101.089599609375\n","Train -> sample/numSamples/epoch: 582/1182/4, Loss: 1179.7425537109375\n","Train -> sample/numSamples/epoch: 583/1182/4, Loss: 709.26220703125\n","Train -> sample/numSamples/epoch: 584/1182/4, Loss: 1154.86474609375\n","Train -> sample/numSamples/epoch: 585/1182/4, Loss: 994.1524658203125\n","Train -> sample/numSamples/epoch: 586/1182/4, Loss: 915.61962890625\n","Train -> sample/numSamples/epoch: 587/1182/4, Loss: 1068.6064453125\n","Train -> sample/numSamples/epoch: 588/1182/4, Loss: 709.2096557617188\n","Train -> sample/numSamples/epoch: 589/1182/4, Loss: 1329.123291015625\n","Train -> sample/numSamples/epoch: 590/1182/4, Loss: 708.89208984375\n","Train -> sample/numSamples/epoch: 591/1182/4, Loss: 1123.8778076171875\n","Train -> sample/numSamples/epoch: 592/1182/4, Loss: 1191.0224609375\n","Train -> sample/numSamples/epoch: 593/1182/4, Loss: 1082.79541015625\n","Train -> sample/numSamples/epoch: 594/1182/4, Loss: 1020.130859375\n","Train -> sample/numSamples/epoch: 595/1182/4, Loss: 1134.4876708984375\n","Train -> sample/numSamples/epoch: 596/1182/4, Loss: 858.1343383789062\n","Train -> sample/numSamples/epoch: 597/1182/4, Loss: 832.8949584960938\n","Train -> sample/numSamples/epoch: 598/1182/4, Loss: 949.8414916992188\n","Train -> sample/numSamples/epoch: 599/1182/4, Loss: 1161.8909912109375\n","Train -> sample/numSamples/epoch: 600/1182/4, Loss: 1065.236083984375\n","Train -> sample/numSamples/epoch: 601/1182/4, Loss: 1304.193603515625\n","Train -> sample/numSamples/epoch: 602/1182/4, Loss: 867.6724853515625\n","Train -> sample/numSamples/epoch: 603/1182/4, Loss: 546.80126953125\n","Train -> sample/numSamples/epoch: 604/1182/4, Loss: 980.964599609375\n","Train -> sample/numSamples/epoch: 605/1182/4, Loss: 411.83050537109375\n","Train -> sample/numSamples/epoch: 606/1182/4, Loss: 740.0010986328125\n","Train -> sample/numSamples/epoch: 607/1182/4, Loss: 1261.2657470703125\n","Train -> sample/numSamples/epoch: 608/1182/4, Loss: 982.6422729492188\n","Train -> sample/numSamples/epoch: 609/1182/4, Loss: 1008.6322631835938\n","Train -> sample/numSamples/epoch: 610/1182/4, Loss: 765.2377319335938\n","Train -> sample/numSamples/epoch: 611/1182/4, Loss: 906.020263671875\n","Train -> sample/numSamples/epoch: 612/1182/4, Loss: 792.1245727539062\n","Train -> sample/numSamples/epoch: 613/1182/4, Loss: 1409.2137451171875\n","Train -> sample/numSamples/epoch: 614/1182/4, Loss: 1205.760986328125\n","Train -> sample/numSamples/epoch: 615/1182/4, Loss: 689.5466918945312\n","Train -> sample/numSamples/epoch: 616/1182/4, Loss: 641.0958251953125\n","Train -> sample/numSamples/epoch: 617/1182/4, Loss: 883.8555908203125\n","Train -> sample/numSamples/epoch: 618/1182/4, Loss: 868.5829467773438\n","Train -> sample/numSamples/epoch: 619/1182/4, Loss: 1165.2708740234375\n","Train -> sample/numSamples/epoch: 620/1182/4, Loss: 478.60760498046875\n","Train -> sample/numSamples/epoch: 621/1182/4, Loss: 1176.115966796875\n","Train -> sample/numSamples/epoch: 622/1182/4, Loss: 1042.1092529296875\n","Train -> sample/numSamples/epoch: 623/1182/4, Loss: 1136.7945556640625\n","Train -> sample/numSamples/epoch: 624/1182/4, Loss: 1373.6654052734375\n","Train -> sample/numSamples/epoch: 625/1182/4, Loss: 679.8585205078125\n","Train -> sample/numSamples/epoch: 626/1182/4, Loss: 1286.6856689453125\n","Train -> sample/numSamples/epoch: 627/1182/4, Loss: 996.18310546875\n","Train -> sample/numSamples/epoch: 628/1182/4, Loss: 738.1510620117188\n","Train -> sample/numSamples/epoch: 629/1182/4, Loss: 611.9773559570312\n","Train -> sample/numSamples/epoch: 630/1182/4, Loss: 1215.1463623046875\n","Train -> sample/numSamples/epoch: 631/1182/4, Loss: 1070.0350341796875\n","Train -> sample/numSamples/epoch: 632/1182/4, Loss: 1127.268798828125\n","Train -> sample/numSamples/epoch: 633/1182/4, Loss: 724.3732299804688\n","Train -> sample/numSamples/epoch: 634/1182/4, Loss: 883.2158203125\n","Train -> sample/numSamples/epoch: 635/1182/4, Loss: 438.04833984375\n","Train -> sample/numSamples/epoch: 636/1182/4, Loss: 962.888916015625\n","Train -> sample/numSamples/epoch: 637/1182/4, Loss: 894.1016235351562\n","Train -> sample/numSamples/epoch: 638/1182/4, Loss: 755.5721435546875\n","Train -> sample/numSamples/epoch: 639/1182/4, Loss: 956.1256103515625\n","Train -> sample/numSamples/epoch: 640/1182/4, Loss: 1023.153076171875\n","Train -> sample/numSamples/epoch: 641/1182/4, Loss: 922.70849609375\n","Train -> sample/numSamples/epoch: 642/1182/4, Loss: 1007.2409057617188\n","Train -> sample/numSamples/epoch: 643/1182/4, Loss: 838.430419921875\n","Train -> sample/numSamples/epoch: 644/1182/4, Loss: 1185.8970947265625\n","Train -> sample/numSamples/epoch: 645/1182/4, Loss: 1075.5782470703125\n","Train -> sample/numSamples/epoch: 646/1182/4, Loss: 991.5835571289062\n","Train -> sample/numSamples/epoch: 647/1182/4, Loss: 1028.875\n","Train -> sample/numSamples/epoch: 648/1182/4, Loss: 1161.5390625\n","Train -> sample/numSamples/epoch: 649/1182/4, Loss: 784.5407104492188\n","Train -> sample/numSamples/epoch: 650/1182/4, Loss: 1278.7086181640625\n","Train -> sample/numSamples/epoch: 651/1182/4, Loss: 861.1860961914062\n","Train -> sample/numSamples/epoch: 652/1182/4, Loss: 861.0261840820312\n","Train -> sample/numSamples/epoch: 653/1182/4, Loss: 720.2239379882812\n","Train -> sample/numSamples/epoch: 654/1182/4, Loss: 795.2203979492188\n","Train -> sample/numSamples/epoch: 655/1182/4, Loss: 731.3316650390625\n","Train -> sample/numSamples/epoch: 656/1182/4, Loss: 1000.6919555664062\n","Train -> sample/numSamples/epoch: 657/1182/4, Loss: 1322.3953857421875\n","Train -> sample/numSamples/epoch: 658/1182/4, Loss: 566.752685546875\n","Train -> sample/numSamples/epoch: 659/1182/4, Loss: 1062.65380859375\n","Train -> sample/numSamples/epoch: 660/1182/4, Loss: 1026.06982421875\n","Train -> sample/numSamples/epoch: 661/1182/4, Loss: 1137.4925537109375\n","Train -> sample/numSamples/epoch: 662/1182/4, Loss: 1089.5263671875\n","Train -> sample/numSamples/epoch: 663/1182/4, Loss: 1025.28173828125\n","Train -> sample/numSamples/epoch: 664/1182/4, Loss: 1149.3707275390625\n","Train -> sample/numSamples/epoch: 665/1182/4, Loss: 803.388427734375\n","Train -> sample/numSamples/epoch: 666/1182/4, Loss: 2053.351806640625\n","Train -> sample/numSamples/epoch: 667/1182/4, Loss: 1096.763916015625\n","Train -> sample/numSamples/epoch: 668/1182/4, Loss: 1074.1951904296875\n","Train -> sample/numSamples/epoch: 669/1182/4, Loss: 1163.0098876953125\n","Train -> sample/numSamples/epoch: 670/1182/4, Loss: 889.10107421875\n","Train -> sample/numSamples/epoch: 671/1182/4, Loss: 839.4618530273438\n","imatge guardada\n","Train -> sample/numSamples/epoch: 672/1182/4, Loss: 1034.96875\n","Train -> sample/numSamples/epoch: 673/1182/4, Loss: 1034.0870361328125\n","Train -> sample/numSamples/epoch: 674/1182/4, Loss: 1123.9859619140625\n","Train -> sample/numSamples/epoch: 675/1182/4, Loss: 1035.834716796875\n","Train -> sample/numSamples/epoch: 676/1182/4, Loss: 1062.498291015625\n","Train -> sample/numSamples/epoch: 677/1182/4, Loss: 1155.62451171875\n","Train -> sample/numSamples/epoch: 678/1182/4, Loss: 1146.0340576171875\n","Train -> sample/numSamples/epoch: 679/1182/4, Loss: 1421.1097412109375\n","Train -> sample/numSamples/epoch: 680/1182/4, Loss: 1079.562255859375\n","Train -> sample/numSamples/epoch: 681/1182/4, Loss: 1093.3486328125\n","Train -> sample/numSamples/epoch: 682/1182/4, Loss: 611.9965209960938\n","Train -> sample/numSamples/epoch: 683/1182/4, Loss: 830.28076171875\n","Train -> sample/numSamples/epoch: 684/1182/4, Loss: 843.8662719726562\n","Train -> sample/numSamples/epoch: 685/1182/4, Loss: 1370.30126953125\n","Train -> sample/numSamples/epoch: 686/1182/4, Loss: 883.0725708007812\n","Train -> sample/numSamples/epoch: 687/1182/4, Loss: 1433.47802734375\n","Train -> sample/numSamples/epoch: 688/1182/4, Loss: 960.2658081054688\n","Train -> sample/numSamples/epoch: 689/1182/4, Loss: 773.748779296875\n","Train -> sample/numSamples/epoch: 690/1182/4, Loss: 1124.13037109375\n","Train -> sample/numSamples/epoch: 691/1182/4, Loss: 1261.9515380859375\n","Train -> sample/numSamples/epoch: 692/1182/4, Loss: 875.8543090820312\n","Train -> sample/numSamples/epoch: 693/1182/4, Loss: 1015.3717041015625\n","Train -> sample/numSamples/epoch: 694/1182/4, Loss: 1201.7288818359375\n","Train -> sample/numSamples/epoch: 695/1182/4, Loss: 1112.09326171875\n","Train -> sample/numSamples/epoch: 696/1182/4, Loss: 1128.302978515625\n","Train -> sample/numSamples/epoch: 697/1182/4, Loss: 1065.507568359375\n","Train -> sample/numSamples/epoch: 698/1182/4, Loss: 1190.62939453125\n","Train -> sample/numSamples/epoch: 699/1182/4, Loss: 1504.110107421875\n","Train -> sample/numSamples/epoch: 700/1182/4, Loss: 553.94677734375\n","Train -> sample/numSamples/epoch: 701/1182/4, Loss: 1048.6031494140625\n","Train -> sample/numSamples/epoch: 702/1182/4, Loss: 888.3313598632812\n","Train -> sample/numSamples/epoch: 703/1182/4, Loss: 884.064453125\n","Train -> sample/numSamples/epoch: 704/1182/4, Loss: 1201.3240966796875\n","Train -> sample/numSamples/epoch: 705/1182/4, Loss: 806.61767578125\n","Train -> sample/numSamples/epoch: 706/1182/4, Loss: 800.4566040039062\n","Train -> sample/numSamples/epoch: 707/1182/4, Loss: 597.4315795898438\n","Train -> sample/numSamples/epoch: 708/1182/4, Loss: 788.41455078125\n","Train -> sample/numSamples/epoch: 709/1182/4, Loss: 812.28857421875\n","Train -> sample/numSamples/epoch: 710/1182/4, Loss: 1064.06689453125\n","Train -> sample/numSamples/epoch: 711/1182/4, Loss: 1018.8212890625\n","Train -> sample/numSamples/epoch: 712/1182/4, Loss: 781.483154296875\n","Train -> sample/numSamples/epoch: 713/1182/4, Loss: 944.0423583984375\n","Train -> sample/numSamples/epoch: 714/1182/4, Loss: 1180.059814453125\n","Train -> sample/numSamples/epoch: 715/1182/4, Loss: 850.0660400390625\n","Train -> sample/numSamples/epoch: 716/1182/4, Loss: 929.578857421875\n","Train -> sample/numSamples/epoch: 717/1182/4, Loss: 835.3113403320312\n","Train -> sample/numSamples/epoch: 718/1182/4, Loss: 881.7109985351562\n","Train -> sample/numSamples/epoch: 719/1182/4, Loss: 682.5927124023438\n","Train -> sample/numSamples/epoch: 720/1182/4, Loss: 1704.8812255859375\n","Train -> sample/numSamples/epoch: 721/1182/4, Loss: 1372.45654296875\n","Train -> sample/numSamples/epoch: 722/1182/4, Loss: 1028.899658203125\n","Train -> sample/numSamples/epoch: 723/1182/4, Loss: 471.1527099609375\n","Train -> sample/numSamples/epoch: 724/1182/4, Loss: 1003.3485107421875\n","Train -> sample/numSamples/epoch: 725/1182/4, Loss: 821.9852905273438\n","Train -> sample/numSamples/epoch: 726/1182/4, Loss: 1125.192138671875\n","Train -> sample/numSamples/epoch: 727/1182/4, Loss: 1217.0853271484375\n","Train -> sample/numSamples/epoch: 728/1182/4, Loss: 1421.9722900390625\n","Train -> sample/numSamples/epoch: 729/1182/4, Loss: 1295.88623046875\n","Train -> sample/numSamples/epoch: 730/1182/4, Loss: 749.4008178710938\n","Train -> sample/numSamples/epoch: 731/1182/4, Loss: 1077.7032470703125\n","Train -> sample/numSamples/epoch: 732/1182/4, Loss: 571.9225463867188\n","Train -> sample/numSamples/epoch: 733/1182/4, Loss: 896.0244140625\n","Train -> sample/numSamples/epoch: 734/1182/4, Loss: 903.2892456054688\n","Train -> sample/numSamples/epoch: 735/1182/4, Loss: 1044.251220703125\n","Train -> sample/numSamples/epoch: 736/1182/4, Loss: 1252.407470703125\n","Train -> sample/numSamples/epoch: 737/1182/4, Loss: 1541.291015625\n","Train -> sample/numSamples/epoch: 738/1182/4, Loss: 723.4889526367188\n","Train -> sample/numSamples/epoch: 739/1182/4, Loss: 1479.0009765625\n","Train -> sample/numSamples/epoch: 740/1182/4, Loss: 1103.849609375\n","Train -> sample/numSamples/epoch: 741/1182/4, Loss: 1014.8753662109375\n","Train -> sample/numSamples/epoch: 742/1182/4, Loss: 736.0250854492188\n","Train -> sample/numSamples/epoch: 743/1182/4, Loss: 500.1103210449219\n","Train -> sample/numSamples/epoch: 744/1182/4, Loss: 1490.17578125\n","Train -> sample/numSamples/epoch: 745/1182/4, Loss: 803.9557495117188\n","Train -> sample/numSamples/epoch: 746/1182/4, Loss: 1400.43896484375\n","Train -> sample/numSamples/epoch: 747/1182/4, Loss: 1332.85009765625\n","Train -> sample/numSamples/epoch: 748/1182/4, Loss: 659.03955078125\n","Train -> sample/numSamples/epoch: 749/1182/4, Loss: 1559.0125732421875\n","Train -> sample/numSamples/epoch: 750/1182/4, Loss: 769.5424194335938\n","Train -> sample/numSamples/epoch: 751/1182/4, Loss: 1149.6773681640625\n","Train -> sample/numSamples/epoch: 752/1182/4, Loss: 1153.792236328125\n","Train -> sample/numSamples/epoch: 753/1182/4, Loss: 1626.4244384765625\n","Train -> sample/numSamples/epoch: 754/1182/4, Loss: 1095.3291015625\n","Train -> sample/numSamples/epoch: 755/1182/4, Loss: 1402.8983154296875\n","Train -> sample/numSamples/epoch: 756/1182/4, Loss: 1117.1644287109375\n","Train -> sample/numSamples/epoch: 757/1182/4, Loss: 1116.0648193359375\n","Train -> sample/numSamples/epoch: 758/1182/4, Loss: 1112.275390625\n","Train -> sample/numSamples/epoch: 759/1182/4, Loss: 838.111083984375\n","Train -> sample/numSamples/epoch: 760/1182/4, Loss: 915.8446655273438\n","Train -> sample/numSamples/epoch: 761/1182/4, Loss: 1258.0787353515625\n","Train -> sample/numSamples/epoch: 762/1182/4, Loss: 743.2759399414062\n","Train -> sample/numSamples/epoch: 763/1182/4, Loss: 739.7446899414062\n","Train -> sample/numSamples/epoch: 764/1182/4, Loss: 1101.20068359375\n","Train -> sample/numSamples/epoch: 765/1182/4, Loss: 1218.813720703125\n","Train -> sample/numSamples/epoch: 766/1182/4, Loss: 1313.6888427734375\n","Train -> sample/numSamples/epoch: 767/1182/4, Loss: 1460.41162109375\n","Train -> sample/numSamples/epoch: 768/1182/4, Loss: 1009.5049438476562\n","Train -> sample/numSamples/epoch: 769/1182/4, Loss: 1079.399658203125\n","Train -> sample/numSamples/epoch: 770/1182/4, Loss: 710.9420166015625\n","Train -> sample/numSamples/epoch: 771/1182/4, Loss: 760.5165405273438\n","imatge guardada\n","Train -> sample/numSamples/epoch: 772/1182/4, Loss: 771.0946655273438\n","Train -> sample/numSamples/epoch: 773/1182/4, Loss: 404.5272521972656\n","Train -> sample/numSamples/epoch: 774/1182/4, Loss: 821.339599609375\n","Train -> sample/numSamples/epoch: 775/1182/4, Loss: 891.8932495117188\n","Train -> sample/numSamples/epoch: 776/1182/4, Loss: 796.0443725585938\n","Train -> sample/numSamples/epoch: 777/1182/4, Loss: 1058.8441162109375\n","Train -> sample/numSamples/epoch: 778/1182/4, Loss: 1152.667724609375\n","Train -> sample/numSamples/epoch: 779/1182/4, Loss: 1157.8258056640625\n","Train -> sample/numSamples/epoch: 780/1182/4, Loss: 709.8025512695312\n","Train -> sample/numSamples/epoch: 781/1182/4, Loss: 849.8505859375\n","Train -> sample/numSamples/epoch: 782/1182/4, Loss: 698.5646362304688\n","Train -> sample/numSamples/epoch: 783/1182/4, Loss: 560.8265380859375\n","Train -> sample/numSamples/epoch: 784/1182/4, Loss: 1160.203125\n","Train -> sample/numSamples/epoch: 785/1182/4, Loss: 1436.890625\n","Train -> sample/numSamples/epoch: 786/1182/4, Loss: 943.5277099609375\n","Train -> sample/numSamples/epoch: 787/1182/4, Loss: 1634.589111328125\n","Train -> sample/numSamples/epoch: 788/1182/4, Loss: 1228.974609375\n","Train -> sample/numSamples/epoch: 789/1182/4, Loss: 701.3171997070312\n","Train -> sample/numSamples/epoch: 790/1182/4, Loss: 1029.05859375\n","Train -> sample/numSamples/epoch: 791/1182/4, Loss: 1090.7745361328125\n","Train -> sample/numSamples/epoch: 792/1182/4, Loss: 1158.4346923828125\n","Train -> sample/numSamples/epoch: 793/1182/4, Loss: 1036.9666748046875\n","Train -> sample/numSamples/epoch: 794/1182/4, Loss: 733.5701904296875\n","Train -> sample/numSamples/epoch: 795/1182/4, Loss: 1421.1844482421875\n","Train -> sample/numSamples/epoch: 796/1182/4, Loss: 865.3858642578125\n","Train -> sample/numSamples/epoch: 797/1182/4, Loss: 964.8733520507812\n","Train -> sample/numSamples/epoch: 798/1182/4, Loss: 1378.37255859375\n","Train -> sample/numSamples/epoch: 799/1182/4, Loss: 1131.86767578125\n","Train -> sample/numSamples/epoch: 800/1182/4, Loss: 584.2053833007812\n","Train -> sample/numSamples/epoch: 801/1182/4, Loss: 1165.5274658203125\n","Train -> sample/numSamples/epoch: 802/1182/4, Loss: 815.322021484375\n","Train -> sample/numSamples/epoch: 803/1182/4, Loss: 1284.1624755859375\n","Train -> sample/numSamples/epoch: 804/1182/4, Loss: 1526.13818359375\n","Train -> sample/numSamples/epoch: 805/1182/4, Loss: 1031.12109375\n","Train -> sample/numSamples/epoch: 806/1182/4, Loss: 645.86474609375\n","Train -> sample/numSamples/epoch: 807/1182/4, Loss: 1014.0676879882812\n","Train -> sample/numSamples/epoch: 808/1182/4, Loss: 868.9983520507812\n","Train -> sample/numSamples/epoch: 809/1182/4, Loss: 1385.2078857421875\n","Train -> sample/numSamples/epoch: 810/1182/4, Loss: 1085.5899658203125\n","Train -> sample/numSamples/epoch: 811/1182/4, Loss: 1170.39013671875\n","Train -> sample/numSamples/epoch: 812/1182/4, Loss: 1085.529052734375\n","Train -> sample/numSamples/epoch: 813/1182/4, Loss: 1280.2281494140625\n","Train -> sample/numSamples/epoch: 814/1182/4, Loss: 821.7998657226562\n","Train -> sample/numSamples/epoch: 815/1182/4, Loss: 1174.82373046875\n","Train -> sample/numSamples/epoch: 816/1182/4, Loss: 1073.4215087890625\n","Train -> sample/numSamples/epoch: 817/1182/4, Loss: 1506.8212890625\n","Train -> sample/numSamples/epoch: 818/1182/4, Loss: 1210.7237548828125\n","Train -> sample/numSamples/epoch: 819/1182/4, Loss: 1219.705322265625\n","Train -> sample/numSamples/epoch: 820/1182/4, Loss: 938.0972900390625\n","Train -> sample/numSamples/epoch: 821/1182/4, Loss: 855.49853515625\n","Train -> sample/numSamples/epoch: 822/1182/4, Loss: 1052.012451171875\n","Train -> sample/numSamples/epoch: 823/1182/4, Loss: 1012.4659423828125\n","Train -> sample/numSamples/epoch: 824/1182/4, Loss: 957.7306518554688\n","Train -> sample/numSamples/epoch: 825/1182/4, Loss: 1202.380126953125\n","Train -> sample/numSamples/epoch: 826/1182/4, Loss: 497.16729736328125\n","Train -> sample/numSamples/epoch: 827/1182/4, Loss: 1044.7147216796875\n","Train -> sample/numSamples/epoch: 828/1182/4, Loss: 1010.6268920898438\n","Train -> sample/numSamples/epoch: 829/1182/4, Loss: 1407.3988037109375\n","Train -> sample/numSamples/epoch: 830/1182/4, Loss: 1061.695068359375\n","Train -> sample/numSamples/epoch: 831/1182/4, Loss: 1126.98095703125\n","Train -> sample/numSamples/epoch: 832/1182/4, Loss: 836.7448120117188\n","Train -> sample/numSamples/epoch: 833/1182/4, Loss: 749.0618896484375\n","Train -> sample/numSamples/epoch: 834/1182/4, Loss: 931.3834838867188\n","Train -> sample/numSamples/epoch: 835/1182/4, Loss: 1106.366943359375\n","Train -> sample/numSamples/epoch: 836/1182/4, Loss: 1161.5970458984375\n","Train -> sample/numSamples/epoch: 837/1182/4, Loss: 562.624755859375\n","Train -> sample/numSamples/epoch: 838/1182/4, Loss: 733.1694946289062\n","Train -> sample/numSamples/epoch: 839/1182/4, Loss: 501.8848876953125\n","Train -> sample/numSamples/epoch: 840/1182/4, Loss: 842.7418212890625\n","Train -> sample/numSamples/epoch: 841/1182/4, Loss: 819.37841796875\n","Train -> sample/numSamples/epoch: 842/1182/4, Loss: 944.4197387695312\n","Train -> sample/numSamples/epoch: 843/1182/4, Loss: 826.25390625\n","Train -> sample/numSamples/epoch: 844/1182/4, Loss: 776.260498046875\n","Train -> sample/numSamples/epoch: 845/1182/4, Loss: 825.47509765625\n","Train -> sample/numSamples/epoch: 846/1182/4, Loss: 1030.57470703125\n","Train -> sample/numSamples/epoch: 847/1182/4, Loss: 1210.7781982421875\n","Train -> sample/numSamples/epoch: 848/1182/4, Loss: 915.6553955078125\n","Train -> sample/numSamples/epoch: 849/1182/4, Loss: 1040.4788818359375\n","Train -> sample/numSamples/epoch: 850/1182/4, Loss: 1253.5118408203125\n","Train -> sample/numSamples/epoch: 851/1182/4, Loss: 1315.2686767578125\n","Train -> sample/numSamples/epoch: 852/1182/4, Loss: 1747.031982421875\n","Train -> sample/numSamples/epoch: 853/1182/4, Loss: 816.1010131835938\n","Train -> sample/numSamples/epoch: 854/1182/4, Loss: 1047.59912109375\n","Train -> sample/numSamples/epoch: 855/1182/4, Loss: 1270.89404296875\n","Train -> sample/numSamples/epoch: 856/1182/4, Loss: 645.6058349609375\n","Train -> sample/numSamples/epoch: 857/1182/4, Loss: 1043.4945068359375\n","Train -> sample/numSamples/epoch: 858/1182/4, Loss: 1162.9266357421875\n","Train -> sample/numSamples/epoch: 859/1182/4, Loss: 1106.6427001953125\n","Train -> sample/numSamples/epoch: 860/1182/4, Loss: 708.3779296875\n","Train -> sample/numSamples/epoch: 861/1182/4, Loss: 1085.636962890625\n","Train -> sample/numSamples/epoch: 862/1182/4, Loss: 1416.0804443359375\n","Train -> sample/numSamples/epoch: 863/1182/4, Loss: 923.303466796875\n","Train -> sample/numSamples/epoch: 864/1182/4, Loss: 1336.1796875\n","Train -> sample/numSamples/epoch: 865/1182/4, Loss: 1067.3699951171875\n","Train -> sample/numSamples/epoch: 866/1182/4, Loss: 732.9806518554688\n","Train -> sample/numSamples/epoch: 867/1182/4, Loss: 903.5863037109375\n","Train -> sample/numSamples/epoch: 868/1182/4, Loss: 1160.62060546875\n","Train -> sample/numSamples/epoch: 869/1182/4, Loss: 607.5278930664062\n","Train -> sample/numSamples/epoch: 870/1182/4, Loss: 535.4652099609375\n","Train -> sample/numSamples/epoch: 871/1182/4, Loss: 1262.2415771484375\n","imatge guardada\n","Train -> sample/numSamples/epoch: 872/1182/4, Loss: 891.6510009765625\n","Train -> sample/numSamples/epoch: 873/1182/4, Loss: 1031.4554443359375\n","Train -> sample/numSamples/epoch: 874/1182/4, Loss: 1183.3536376953125\n","Train -> sample/numSamples/epoch: 875/1182/4, Loss: 841.6776733398438\n","Train -> sample/numSamples/epoch: 876/1182/4, Loss: 810.9571533203125\n","Train -> sample/numSamples/epoch: 877/1182/4, Loss: 1130.108642578125\n","Train -> sample/numSamples/epoch: 878/1182/4, Loss: 886.1328735351562\n","Train -> sample/numSamples/epoch: 879/1182/4, Loss: 1299.826904296875\n","Train -> sample/numSamples/epoch: 880/1182/4, Loss: 1004.6029663085938\n","Train -> sample/numSamples/epoch: 881/1182/4, Loss: 923.1591186523438\n","Train -> sample/numSamples/epoch: 882/1182/4, Loss: 974.9716796875\n","Train -> sample/numSamples/epoch: 883/1182/4, Loss: 1251.1466064453125\n","Train -> sample/numSamples/epoch: 884/1182/4, Loss: 1288.1419677734375\n","Train -> sample/numSamples/epoch: 885/1182/4, Loss: 1410.7076416015625\n","Train -> sample/numSamples/epoch: 886/1182/4, Loss: 903.3160400390625\n","Train -> sample/numSamples/epoch: 887/1182/4, Loss: 1036.473876953125\n","Train -> sample/numSamples/epoch: 888/1182/4, Loss: 1102.42822265625\n","Train -> sample/numSamples/epoch: 889/1182/4, Loss: 1128.467041015625\n","Train -> sample/numSamples/epoch: 890/1182/4, Loss: 1612.048583984375\n","Train -> sample/numSamples/epoch: 891/1182/4, Loss: 1012.9888916015625\n","Train -> sample/numSamples/epoch: 892/1182/4, Loss: 903.035400390625\n","Train -> sample/numSamples/epoch: 893/1182/4, Loss: 1042.365478515625\n","Train -> sample/numSamples/epoch: 894/1182/4, Loss: 986.62841796875\n","Train -> sample/numSamples/epoch: 895/1182/4, Loss: 1110.700927734375\n","Train -> sample/numSamples/epoch: 896/1182/4, Loss: 1073.6646728515625\n","Train -> sample/numSamples/epoch: 897/1182/4, Loss: 316.5464782714844\n","Train -> sample/numSamples/epoch: 898/1182/4, Loss: 874.4786376953125\n","Train -> sample/numSamples/epoch: 899/1182/4, Loss: 850.9749755859375\n","Train -> sample/numSamples/epoch: 900/1182/4, Loss: 786.454345703125\n","Train -> sample/numSamples/epoch: 901/1182/4, Loss: 973.4141235351562\n","Train -> sample/numSamples/epoch: 902/1182/4, Loss: 707.1828002929688\n","Train -> sample/numSamples/epoch: 903/1182/4, Loss: 857.2345581054688\n","Train -> sample/numSamples/epoch: 904/1182/4, Loss: 932.7872314453125\n","Train -> sample/numSamples/epoch: 905/1182/4, Loss: 1547.282958984375\n","Train -> sample/numSamples/epoch: 906/1182/4, Loss: 677.2337036132812\n","Train -> sample/numSamples/epoch: 907/1182/4, Loss: 992.3807373046875\n","Train -> sample/numSamples/epoch: 908/1182/4, Loss: 755.2835083007812\n","Train -> sample/numSamples/epoch: 909/1182/4, Loss: 911.9129638671875\n","Train -> sample/numSamples/epoch: 910/1182/4, Loss: 948.3584594726562\n","Train -> sample/numSamples/epoch: 911/1182/4, Loss: 1183.0411376953125\n","Train -> sample/numSamples/epoch: 912/1182/4, Loss: 1338.5325927734375\n","Train -> sample/numSamples/epoch: 913/1182/4, Loss: 1022.3700561523438\n","Train -> sample/numSamples/epoch: 914/1182/4, Loss: 952.1337890625\n","Train -> sample/numSamples/epoch: 915/1182/4, Loss: 997.6947631835938\n","Train -> sample/numSamples/epoch: 916/1182/4, Loss: 1478.6328125\n","Train -> sample/numSamples/epoch: 917/1182/4, Loss: 1367.6300048828125\n","Train -> sample/numSamples/epoch: 918/1182/4, Loss: 1301.1622314453125\n","Train -> sample/numSamples/epoch: 919/1182/4, Loss: 1090.82421875\n","Train -> sample/numSamples/epoch: 920/1182/4, Loss: 941.9055786132812\n","Train -> sample/numSamples/epoch: 921/1182/4, Loss: 985.623046875\n","Train -> sample/numSamples/epoch: 922/1182/4, Loss: 1115.0152587890625\n","Train -> sample/numSamples/epoch: 923/1182/4, Loss: 1228.235107421875\n","Train -> sample/numSamples/epoch: 924/1182/4, Loss: 1162.1014404296875\n","Train -> sample/numSamples/epoch: 925/1182/4, Loss: 1198.1766357421875\n","Train -> sample/numSamples/epoch: 926/1182/4, Loss: 1485.2034912109375\n","Train -> sample/numSamples/epoch: 927/1182/4, Loss: 1021.5335083007812\n","Train -> sample/numSamples/epoch: 928/1182/4, Loss: 917.0625610351562\n","Train -> sample/numSamples/epoch: 929/1182/4, Loss: 786.4005126953125\n","Train -> sample/numSamples/epoch: 930/1182/4, Loss: 1259.2530517578125\n","Train -> sample/numSamples/epoch: 931/1182/4, Loss: 1070.738037109375\n","Train -> sample/numSamples/epoch: 932/1182/4, Loss: 1176.0604248046875\n","Train -> sample/numSamples/epoch: 933/1182/4, Loss: 1086.19677734375\n","Train -> sample/numSamples/epoch: 934/1182/4, Loss: 1434.7625732421875\n","Train -> sample/numSamples/epoch: 935/1182/4, Loss: 1061.55908203125\n","Train -> sample/numSamples/epoch: 936/1182/4, Loss: 653.8919677734375\n","Train -> sample/numSamples/epoch: 937/1182/4, Loss: 1073.37255859375\n","Train -> sample/numSamples/epoch: 938/1182/4, Loss: 961.2241821289062\n","Train -> sample/numSamples/epoch: 939/1182/4, Loss: 937.3582763671875\n","Train -> sample/numSamples/epoch: 940/1182/4, Loss: 1190.525634765625\n","Train -> sample/numSamples/epoch: 941/1182/4, Loss: 1293.6375732421875\n","Train -> sample/numSamples/epoch: 942/1182/4, Loss: 1097.9049072265625\n","Train -> sample/numSamples/epoch: 943/1182/4, Loss: 1127.3048095703125\n","Train -> sample/numSamples/epoch: 944/1182/4, Loss: 1053.86669921875\n","Train -> sample/numSamples/epoch: 945/1182/4, Loss: 1053.640625\n","Train -> sample/numSamples/epoch: 946/1182/4, Loss: 1115.2899169921875\n","Train -> sample/numSamples/epoch: 947/1182/4, Loss: 863.8361206054688\n","Train -> sample/numSamples/epoch: 948/1182/4, Loss: 723.7838134765625\n","Train -> sample/numSamples/epoch: 949/1182/4, Loss: 1314.1707763671875\n","Train -> sample/numSamples/epoch: 950/1182/4, Loss: 1290.99462890625\n","Train -> sample/numSamples/epoch: 951/1182/4, Loss: 732.6459350585938\n","Train -> sample/numSamples/epoch: 952/1182/4, Loss: 1125.6163330078125\n","Train -> sample/numSamples/epoch: 953/1182/4, Loss: 790.9615478515625\n","Train -> sample/numSamples/epoch: 954/1182/4, Loss: 761.2590942382812\n","Train -> sample/numSamples/epoch: 955/1182/4, Loss: 715.8644409179688\n","Train -> sample/numSamples/epoch: 956/1182/4, Loss: 971.7955322265625\n","Train -> sample/numSamples/epoch: 957/1182/4, Loss: 932.7660522460938\n","Train -> sample/numSamples/epoch: 958/1182/4, Loss: 984.79248046875\n","Train -> sample/numSamples/epoch: 959/1182/4, Loss: 966.5193481445312\n","Train -> sample/numSamples/epoch: 960/1182/4, Loss: 1205.5626220703125\n","Train -> sample/numSamples/epoch: 961/1182/4, Loss: 900.7689819335938\n","Train -> sample/numSamples/epoch: 962/1182/4, Loss: 1221.8118896484375\n","Train -> sample/numSamples/epoch: 963/1182/4, Loss: 1261.684326171875\n","Train -> sample/numSamples/epoch: 964/1182/4, Loss: 716.8993530273438\n","Train -> sample/numSamples/epoch: 965/1182/4, Loss: 1420.84326171875\n","Train -> sample/numSamples/epoch: 966/1182/4, Loss: 1086.1112060546875\n","Train -> sample/numSamples/epoch: 967/1182/4, Loss: 1198.0220947265625\n","Train -> sample/numSamples/epoch: 968/1182/4, Loss: 756.5376586914062\n","Train -> sample/numSamples/epoch: 969/1182/4, Loss: 1132.896728515625\n","Train -> sample/numSamples/epoch: 970/1182/4, Loss: 1235.304443359375\n","Train -> sample/numSamples/epoch: 971/1182/4, Loss: 872.880126953125\n","imatge guardada\n","Train -> sample/numSamples/epoch: 972/1182/4, Loss: 1339.0364990234375\n","Train -> sample/numSamples/epoch: 973/1182/4, Loss: 856.4183349609375\n","Train -> sample/numSamples/epoch: 974/1182/4, Loss: 1192.57421875\n","Train -> sample/numSamples/epoch: 975/1182/4, Loss: 574.4114990234375\n","Train -> sample/numSamples/epoch: 976/1182/4, Loss: 839.7416381835938\n","Train -> sample/numSamples/epoch: 977/1182/4, Loss: 879.446044921875\n","Train -> sample/numSamples/epoch: 978/1182/4, Loss: 1323.6339111328125\n","Train -> sample/numSamples/epoch: 979/1182/4, Loss: 1105.0133056640625\n","Train -> sample/numSamples/epoch: 980/1182/4, Loss: 730.7229614257812\n","Train -> sample/numSamples/epoch: 981/1182/4, Loss: 824.8162231445312\n","Train -> sample/numSamples/epoch: 982/1182/4, Loss: 1138.178466796875\n","Train -> sample/numSamples/epoch: 983/1182/4, Loss: 895.1104736328125\n","Train -> sample/numSamples/epoch: 984/1182/4, Loss: 1944.8184814453125\n","Train -> sample/numSamples/epoch: 985/1182/4, Loss: 983.2190551757812\n","Train -> sample/numSamples/epoch: 986/1182/4, Loss: 811.60205078125\n","Train -> sample/numSamples/epoch: 987/1182/4, Loss: 830.9324951171875\n","Train -> sample/numSamples/epoch: 988/1182/4, Loss: 917.6019287109375\n","Train -> sample/numSamples/epoch: 989/1182/4, Loss: 1038.87939453125\n","Train -> sample/numSamples/epoch: 990/1182/4, Loss: 1075.0740966796875\n","Train -> sample/numSamples/epoch: 991/1182/4, Loss: 1539.53515625\n","Train -> sample/numSamples/epoch: 992/1182/4, Loss: 1096.7711181640625\n","Train -> sample/numSamples/epoch: 993/1182/4, Loss: 1324.9630126953125\n","Train -> sample/numSamples/epoch: 994/1182/4, Loss: 675.1864624023438\n","Train -> sample/numSamples/epoch: 995/1182/4, Loss: 1110.5059814453125\n","Train -> sample/numSamples/epoch: 996/1182/4, Loss: 879.354248046875\n","Train -> sample/numSamples/epoch: 997/1182/4, Loss: 1445.4429931640625\n","Train -> sample/numSamples/epoch: 998/1182/4, Loss: 741.7303466796875\n","Train -> sample/numSamples/epoch: 999/1182/4, Loss: 812.4714965820312\n","Train -> sample/numSamples/epoch: 1000/1182/4, Loss: 594.3565063476562\n","Train -> sample/numSamples/epoch: 1001/1182/4, Loss: 742.3560180664062\n","Train -> sample/numSamples/epoch: 1002/1182/4, Loss: 1527.3551025390625\n","Train -> sample/numSamples/epoch: 1003/1182/4, Loss: 952.5945434570312\n","Train -> sample/numSamples/epoch: 1004/1182/4, Loss: 963.2365112304688\n","Train -> sample/numSamples/epoch: 1005/1182/4, Loss: 969.4459838867188\n","Train -> sample/numSamples/epoch: 1006/1182/4, Loss: 1191.0772705078125\n","Train -> sample/numSamples/epoch: 1007/1182/4, Loss: 640.5841674804688\n","Train -> sample/numSamples/epoch: 1008/1182/4, Loss: 1054.04736328125\n","Train -> sample/numSamples/epoch: 1009/1182/4, Loss: 1062.899169921875\n","Train -> sample/numSamples/epoch: 1010/1182/4, Loss: 573.595458984375\n","Train -> sample/numSamples/epoch: 1011/1182/4, Loss: 884.34765625\n","Train -> sample/numSamples/epoch: 1012/1182/4, Loss: 869.2449340820312\n","Train -> sample/numSamples/epoch: 1013/1182/4, Loss: 1353.6328125\n","Train -> sample/numSamples/epoch: 1014/1182/4, Loss: 1349.30810546875\n","Train -> sample/numSamples/epoch: 1015/1182/4, Loss: 1077.4691162109375\n","Train -> sample/numSamples/epoch: 1016/1182/4, Loss: 988.3177490234375\n","Train -> sample/numSamples/epoch: 1017/1182/4, Loss: 1223.6898193359375\n","Train -> sample/numSamples/epoch: 1018/1182/4, Loss: 1105.5760498046875\n","Train -> sample/numSamples/epoch: 1019/1182/4, Loss: 1127.903564453125\n","Train -> sample/numSamples/epoch: 1020/1182/4, Loss: 1146.989013671875\n","Train -> sample/numSamples/epoch: 1021/1182/4, Loss: 1175.2801513671875\n","Train -> sample/numSamples/epoch: 1022/1182/4, Loss: 1022.587890625\n","Train -> sample/numSamples/epoch: 1023/1182/4, Loss: 1003.2808227539062\n","Train -> sample/numSamples/epoch: 1024/1182/4, Loss: 1319.0938720703125\n","Train -> sample/numSamples/epoch: 1025/1182/4, Loss: 923.834228515625\n","Train -> sample/numSamples/epoch: 1026/1182/4, Loss: 1279.2828369140625\n","Train -> sample/numSamples/epoch: 1027/1182/4, Loss: 900.6531982421875\n","Train -> sample/numSamples/epoch: 1028/1182/4, Loss: 864.3045654296875\n","Train -> sample/numSamples/epoch: 1029/1182/4, Loss: 743.7522583007812\n","Train -> sample/numSamples/epoch: 1030/1182/4, Loss: 652.1424560546875\n","Train -> sample/numSamples/epoch: 1031/1182/4, Loss: 975.53759765625\n","Train -> sample/numSamples/epoch: 1032/1182/4, Loss: 911.18359375\n","Train -> sample/numSamples/epoch: 1033/1182/4, Loss: 830.1585693359375\n","Train -> sample/numSamples/epoch: 1034/1182/4, Loss: 964.5396118164062\n","Train -> sample/numSamples/epoch: 1035/1182/4, Loss: 839.2293701171875\n","Train -> sample/numSamples/epoch: 1036/1182/4, Loss: 1417.7198486328125\n","Train -> sample/numSamples/epoch: 1037/1182/4, Loss: 1414.4677734375\n","Train -> sample/numSamples/epoch: 1038/1182/4, Loss: 997.9570922851562\n","Train -> sample/numSamples/epoch: 1039/1182/4, Loss: 1011.0399169921875\n","Train -> sample/numSamples/epoch: 1040/1182/4, Loss: 871.8094482421875\n","Train -> sample/numSamples/epoch: 1041/1182/4, Loss: 796.368896484375\n","Train -> sample/numSamples/epoch: 1042/1182/4, Loss: 778.203857421875\n","Train -> sample/numSamples/epoch: 1043/1182/4, Loss: 1023.6446533203125\n","Train -> sample/numSamples/epoch: 1044/1182/4, Loss: 489.7811584472656\n","Train -> sample/numSamples/epoch: 1045/1182/4, Loss: 882.8560180664062\n","Train -> sample/numSamples/epoch: 1046/1182/4, Loss: 927.47998046875\n","Train -> sample/numSamples/epoch: 1047/1182/4, Loss: 850.3359375\n","Train -> sample/numSamples/epoch: 1048/1182/4, Loss: 1087.461181640625\n","Train -> sample/numSamples/epoch: 1049/1182/4, Loss: 1020.5469360351562\n","Train -> sample/numSamples/epoch: 1050/1182/4, Loss: 1554.360595703125\n","Train -> sample/numSamples/epoch: 1051/1182/4, Loss: 1069.13916015625\n","Train -> sample/numSamples/epoch: 1052/1182/4, Loss: 986.8558959960938\n","Train -> sample/numSamples/epoch: 1053/1182/4, Loss: 978.010986328125\n","Train -> sample/numSamples/epoch: 1054/1182/4, Loss: 792.8170166015625\n","Train -> sample/numSamples/epoch: 1055/1182/4, Loss: 1132.29541015625\n","Train -> sample/numSamples/epoch: 1056/1182/4, Loss: 980.1154174804688\n","Train -> sample/numSamples/epoch: 1057/1182/4, Loss: 1061.5850830078125\n","Train -> sample/numSamples/epoch: 1058/1182/4, Loss: 1088.4644775390625\n","Train -> sample/numSamples/epoch: 1059/1182/4, Loss: 840.8958129882812\n","Train -> sample/numSamples/epoch: 1060/1182/4, Loss: 920.3771362304688\n","Train -> sample/numSamples/epoch: 1061/1182/4, Loss: 1419.4576416015625\n","Train -> sample/numSamples/epoch: 1062/1182/4, Loss: 980.0372314453125\n","Train -> sample/numSamples/epoch: 1063/1182/4, Loss: 965.96142578125\n","Train -> sample/numSamples/epoch: 1064/1182/4, Loss: 1086.9195556640625\n","Train -> sample/numSamples/epoch: 1065/1182/4, Loss: 1072.237548828125\n","Train -> sample/numSamples/epoch: 1066/1182/4, Loss: 901.31494140625\n","Train -> sample/numSamples/epoch: 1067/1182/4, Loss: 1116.4244384765625\n","Train -> sample/numSamples/epoch: 1068/1182/4, Loss: 1082.1944580078125\n","Train -> sample/numSamples/epoch: 1069/1182/4, Loss: 919.6376342773438\n","Train -> sample/numSamples/epoch: 1070/1182/4, Loss: 1312.81787109375\n","Train -> sample/numSamples/epoch: 1071/1182/4, Loss: 1150.002685546875\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1072/1182/4, Loss: 868.1949462890625\n","Train -> sample/numSamples/epoch: 1073/1182/4, Loss: 812.62646484375\n","Train -> sample/numSamples/epoch: 1074/1182/4, Loss: 1157.5147705078125\n","Train -> sample/numSamples/epoch: 1075/1182/4, Loss: 1244.617431640625\n","Train -> sample/numSamples/epoch: 1076/1182/4, Loss: 976.5690307617188\n","Train -> sample/numSamples/epoch: 1077/1182/4, Loss: 866.6611328125\n","Train -> sample/numSamples/epoch: 1078/1182/4, Loss: 876.6363525390625\n","Train -> sample/numSamples/epoch: 1079/1182/4, Loss: 762.5001220703125\n","Train -> sample/numSamples/epoch: 1080/1182/4, Loss: 1266.389892578125\n","Train -> sample/numSamples/epoch: 1081/1182/4, Loss: 922.7609252929688\n","Train -> sample/numSamples/epoch: 1082/1182/4, Loss: 1001.3368530273438\n","Train -> sample/numSamples/epoch: 1083/1182/4, Loss: 932.7485961914062\n","Train -> sample/numSamples/epoch: 1084/1182/4, Loss: 1052.3804931640625\n","Train -> sample/numSamples/epoch: 1085/1182/4, Loss: 1274.4678955078125\n","Train -> sample/numSamples/epoch: 1086/1182/4, Loss: 1228.05419921875\n","Train -> sample/numSamples/epoch: 1087/1182/4, Loss: 917.47705078125\n","Train -> sample/numSamples/epoch: 1088/1182/4, Loss: 1241.4984130859375\n","Train -> sample/numSamples/epoch: 1089/1182/4, Loss: 525.638916015625\n","Train -> sample/numSamples/epoch: 1090/1182/4, Loss: 1269.9195556640625\n","Train -> sample/numSamples/epoch: 1091/1182/4, Loss: 1342.2041015625\n","Train -> sample/numSamples/epoch: 1092/1182/4, Loss: 1056.843994140625\n","Train -> sample/numSamples/epoch: 1093/1182/4, Loss: 1154.834716796875\n","Train -> sample/numSamples/epoch: 1094/1182/4, Loss: 960.1558837890625\n","Train -> sample/numSamples/epoch: 1095/1182/4, Loss: 1095.6226806640625\n","Train -> sample/numSamples/epoch: 1096/1182/4, Loss: 1321.269775390625\n","Train -> sample/numSamples/epoch: 1097/1182/4, Loss: 1230.0390625\n","Train -> sample/numSamples/epoch: 1098/1182/4, Loss: 1030.588623046875\n","Train -> sample/numSamples/epoch: 1099/1182/4, Loss: 630.5179443359375\n","Train -> sample/numSamples/epoch: 1100/1182/4, Loss: 1009.2489013671875\n","Train -> sample/numSamples/epoch: 1101/1182/4, Loss: 1288.780029296875\n","Train -> sample/numSamples/epoch: 1102/1182/4, Loss: 792.7158203125\n","Train -> sample/numSamples/epoch: 1103/1182/4, Loss: 1126.75439453125\n","Train -> sample/numSamples/epoch: 1104/1182/4, Loss: 630.100341796875\n","Train -> sample/numSamples/epoch: 1105/1182/4, Loss: 1264.677490234375\n","Train -> sample/numSamples/epoch: 1106/1182/4, Loss: 983.82373046875\n","Train -> sample/numSamples/epoch: 1107/1182/4, Loss: 851.341064453125\n","Train -> sample/numSamples/epoch: 1108/1182/4, Loss: 749.8389892578125\n","Train -> sample/numSamples/epoch: 1109/1182/4, Loss: 1373.28564453125\n","Train -> sample/numSamples/epoch: 1110/1182/4, Loss: 632.118408203125\n","Train -> sample/numSamples/epoch: 1111/1182/4, Loss: 891.1367797851562\n","Train -> sample/numSamples/epoch: 1112/1182/4, Loss: 595.96728515625\n","Train -> sample/numSamples/epoch: 1113/1182/4, Loss: 1162.33837890625\n","Train -> sample/numSamples/epoch: 1114/1182/4, Loss: 632.4957885742188\n","Train -> sample/numSamples/epoch: 1115/1182/4, Loss: 829.4127197265625\n","Train -> sample/numSamples/epoch: 1116/1182/4, Loss: 910.1965942382812\n","Train -> sample/numSamples/epoch: 1117/1182/4, Loss: 830.454833984375\n","Train -> sample/numSamples/epoch: 1118/1182/4, Loss: 986.433349609375\n","Train -> sample/numSamples/epoch: 1119/1182/4, Loss: 523.4313354492188\n","Train -> sample/numSamples/epoch: 1120/1182/4, Loss: 1041.7388916015625\n","Train -> sample/numSamples/epoch: 1121/1182/4, Loss: 1022.7265014648438\n","Train -> sample/numSamples/epoch: 1122/1182/4, Loss: 1059.3406982421875\n","Train -> sample/numSamples/epoch: 1123/1182/4, Loss: 853.553955078125\n","Train -> sample/numSamples/epoch: 1124/1182/4, Loss: 742.8859252929688\n","Train -> sample/numSamples/epoch: 1125/1182/4, Loss: 1075.14794921875\n","Train -> sample/numSamples/epoch: 1126/1182/4, Loss: 1572.0635986328125\n","Train -> sample/numSamples/epoch: 1127/1182/4, Loss: 1198.2672119140625\n","Train -> sample/numSamples/epoch: 1128/1182/4, Loss: 973.068359375\n","Train -> sample/numSamples/epoch: 1129/1182/4, Loss: 1151.7696533203125\n","Train -> sample/numSamples/epoch: 1130/1182/4, Loss: 1021.0966186523438\n","Train -> sample/numSamples/epoch: 1131/1182/4, Loss: 1224.805908203125\n","Train -> sample/numSamples/epoch: 1132/1182/4, Loss: 843.9494018554688\n","Train -> sample/numSamples/epoch: 1133/1182/4, Loss: 810.2431030273438\n","Train -> sample/numSamples/epoch: 1134/1182/4, Loss: 1216.9619140625\n","Train -> sample/numSamples/epoch: 1135/1182/4, Loss: 1303.65478515625\n","Train -> sample/numSamples/epoch: 1136/1182/4, Loss: 985.546142578125\n","Train -> sample/numSamples/epoch: 1137/1182/4, Loss: 1289.47705078125\n","Train -> sample/numSamples/epoch: 1138/1182/4, Loss: 610.3447265625\n","Train -> sample/numSamples/epoch: 1139/1182/4, Loss: 1359.239990234375\n","Train -> sample/numSamples/epoch: 1140/1182/4, Loss: 667.2529296875\n","Train -> sample/numSamples/epoch: 1141/1182/4, Loss: 875.2710571289062\n","Train -> sample/numSamples/epoch: 1142/1182/4, Loss: 928.4175415039062\n","Train -> sample/numSamples/epoch: 1143/1182/4, Loss: 935.9628295898438\n","Train -> sample/numSamples/epoch: 1144/1182/4, Loss: 910.4274291992188\n","Train -> sample/numSamples/epoch: 1145/1182/4, Loss: 1005.7171020507812\n","Train -> sample/numSamples/epoch: 1146/1182/4, Loss: 995.6625366210938\n","Train -> sample/numSamples/epoch: 1147/1182/4, Loss: 763.5667724609375\n","Train -> sample/numSamples/epoch: 1148/1182/4, Loss: 976.735107421875\n","Train -> sample/numSamples/epoch: 1149/1182/4, Loss: 956.2379760742188\n","Train -> sample/numSamples/epoch: 1150/1182/4, Loss: 1312.71337890625\n","Train -> sample/numSamples/epoch: 1151/1182/4, Loss: 950.7544555664062\n","Train -> sample/numSamples/epoch: 1152/1182/4, Loss: 1196.83251953125\n","Train -> sample/numSamples/epoch: 1153/1182/4, Loss: 1754.2469482421875\n","Train -> sample/numSamples/epoch: 1154/1182/4, Loss: 708.55712890625\n","Train -> sample/numSamples/epoch: 1155/1182/4, Loss: 1251.0245361328125\n","Train -> sample/numSamples/epoch: 1156/1182/4, Loss: 918.8102416992188\n","Train -> sample/numSamples/epoch: 1157/1182/4, Loss: 1264.4591064453125\n","Train -> sample/numSamples/epoch: 1158/1182/4, Loss: 1362.358642578125\n","Train -> sample/numSamples/epoch: 1159/1182/4, Loss: 1066.947509765625\n","Train -> sample/numSamples/epoch: 1160/1182/4, Loss: 1092.777099609375\n","Train -> sample/numSamples/epoch: 1161/1182/4, Loss: 1314.8890380859375\n","Train -> sample/numSamples/epoch: 1162/1182/4, Loss: 1008.7275390625\n","Train -> sample/numSamples/epoch: 1163/1182/4, Loss: 1486.7684326171875\n","Train -> sample/numSamples/epoch: 1164/1182/4, Loss: 549.9727172851562\n","Train -> sample/numSamples/epoch: 1165/1182/4, Loss: 552.9528198242188\n","Train -> sample/numSamples/epoch: 1166/1182/4, Loss: 801.5738525390625\n","Train -> sample/numSamples/epoch: 1167/1182/4, Loss: 1216.7001953125\n","Train -> sample/numSamples/epoch: 1168/1182/4, Loss: 570.5901489257812\n","Train -> sample/numSamples/epoch: 1169/1182/4, Loss: 884.5935668945312\n","Train -> sample/numSamples/epoch: 1170/1182/4, Loss: 643.597900390625\n","Train -> sample/numSamples/epoch: 1171/1182/4, Loss: 952.1500244140625\n","imatge guardada\n","Train -> sample/numSamples/epoch: 1172/1182/4, Loss: 1425.643798828125\n","Train -> sample/numSamples/epoch: 1173/1182/4, Loss: 1148.052490234375\n","Train -> sample/numSamples/epoch: 1174/1182/4, Loss: 947.9197387695312\n","Train -> sample/numSamples/epoch: 1175/1182/4, Loss: 1380.2435302734375\n","Train -> sample/numSamples/epoch: 1176/1182/4, Loss: 1286.34716796875\n","Train -> sample/numSamples/epoch: 1177/1182/4, Loss: 915.683349609375\n","Train -> sample/numSamples/epoch: 1178/1182/4, Loss: 1015.1010131835938\n","Train -> sample/numSamples/epoch: 1179/1182/4, Loss: 998.500732421875\n","Train -> sample/numSamples/epoch: 1180/1182/4, Loss: 1015.9522094726562\n","Train -> sample/numSamples/epoch: 1181/1182/4, Loss: 1148.3314208984375\n"]}],"source":["max_epochs = 5\n","for epoch in range(max_epochs):\n","    # Training\n","    train(epoch, training_generator, unet, criterion, optimizer, device)          "]},{"cell_type":"code","execution_count":18,"metadata":{"id":"5t9dEnL_3eO1","outputId":"294300e2-d207-4525-e8e3-45309541b52d","trusted":true},"outputs":[],"source":["torch.save(unet, 'model.pth')"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"DKJ7oeagl453","trusted":true},"outputs":[],"source":["torch.save(unet.state_dict(), 'model_weights.pth')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["model = UNet(3)\n","model.load_state_dict(torch.load('model_weights.pth'))\n","model.to(device)\n","print()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def accuracy(im1, im2):\n","    i1 = im1.cpu().detach().numpy()\n","    i2 = im2.cpu().detach().numpy()\n","    res = cv2.absdiff(i1, i2)\n","    #res = res.astype(np.uint8)\n","    percentage = (np.count_nonzero(res) * 100)/ res.size\n","    return res#percentage"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def validation(epoch, dataloader, model, criterion, dev):\n","    model.eval()\n","    total_losses = []\n","    lensamples = len(dataloader)\n","    for i_batch, sample_batched in enumerate(dataloader):\n","        images = sample_batched['img'].to(device=dev, dtype=torch.float)\n","        ground = sample_batched['gth'].to(device=dev, dtype=torch.float)\n","        n_iter = epoch*lensamples + i_batch\n","        \n","        output = model(images)\n","\n","        if n_iter%100==0:\n","            xi = vutils.make_grid(images, normalize=True, scale_each=True)\n","            xl = vutils.make_grid(ground, normalize=True, scale_each=True)\n","            xo = vutils.make_grid(output, normalize=True, scale_each=True)\n","            x = torch.cat((xi,xl,xo),1)\n","            writer.add_image('validation/output', x, n_iter)\n","            \n","        loss = criterion(output, ground)\n","\n","        writer.add_scalar('validation/accuracy', loss, n_iter)\n","\n","        print('Validation -> sample/numSamples/epoch: {0}/{1}/{2}, Accuracy: {3}%' \\\n","              .format(i_batch, lensamples, epoch, loss))\n","\n","        total_losses.append(loss)\n","        del images\n","        del ground\n","        del n_iter\n","        del output\n","    mean_loss = torch.stack(total_losses).mean()\n","    print('Mean Accuracy: {}'.format(mean_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"qE_HzZ30mHFR"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\ger-m\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"ename":"AssertionError","evalue":"scalar should be 0D","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-16-cb82443243ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-15-e7dee3e878e8>\u001b[0m in \u001b[0;36mvalidation\u001b[1;34m(epoch, dataloader, model, criterion, dev)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mground\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'validation/accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         print('Validation -> sample/numSamples/epoch: {0}/{1}/{2}, Accuracy: {3}%' \\\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorboardX\\writer.py\u001b[0m in \u001b[0;36madd_scalar\u001b[1;34m(self, tag, scalar_value, global_step, walltime, display_name, summary_description)\u001b[0m\n\u001b[0;32m    450\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input value: \\\"{}\\\" is not a scalar\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         self._get_file_writer().add_summary(\n\u001b[1;32m--> 452\u001b[1;33m             scalar(tag, scalar_value, display_name, summary_description), global_step, walltime)\n\u001b[0m\u001b[0;32m    453\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomet_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalar_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorboardX\\summary.py\u001b[0m in \u001b[0;36mscalar\u001b[1;34m(name, scalar, display_name, summary_description)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_clean_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mscalar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'scalar should be 0D'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[0mscalar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdisplay_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msummary_description\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mAssertionError\u001b[0m: scalar should be 0D"]}],"source":["EPOCH = 1\n","\n","for epoch in range(EPOCH):\n","    validation(epoch, validation_generator, model, accuracy, device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
